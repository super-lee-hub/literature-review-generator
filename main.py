#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ–‡çŒ®ç»¼è¿°è‡ªåŠ¨ç”Ÿæˆå™¨ - å·¥ä¸šçº§ç‰ˆæœ¬
æ”¯æŒèº«ä»½åŸºæ–­ç‚¹ç»­ä¼ ã€åŒé‡å·¥ä½œæ¨¡å¼ã€æ™ºèƒ½ç»­å†™ã€é¡¹ç›®å‘½åç©ºé—´ã€æ™ºèƒ½æ–‡ä»¶æŸ¥æ‰¾ã€åŒå¼•æ“PDFæå–ã€é€‚åº”æ€§é€Ÿç‡æ§åˆ¶ã€å¹¶å‘å¤„ç†ã€é”™è¯¯ç®¡ç†ã€è‡ªåŠ¨é‡è¯•æœºåˆ¶å’Œäº¤äº’å¼å®‰è£…å‘å¯¼ã€‚

ä½œè€…: llm_reviewer_generator æ–‡çŒ®ç»¼è¿°è‡ªåŠ¨ç”Ÿæˆå™¨å¼€å‘å›¢é˜Ÿ
ç‰ˆæœ¬: 1.2
æ›´æ–°æ—¥æœŸ: 2025-10-15
"""

import sys
import os
import time
import argparse
import traceback
import concurrent.futures
import threading
import json
import logging
from typing import List, Dict, Any, Optional, Set, Tuple, Iterator, Union
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from models import (
    PaperInfo, ProcessingResult, FailedPaper, SummariesList,
    APIConfig, AISummary
)
from config_loader import load_config, ConfigDict
from zotero_parser import parse_zotero_report
from file_finder import create_file_index, FileIndex, find_pdf
from pdf_extractor import extract_text_from_pdf  # type: ignore
from ai_interface import get_summary_from_ai, get_summary_from_ai_with_fallback, get_concept_analysis, _call_ai_api  # type: ignore
from docx_writer import create_word_document, append_section_to_word_document, generate_word_table_of_contents, generate_apa_references
from report_generator import generate_excel_report, generate_failure_report, generate_retry_zotero_report  # type: ignore
from utils import ensure_dir, sanitize_path_component
from setup_wizard import run_setup_wizard
import validator

# å¯¼å…¥ä¸Šä¸‹æ–‡ç®¡ç†æ¨¡å—
from context_manager import validate_summary_quality, optimize_context_for_synthesis, optimize_context_for_outline, estimate_tokens



# ä¼˜é›…åœ°å¤„ç†å¯é€‰ä¾èµ–
try:
    from docx import Document  # type: ignore
    from docx.shared import Pt  # type: ignore
    from docx.enum.text import WD_PARAGRAPH_ALIGNMENT  # type: ignore
    from docx.oxml.ns import qn  # type: ignore
    _docx_available = True
except ImportError:
    _docx_available = False
    Document = None  # type: ignore
    Pt = None  # type: ignore
    WD_PARAGRAPH_ALIGNMENT = None  # type: ignore
    qn = None  # type: ignore

# å®šä¹‰å¸¸é‡ï¼Œé¿å…é‡å®šä¹‰é—®é¢˜
DOCX_AVAILABLE = _docx_available

try:
    from tqdm import tqdm  # type: ignore
    _tqdm_available = True
except ImportError:
    _tqdm_available = False
    # åˆ›å»ºä¸€ä¸ªå‡çš„tqdmç±»ä»¥é¿å…åœ¨ä»£ç ä¸­è¿›è¡Œå¤§é‡çš„ifæ£€æŸ¥
    class tqdm:
        def __init__(self, iterable: Optional[Any] = None, **kwargs: Any) -> None:
            self.iterable: List[Any] = iterable if iterable else []
        def __iter__(self) -> Iterator[Any]:
            return iter(self.iterable)
        def __enter__(self) -> 'tqdm':
            return self
        def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
            pass
        def update(self, n: int = 1) -> None:
            pass
        def set_postfix_str(self, s: str) -> None:
            pass
        def close(self) -> None:
            pass

# å®šä¹‰å¸¸é‡ï¼Œé¿å…é‡å®šä¹‰é—®é¢˜
TQDM_AVAILABLE = _tqdm_available

# åœ¨æ–‡ä»¶å¼€å¤´æ‰“å°è­¦å‘Šä¿¡æ¯ï¼ˆä½¿ç”¨loggingè€Œä¸æ˜¯printï¼‰
if not DOCX_AVAILABLE:
    logging.warning("æœªå®‰è£… 'python-docx'ã€‚ç”ŸæˆWordæ–‡æ¡£å’Œç¬¬äºŒé˜¶æ®µéªŒè¯åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚è¯·è¿è¡Œ: pip install python-docx")
if not TQDM_AVAILABLE:
    logging.warning("æœªå®‰è£… 'tqdm'ã€‚å°†æ— æ³•æ˜¾ç¤ºè¿›åº¦æ¡ã€‚è¯·è¿è¡Œ: pip install tqdm")

class CustomLogger(logging.Logger):
    def success(self, msg: str, *args: Any, **kwargs: Any) -> None:
        self.info(f"[SUCCESS] {msg}", *args, **kwargs)
    def warn(self, msg: str, *args: Any, **kwargs: Any) -> None:
        self.warning(f"[WARN] {msg}", *args, **kwargs)

logging.setLoggerClass(CustomLogger)

# ==========================================================

class Counter:
    """çº¿ç¨‹å®‰å…¨è®¡æ•°å™¨ - ç®€åŒ–ç‰ˆæœ¬æé«˜æ€§èƒ½"""
    def __init__(self, initial_value: int = 0):
        self._value = initial_value
        self._lock = threading.Lock()

    def increment(self) -> int:
        with self._lock:
            self._value += 1
            return self._value

    def decrement(self) -> int:
        with self._lock:
            self._value -= 1
            return self._value

    @property
    def value(self) -> int:
        """è·å–å½“å‰å€¼ï¼ˆå±æ€§æ–¹å¼è®¿é—®ï¼‰"""
        with self._lock:
            return self._value

    def set(self, new_value: int) -> None:
        """è®¾ç½®å€¼ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨"""
        with self._lock:
            self._value = new_value

    def get_value(self) -> int:
        """è·å–å½“å‰å€¼ï¼Œé¿å…å±æ€§è£…é¥°å™¨çš„å¼€é”€"""
        with self._lock:
            return self._value

    def set_value(self, new_value: int) -> None:
        """è®¾ç½®å€¼ï¼Œç¡®ä¿çº¿ç¨‹å®‰å…¨"""
        with self._lock:
            self._value = new_value

class ReportingService:
    """æŠ¥å‘Šç”ŸæˆæœåŠ¡ - ä¸“é—¨è´Ÿè´£ç”Ÿæˆå„ç§åˆ†ææŠ¥å‘Š"""

    def __init__(self, logger: CustomLogger):
        self.logger: CustomLogger = logger if logger is not None else logging.getLogger(__name__)  # type: ignore

    def generate_all_reports(self, generator: 'LiteratureReviewGenerator') -> None:
        """ç”Ÿæˆæ‰€æœ‰åˆ†æé˜¶æ®µçš„æŠ¥å‘Š"""
        self.logger.info("æ­£åœ¨ç”Ÿæˆæ‰€æœ‰åˆ†ææŠ¥å‘Š...")

        # ç”ŸæˆExcelæŠ¥å‘Š
        if not generate_excel_report(generator):
            self.logger.warning("ExcelæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½†ä¸å½±å“æ•´ä½“å¤„ç†ç»“æœ")

        # ç”Ÿæˆå¤±è´¥æŠ¥å‘Šï¼ˆå¦‚æœæœ‰å¤±è´¥çš„è®ºæ–‡ï¼‰
        if generator.failed_papers:
            if not generate_failure_report(generator):
                self.logger.warning("å¤±è´¥æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½†ä¸å½±å“æ•´ä½“å¤„ç†ç»“æœ")

        # åªåœ¨Zoteroæ¨¡å¼ä¸‹ç”Ÿæˆè‡ªåŠ¨åŒ–é‡è·‘æŠ¥å‘Š
        if generator.mode == "zotero" and generator.failed_papers:
            if not generate_retry_zotero_report(generator):
                self.logger.warning("é‡è·‘æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œä½†ä¸å½±å“æ•´ä½“å¤„ç†ç»“æœ")

        self.logger.success("æ‰€æœ‰åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæ¯•ã€‚")


class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨ - ä¸“é—¨è´Ÿè´£å¤„ç†åŸºäºèº«ä»½çš„æ–­ç‚¹ç»­ä¼ """

    def __init__(self, logger: CustomLogger):
        self.logger: CustomLogger = logger or logging.getLogger(__name__)  # type: ignore

    def save_checkpoint(self, generator: 'LiteratureReviewGenerator') -> bool:
        """ä¿å­˜åŸºäºèº«ä»½çš„æ–­ç‚¹æ–‡ä»¶"""
        try:
            if not generator.output_dir or not generator.project_name:
                return False

            checkpoint_file = os.path.join(generator.output_dir, f'{generator.project_name}_checkpoint.json')

            # åˆ›å»ºå·²å¤„ç†è®ºæ–‡çš„èº«ä»½é›†åˆ
            processed_papers: Set[str] = set()
            for summary in generator.summaries:
                if summary.get('status') == 'success':
                    paper_info: PaperInfo = summary.get('paper_info', {})  # type: ignore
                    paper_key: str = LiteratureReviewGenerator.get_paper_key(paper_info)  # type: ignore
                    processed_papers.add(paper_key)

            # åˆ›å»ºå¤±è´¥è®ºæ–‡çš„èº«ä»½é›†åˆ
            failed_papers: Set[str] = set()
            for failed_item in generator.failed_papers:
                paper_info: PaperInfo = failed_item.get('paper_info', {})  # type: ignore
                paper_key: str = LiteratureReviewGenerator.get_paper_key(paper_info)  # type: ignore
                failed_papers.add(paper_key)

            checkpoint_data: Dict[str, Any] = {
                'version': '2.0',  # èº«ä»½åŸºæ–­ç‚¹ç‰ˆæœ¬
                'project_name': generator.project_name,
                'update_time': datetime.now().isoformat(),
                'total_papers': len(generator.papers),
                'processed_count': len(processed_papers),
                'failed_count': len(failed_papers),
                'processed_papers': list(processed_papers),  # åŸºäºèº«ä»½çš„å·²å¤„ç†åˆ—è¡¨
                'failed_papers': list(failed_papers),        # åŸºäºèº«ä»½çš„å¤±è´¥åˆ—è¡¨
                'processing_stats': {
                    'processed_success': generator.processed_count.value,
                    'failed_attempts': generator.failed_count.value
                }
            }

            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)

            self.logger.info(f"[æ–­ç‚¹ä¿å­˜] å·²ä¿å­˜å¤„ç†è¿›åº¦: {len(processed_papers)}æˆåŠŸ, {len(failed_papers)}å¤±è´¥")
            return True

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–­ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def load_checkpoint(self, generator: 'LiteratureReviewGenerator') -> bool:
        """åŠ è½½åŸºäºèº«ä»½çš„æ–­ç‚¹æ–‡ä»¶"""
        try:
            if not generator.output_dir or not generator.project_name:
                return False

            checkpoint_file = os.path.join(generator.output_dir, f'{generator.project_name}_checkpoint.json')

            if not os.path.exists(checkpoint_file):
                self.logger.info("[æ–­ç‚¹åŠ è½½] æœªæ‰¾åˆ°æ–­ç‚¹æ–‡ä»¶ï¼Œå°†å¼€å§‹å…¨æ–°å¤„ç†")
                return False

            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data: Dict[str, Any] = json.load(f)

            # éªŒè¯æ–­ç‚¹æ–‡ä»¶ç‰ˆæœ¬
            version = checkpoint_data.get('version', '1.0')
            if version != '2.0':
                self.logger.warning(f"[æ–­ç‚¹åŠ è½½] æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ–­ç‚¹æ–‡ä»¶(v{version})ï¼Œå°†å¼€å§‹å…¨æ–°å¤„ç†")
                return False

            # éªŒè¯é¡¹ç›®åç§°åŒ¹é…
            checkpoint_project = checkpoint_data.get('project_name')
            if checkpoint_project != generator.project_name:
                self.logger.warning(f"[æ–­ç‚¹åŠ è½½] é¡¹ç›®åç§°ä¸åŒ¹é…({checkpoint_project} != {generator.project_name})ï¼Œå°†å¼€å§‹å…¨æ–°å¤„ç†")
                return False

            # æå–å·²å¤„ç†å’Œå¤±è´¥çš„è®ºæ–‡èº«ä»½
            processed_papers = set(checkpoint_data.get('processed_papers', []))
            failed_papers = set(checkpoint_data.get('failed_papers', []))
            update_time = checkpoint_data.get('update_time', 'æœªçŸ¥æ—¶é—´')

            self.logger.info(f"[æ–­ç‚¹åŠ è½½] æˆåŠŸåŠ è½½æ–­ç‚¹æ–‡ä»¶ (æ›´æ–°æ—¶é—´: {update_time})")
            self.logger.info(f"[æ–­ç‚¹åŠ è½½] å·²å¤„ç†è®ºæ–‡: {len(processed_papers)}ç¯‡")
            self.logger.info(f"[æ–­ç‚¹åŠ è½½] å¤±è´¥è®ºæ–‡: {len(failed_papers)}ç¯‡")

            # å°†æ–­ç‚¹ä¿¡æ¯å­˜å‚¨åˆ°å®ä¾‹å˜é‡ä¸­ï¼Œä¾›process_all_papersä½¿ç”¨
            generator._checkpoint_processed_papers = processed_papers  # type: ignore
            generator._checkpoint_failed_papers = failed_papers  # type: ignore

            # æ¢å¤è®¡æ•°å™¨
            processing_stats: Dict[str, Any] = checkpoint_data.get('processing_stats') or {}
            generator.processed_count.set(processing_stats.get('processed_success', 0))  # type: ignore
            generator.failed_count.set(processing_stats.get('failed_attempts', 0))  # type: ignore

            return True

        except Exception as e:
            self.logger.error(f"åŠ è½½æ–­ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
            return False


class LiteratureReviewGenerator:
    """æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨ä¸»ç±»"""
    
    logger: CustomLogger
    
    def __init__(self, config_file: str = 'config.ini', project_name: Optional[str] = None, pdf_folder: Optional[str] = None):
        self.config_file: str = config_file
        self.project_name: Optional[str] = project_name
        self.pdf_folder: Optional[str] = pdf_folder
        self.config: Optional['ConfigDict'] = None
        self.output_dir: Optional[str] = None
        self.summary_file: Optional[str] = None
        self.papers: List[PaperInfo] = []
        self.summaries: SummariesList = []
        self.failed_papers: List[FailedPaper] = []
        self.processed_count: Counter = Counter(0)
        self.failed_count: Counter = Counter(0)
        self.save_lock: threading.Lock = threading.Lock()

        # èº«ä»½åŸºæ–­ç‚¹ç»­ä¼ ç›¸å…³å˜é‡
        self._checkpoint_processed_papers: Set[str] = set()
        self._checkpoint_failed_papers: Set[str] = set()

        # æ¦‚å¿µå¢å¼ºæ¨¡å¼ç›¸å…³å˜é‡
        self.concept_mode: bool = False
        self.concept_profile: Optional[Dict[str, Any]] = None

        # æ ¹æ®å‚æ•°ç¡®å®šè¿è¡Œæ¨¡å¼
        if pdf_folder:
            self.mode: str = "direct"  # ç›´æ¥PDFæ¨¡å¼
            self.pdf_folder = os.path.abspath(pdf_folder)
        else:
            self.mode: str = "zotero"  # Zoteroæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰

        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        self._init_logger()

        # åˆå§‹åŒ–æœåŠ¡ç»„ä»¶
        self.reporting_service: ReportingService = ReportingService(self.logger)
        self.checkpoint_manager: CheckpointManager = CheckpointManager(self.logger)
    
    def _init_logger(self):
        """åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨"""
        import logging
        import os
        from datetime import datetime
        
        # åˆ›å»ºæ—¥å¿—è®°å½•å™¨
        self.logger = logging.getLogger(f"llm_reviewer_generator_{datetime.now().strftime('%Y%m%d_%H%M%S')}")  # type: ignore
        self.logger.setLevel(logging.INFO)
        
        # å¦‚æœè®°å½•å™¨å·²ç»æœ‰å¤„ç†å™¨ï¼Œå…ˆæ¸…é™¤
        if self.logger.handlers:
            self.logger.handlers.clear()
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] %(message)s', 
                                    datefmt='%H:%M:%S')
        console_handler.setFormatter(formatter)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        try:
            # åˆ›å»ºlogsç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            logs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'logs')
            os.makedirs(logs_dir, exist_ok=True)
            
            # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶åï¼šä½¿ç”¨æ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            log_file = os.path.join(logs_dir, f'llm_reviewer_{timestamp}.log')
            
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.INFO)
            file_handler.setFormatter(formatter)
            
            # æ·»åŠ å¤„ç†å™¨åˆ°è®°å½•å™¨
            self.logger.addHandler(console_handler)
            self.logger.addHandler(file_handler)
            
            # è®°å½•æ—¥å¿—æ–‡ä»¶ä½ç½®
            self.logger.info(f"æ—¥å¿—æ–‡ä»¶å·²åˆ›å»º: {log_file}")
            
        except Exception as e:
            # å¦‚æœæ–‡ä»¶æ—¥å¿—å¤±è´¥ï¼Œåªä½¿ç”¨æ§åˆ¶å°æ—¥å¿—
            self.logger.warning(f"æ— æ³•åˆ›å»ºæ–‡ä»¶æ—¥å¿—ï¼Œä»…ä½¿ç”¨æ§åˆ¶å°æ—¥å¿—: {e}")
            self.logger.addHandler(console_handler)
    
    def load_configuration(self) -> bool:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            self.config = load_config(self.config_file)
            if not self.config:
                self.logger.error("é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥æˆ–ä¸ºç©º")
                return False
            self.logger.success("é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            self.logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¼‚å¸¸: {e}")
            return False
    
    def setup_output_directory(self) -> bool:
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        try:
            # æ£€æŸ¥é…ç½®æ˜¯å¦å·²åŠ è½½
            if not self.config:
                self.logger.error("é…ç½®æœªåŠ è½½ï¼Œæ— æ³•è®¾ç½®è¾“å‡ºç›®å½•")
                return False
            
            # ç¡®å®šé¡¹ç›®åç§°
            if not self.project_name:
                if self.mode == "zotero":
                    # Zoteroæ¨¡å¼ä½¿ç”¨é»˜è®¤é¡¹ç›®å
                    self.project_name = "literature_review"
                else:
                    # ç›´æ¥PDFæ¨¡å¼ä½¿ç”¨æ–‡ä»¶å¤¹åä½œä¸ºé¡¹ç›®å
                    self.project_name = os.path.basename((self.pdf_folder or '').rstrip('/\\'))
            
            # æ¸…ç†é¡¹ç›®åç§°ï¼Œç§»é™¤éæ³•å­—ç¬¦
            self.project_name = sanitize_path_component(self.project_name)
            
            # ç¡®å®šè¾“å‡ºè·¯å¾„
            paths_config: Dict[str, str] = self.config.get('Paths', {}) if self.config else {}
            output_base_path: str = paths_config.get('output_path', './output')
            self.output_dir = os.path.join(output_base_path, self.project_name)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            if ensure_dir(self.output_dir):
                self.logger.success(f"è¾“å‡ºç›®å½•å·²åˆ›å»º: {self.output_dir}")
            else:
                self.logger.error(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {self.output_dir}")
                return False
            
            # ç¡®å®šæ‘˜è¦æ–‡ä»¶è·¯å¾„
            self.summary_file = os.path.join(self.output_dir, f'{self.project_name}_summaries.json')
            
            return True
        except Exception as e:
            self.logger.error(f"è®¾ç½®è¾“å‡ºç›®å½•å¤±è´¥: {e}")
            return False
    
    def scan_pdf_folder(self) -> bool:
        """æ‰«æPDFæ–‡ä»¶å¤¹ï¼ˆç›´æ¥æ¨¡å¼ä¸“ç”¨ï¼‰"""
        try:
            if self.mode != "direct":
                self.logger.error("scan_pdf_folderåªèƒ½åœ¨ç›´æ¥PDFæ¨¡å¼ä¸‹è°ƒç”¨")
                return False
            
            if not self.pdf_folder or not os.path.exists(self.pdf_folder):
                self.logger.error(f"PDFæ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.pdf_folder}")
                return False
            
            self.logger.info(f"æ­£åœ¨æ‰«æPDFæ–‡ä»¶å¤¹: {self.pdf_folder}")
            
            # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
            pdf_files: List[str] = []
            for root, _dirs, files in os.walk(self.pdf_folder):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        pdf_files.append(os.path.join(root, file))
            
            self.logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
            
            # ä¸ºæ¯ä¸ªPDFæ–‡ä»¶åˆ›å»ºè®ºæ–‡ä¿¡æ¯
            self.papers: List[PaperInfo] = []
            for i, pdf_path in enumerate(pdf_files):
                # ä»æ–‡ä»¶åæå–æ ‡é¢˜ï¼ˆç§»é™¤.pdfæ‰©å±•åï¼‰
                title = os.path.splitext(os.path.basename(pdf_path))[0]
                
                # å°è¯•ä»PDFæ–‡ä»¶ä¸­æå–é¢å¤–ä¿¡æ¯
                pdf_info: Optional[Dict[str, str]] = None  # æ˜ç¡®æŒ‡å®šå­—å…¸å€¼çš„ç±»å‹
                try:
                    from pdf_extractor import get_pdf_info  # type: ignore
                    pdf_info = get_pdf_info(pdf_path)
                except Exception as e:
                    self.logger.warning(f"æ— æ³•ä»PDFæ–‡ä»¶æå–å…ƒæ•°æ®: {pdf_path}, é”™è¯¯: {e}")
                
                # åˆ›å»ºè®ºæ–‡ä¿¡æ¯å­—å…¸
                paper_info: PaperInfo = {
                    'title': title,
                    'authors': [],  # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
                    'year': 'æœªçŸ¥å¹´ä»½',  # å¹´ä»½é€šå¸¸éœ€è¦OCRæ‰èƒ½ä»PDFä¸­æå–ï¼Œæš‚æ—¶è®¾ä¸ºé»˜è®¤å€¼
                    'journal': 'æœªçŸ¥æœŸåˆŠ',  # æœŸåˆŠä¿¡æ¯é€šå¸¸éœ€è¦OCRæ‰èƒ½ä»PDFä¸­æå–ï¼Œæš‚æ—¶è®¾ä¸ºé»˜è®¤å€¼
                    'doi': '',  # ç›´æ¥æ¨¡å¼ä¸‹DOIä¸ºç©º
                    'pdf_path': pdf_path,  # PDFæ–‡ä»¶è·¯å¾„
                    'file_index': i  # æ–‡ä»¶ç´¢å¼•
                }
                
                # ä»PDFä¿¡æ¯ä¸­æå–ä½œè€…
                if pdf_info:
                    author_str = pdf_info.get('author', '')
                    if author_str and author_str.strip():
                        # å°†ä½œè€…å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
                        paper_info['authors'] = [author_str.strip()]
                
                # å¦‚æœPDFä¿¡æ¯ä¸ºç©ºæˆ–æ— ä½œè€…ï¼Œè®¾ä¸ºç©ºæ•°ç»„
                authors = paper_info.get('authors', [])
                if not authors:
                    paper_info['authors'] = []
                elif any(author.strip() in ['Unknown', 'æœªçŸ¥'] for author in authors):
                    paper_info['authors'] = []
                
                # å°è¯•ä»æ–‡ä»¶åä¸­æå–å¹´ä»½ï¼ˆç®€å•æ¨¡å¼åŒ¹é…ï¼‰
                import re
                year_match = re.search(r'(20\d{2})', title)  # æœç´¢2020-2099å¹´ä»½
                if year_match:
                    paper_info['year'] = year_match.group(1)
                
                # å°è¯•ä»æ–‡ä»¶åä¸­æå–ä½œè€…ï¼ˆå¦‚æœæ–‡ä»¶åæ ¼å¼åŒ…å«ä¸‹åˆ’çº¿åˆ†éš”çš„ä½œè€…åï¼‰
                if '_' in title:
                    # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º: "æ ‡é¢˜_ä½œè€….pdf" æˆ– "æ ‡é¢˜_ä½œè€…_å…¶ä»–ä¿¡æ¯.pdf"
                    parts = title.split('_')
                    if len(parts) >= 2:
                        potential_author = parts[-1].strip()
                        if potential_author and potential_author != 'ä¾¯ç”œç”œ' and potential_author != 'è´ºçˆ±å¿ ' and potential_author != 'å‘¨å†²' and potential_author != 'ç›˜åŸ' and potential_author != 'å¼ èµ›æ¥ ' and potential_author != 'å½­ä¸½å¾½' and potential_author != 'åº·è¶…' and potential_author != 'åˆ˜ä¼Ÿå' and potential_author != 'æœ±åä¸œ':
                            paper_info['authors'] = [potential_author]
                
                self.papers.append(paper_info)
            
            self.logger.success(f"PDFæ–‡ä»¶å¤¹æ‰«æå®Œæˆï¼Œå…± {len(self.papers)} ç¯‡è®ºæ–‡")
            return True
            
        except Exception as e:
            self.logger.error(f"æ‰«æPDFæ–‡ä»¶å¤¹å¤±è´¥: {e}")
            return False
    
    def parse_zotero_report(self, override_path: Optional[str] = None) -> bool:
        """è§£æZoteroæŠ¥å‘Šï¼ˆZoteroæ¨¡å¼ä¸“ç”¨ï¼‰"""
        try:
            if self.mode != "zotero":
                self.logger.error("parse_zotero_reportåªèƒ½åœ¨Zoteroæ¨¡å¼ä¸‹è°ƒç”¨")
                return False
            
            # ç¡®å®šZoteroæŠ¥å‘Šè·¯å¾„
            if override_path:
                zotero_report_path = override_path
            else:
                paths_config: Dict[str, str] = self.config.get('Paths', {}) if self.config else {}
                zotero_report_path: str = paths_config.get('zotero_report', '')
            
            if not zotero_report_path or not os.path.exists(zotero_report_path):
                self.logger.error(f"ZoteroæŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {zotero_report_path}")
                return False
            
            self.logger.info(f"æ­£åœ¨è§£æZoteroæŠ¥å‘Š: {zotero_report_path}")
            
            # è§£ææŠ¥å‘Š
            self.papers = parse_zotero_report(zotero_report_path)
            
            if not self.papers:
                self.logger.error("ZoteroæŠ¥å‘Šè§£æå¤±è´¥æˆ–æŠ¥å‘Šä¸ºç©º")
                return False
            
            self.logger.success(f"ZoteroæŠ¥å‘Šè§£æå®Œæˆï¼Œå…± {len(self.papers)} ç¯‡è®ºæ–‡")
            return True
            
        except Exception as e:
            self.logger.error(f"è§£æZoteroæŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    @staticmethod
    def get_paper_key(paper: 'Dict[str, Any] | PaperInfo') -> str:
        """ä¸ºè®ºæ–‡ç”Ÿæˆå”¯ä¸€èº«ä»½æ ‡è¯†"""
        # ä¼˜å…ˆä½¿ç”¨DOIä½œä¸ºå”¯ä¸€æ ‡è¯†
        doi = paper.get('doi', '').strip()
        if doi and doi.lower() != 'unknown' and doi.lower() != 'n/a':
            # DOIæ ‡å‡†åŒ–å¤„ç†ï¼šæå–çº¯ç²¹çš„IDéƒ¨åˆ†
            import re
            # åŒ¹é…DOI IDæ¨¡å¼ï¼šä»¥10.å¼€å¤´ï¼Œåè·Ÿæ•°å­—å’Œæ–œæ 
            doi_pattern = r'(10\.\d+/.+)'
            match = re.search(doi_pattern, doi)
            
            if match:
                # è¿”å›æ ‡å‡†åŒ–çš„DOI IDéƒ¨åˆ†
                return match.group(1)
            else:
                # å¦‚æœæ— æ³•æå–æ ‡å‡†æ ¼å¼ï¼Œè¿”å›åŸå§‹DOIï¼ˆä½†è¿›è¡ŒåŸºæœ¬æ¸…ç†ï¼‰
                # ç§»é™¤å¸¸è§çš„DOIå‰ç¼€
                doi_clean = re.sub(r'^https?://(doi\.org|dx\.doi\.org)/', '', doi, flags=re.IGNORECASE)
                return doi_clean
        
        # å¦‚æœæ²¡æœ‰DOIï¼Œä½¿ç”¨æ ‡é¢˜+ä½œè€…ç»„åˆ
        title = paper.get('title', '').strip()
        authors = paper.get('authors', [])
        
        # æ¸…ç†å’Œæ ‡å‡†åŒ–æ ‡é¢˜
        if title:
            import re
            title_clean = re.sub(r'[^\w\s]', '', title.lower())
            title_clean = re.sub(r'\s+', ' ', title_clean).strip()
        else:
            title_clean = 'unknown_title'
        
        # å¤„ç†ä½œè€…åˆ—è¡¨
        if authors and isinstance(authors, list):
            author_surnames: List[str] = []
            for author in authors[:3]:  # åªå–å‰3ä¸ªä½œè€… # type: ignore
                if isinstance(author, str):
                    name_parts: List[str] = author.strip().split()
                    if name_parts:
                        surname: str = name_parts[-1].lower()
                        author_surnames.append(surname)
            
            if len(authors) > 3:  # type: ignore

                author_surnames.append('et_al')
            
            authors_str = '_'.join(author_surnames) if author_surnames else 'unknown_author'
        else:
            authors_str = 'unknown_author'
        
        # ç»„åˆæ ‡é¢˜å’Œä½œè€…ä½œä¸ºå”¯ä¸€æ ‡è¯†
        return f"{title_clean}_{authors_str}"
    
    def load_existing_summaries(self) -> bool:
        """åŠ è½½ç°æœ‰æ‘˜è¦æ–‡ä»¶ï¼ˆç”¨äºæ–­ç‚¹ç»­ä¼ ï¼‰"""
        try:
            if not self.summary_file or not os.path.exists(self.summary_file):
                self.logger.info("æœªæ‰¾åˆ°ç°æœ‰æ‘˜è¦æ–‡ä»¶ï¼Œå°†å¼€å§‹å…¨æ–°å¤„ç†")
                self.summaries = []
                return True
            
            with open(self.summary_file, 'r', encoding='utf-8') as f:
                loaded_data = json.load(f)
                self.summaries = loaded_data if isinstance(loaded_data, list) else []
            
            # éªŒè¯æ•°æ®æ ¼å¼
            if not isinstance(self.summaries, list):  # type: ignore
                self.logger.warning("ç°æœ‰æ‘˜è¦æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œå°†å¼€å§‹å…¨æ–°å¤„ç†")
                self.summaries = []
                return True
            
            success_count = len([s for s in self.summaries if s.get('status') == 'success'])
            failed_count = len([s for s in self.summaries if s.get('status') == 'failed'])
            
            self.logger.success(f"å·²åŠ è½½ç°æœ‰æ‘˜è¦æ–‡ä»¶: {success_count}æˆåŠŸ, {failed_count}å¤±è´¥")
            return True
            
        except Exception as e:
            self.logger.warning(f"åŠ è½½ç°æœ‰æ‘˜è¦æ–‡ä»¶å¤±è´¥ï¼Œå°†å¼€å§‹å…¨æ–°å¤„ç†: {e}")
            self.summaries = []
            return True  # å³ä½¿åŠ è½½å¤±è´¥ä¹Ÿè¿”å›Trueï¼Œå› ä¸ºæˆ‘ä»¬ä»å¯ä»¥ç»§ç»­å¤„ç†
    
    def reset_counters(self):
        """é‡ç½®è®¡æ•°å™¨"""
        self.processed_count.set(0)
        self.failed_count.set(0)
    
    def process_paper(self, paper: PaperInfo, paper_index: int, file_index: Optional[FileIndex], total_papers: int) -> Optional[ProcessingResult]:
        """å¤„ç†å•ç¯‡è®ºæ–‡"""
        try:
            paper_key = LiteratureReviewGenerator.get_paper_key(paper)  # type: ignore
            
            # æ£€æŸ¥æ˜¯å¦å·²åœ¨æ–­ç‚¹ä¸­å¤„ç†è¿‡
            if paper_key in self._checkpoint_processed_papers:
                self.logger.info(f"è·³è¿‡å·²å¤„ç†è®ºæ–‡: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                # ä»ç°æœ‰æ‘˜è¦ä¸­æ‰¾åˆ°å¯¹åº”çš„æ¡ç›®
                for summary in self.summaries:
                    if summary.get('status') == 'success' and LiteratureReviewGenerator.get_paper_key(summary.get('paper_info', {})) == paper_key:
                        return summary
                return None
            
            if paper_key in self._checkpoint_failed_papers:
                self.logger.info(f"è·³è¿‡å·²å¤±è´¥è®ºæ–‡: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                # ä»ç°æœ‰æ‘˜è¦ä¸­æ‰¾åˆ°å¯¹åº”çš„æ¡ç›®
                for summary in self.summaries:
                    if summary.get('status') == 'failed' and LiteratureReviewGenerator.get_paper_key(summary.get('paper_info', {})) == paper_key:
                        return summary
                return None
            
            self.logger.info(f"[{paper_index+1}/{total_papers}] æ­£åœ¨å¤„ç†: {paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
            
            # è·å–PDFæ–‡ä»¶è·¯å¾„
            pdf_path = paper.get('pdf_path')
            if not pdf_path and self.mode == "zotero":
                # Zoteroæ¨¡å¼ä¸‹æŸ¥æ‰¾PDFæ–‡ä»¶
                file_title = paper.get('title', '')
                _file_authors = paper.get('authors', [])
                paths_config: Dict[str, str] = self.config.get('Paths', {}) if self.config else {}
                library_path: str = paths_config.get('library_path', '')
                
                if not library_path:
                    failure_reason = "é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘library_pathè·¯å¾„"
                    self.logger.error(failure_reason)
                    return {
                        'paper_info': paper,
                        'status': 'failed',
                        'failure_reason': failure_reason
                    }
                
                # åˆ›å»ºæ–‡ä»¶ç´¢å¼•ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
                if not file_index:
                    file_index = create_file_index(library_path)
                
                # ä½¿ç”¨ file_finder.py ä¸­å¼ºå¤§çš„ find_pdf å‡½æ•°
                find_result = find_pdf(dict(paper), library_path, file_index)
                
                if find_result and find_result[0]:
                    pdf_path = find_result[0]
                    self.logger.info(f"æ™ºèƒ½æŸ¥æ‰¾åˆ°PDF: {os.path.basename(pdf_path)}")
                else:
                    failure_reason: str = find_result[1] if find_result and len(find_result) > 1 else "æœªæ‰¾åˆ°PDFæ–‡ä»¶"
                    self.logger.error(f"æœªæ‰¾åˆ°PDFæ–‡ä»¶: {file_title} - åŸå› : {failure_reason}")
                    return {
                        'paper_info': paper,
                        'status': 'failed',
                        'failure_reason': failure_reason
                    }
            elif not pdf_path and self.mode == "direct":
                # ç›´æ¥æ¨¡å¼ä¸‹PDFè·¯å¾„åº”è¯¥å·²ç»å­˜åœ¨
                pdf_path = paper.get('pdf_path', '')
            
            if not pdf_path or not os.path.exists(pdf_path):
                failure_reason = f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"
                self.logger.error(failure_reason)
                return {
                    'paper_info': paper,
                    'status': 'failed',
                    'failure_reason': failure_reason
                }
            
            # æå–PDFæ–‡æœ¬

            self.logger.info(f"æ­£åœ¨æå–PDFæ–‡æœ¬: {os.path.basename(pdf_path)}")

            pdf_text = extract_text_from_pdf(pdf_path)  # type: ignore

            

            if not pdf_text or len(pdf_text.strip()) < 500:  # type: ignore

                failure_reason = f"PDFæ–‡æœ¬æå–å¤±è´¥æˆ–å†…å®¹è¿‡å°‘({len(pdf_text) if pdf_text else 0}å­—ç¬¦)"  # type: ignore

                self.logger.error(failure_reason)

                return {

                    'paper_info': paper,

                    'status': 'failed',

                    'failure_reason': failure_reason

                }

            

            self.logger.success(f"PDFæ–‡æœ¬æå–æˆåŠŸ: {len(pdf_text)}å­—ç¬¦")  # type: ignore
            
            # è°ƒç”¨AI APIç”Ÿæˆæ‘˜è¦
            self.logger.info("æ­£åœ¨è°ƒç”¨AIç”Ÿæˆæ‘˜è¦...")
            
            # æå–åˆ†æå¼•æ“APIé…ç½®
            primary_reader_config: Dict[str, str] = self.config.get('Primary_Reader_API', {}) if self.config else {}
            reader_api_config: APIConfig = {
                'api_key': primary_reader_config.get('api_key', ''),
                'model': primary_reader_config.get('model', ''),
                'api_base': primary_reader_config.get('api_base', 'https://api.openai.com/v1')
            }
            
            # æå–å¤‡ç”¨å¼•æ“APIé…ç½®ï¼ˆç”¨äºè¶…é•¿è®ºæ–‡ï¼‰
            backup_reader_config: Dict[str, str] = self.config.get('Backup_Reader_API', {}) if self.config else {}
            backup_api_config: APIConfig = {
                'api_key': backup_reader_config.get('api_key', ''),
                'model': backup_reader_config.get('model', ''),
                'api_base': backup_reader_config.get('api_base', 'https://api.openai.com/v1')
            }
            
            # æ„å»ºå®Œæ•´çš„åˆ†ææç¤ºè¯
            try:
                # ğŸ†• ç›´æ¥ä½¿ç”¨ä¼˜åŒ–çš„åˆ†ææç¤ºè¯
                with open('prompts/optimized_prompt_analyze.txt', 'r', encoding='utf-8') as f:
                    prompt_template = f.read()
                self.logger.info("ä½¿ç”¨ä¼˜åŒ–åçš„åˆ†ææç¤ºè¯")
                
                # æ›¿æ¢å ä½ç¬¦
                analysis_prompt: str = prompt_template.replace('{{PAPER_FULL_TEXT}}', pdf_text)
                
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½ä¼˜åŒ–åˆ†ææç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨ç®€åŒ–æç¤ºè¯: {e}")
                # ç®€åŒ–æç¤ºè¯
                analysis_prompt = f"è¯·åˆ†æä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ï¼š\n\n{pdf_text}"
            
            # è°ƒç”¨AIæ¥å£ç”Ÿæˆæ‘˜è¦ï¼ˆè‡ªåŠ¨å¤„ç†å¼•æ“åˆ‡æ¢ï¼‰
            ai_result = get_summary_from_ai_with_fallback(analysis_prompt, reader_api_config, backup_api_config, logger=self.logger, config=self.config)
            
            if not ai_result:
                failure_reason = "AIæ‘˜è¦ç”Ÿæˆå¤±è´¥"
                self.logger.error(failure_reason)
                return {
                    'paper_info': paper,
                    'status': 'failed',
                    'failure_reason': failure_reason
                }
            
            self.logger.success("AIæ‘˜è¦ç”ŸæˆæˆåŠŸ")
            
            # =================== CONTENT QUALITY CHECK ===================
            # ä½¿ç”¨æ–°çš„ä¸Šä¸‹æ–‡ç®¡ç†æ¨¡å—è¿›è¡Œè´¨é‡æ£€æŸ¥ï¼Œå¦‚æœè´¨é‡ä¸è¾¾æ ‡åˆ™æ ‡è®°ä¸ºå¤±è´¥
            
            # æ„å»ºæ¨¡æ‹Ÿçš„ProcessingResultå¯¹è±¡ç”¨äºè´¨é‡æ£€æŸ¥
            temp_result: Dict[str, Any] = {
                'paper_info': paper,
                'status': 'success',
                'ai_summary': ai_result
            }
            
            # ä½¿ç”¨context_managerçš„è´¨é‡æ£€æŸ¥åŠŸèƒ½
            is_quality_ok, quality_reason = validate_summary_quality(temp_result)
            
            if not is_quality_ok:
                # ğŸš¨ å†…å®¹è´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å¼•æ“
                failure_reason = f"AIç”Ÿæˆå†…å®¹ä¸ºç©ºæˆ–ä¸å®Œæ•´: {quality_reason}"
                self.logger.warning(failure_reason)
                
                # æ£€æŸ¥æ˜¯å¦é…ç½®äº†å¤‡ç”¨å¼•æ“
                backup_api_key = backup_api_config.get('api_key', '')
                if backup_api_key and backup_api_key.strip():
                    self.logger.info("ä¸»å¼•æ“å†…å®¹è´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å¼•æ“...")
                    
                    # ä½¿ç”¨å¤‡ç”¨å¼•æ“ç›´æ¥è°ƒç”¨ï¼ˆç»•è¿‡ä¸»å¼•æ“ï¼‰
                    backup_result = get_summary_from_ai(analysis_prompt, reader_api_config, backup_api_config,
                                                       engine_type='backup', logger=self.logger, config=self.config)
                    
                    if backup_result:
                        self.logger.success("å¤‡ç”¨å¼•æ“AIæ‘˜è¦ç”ŸæˆæˆåŠŸ")
                        
                        # æ£€æŸ¥å¤‡ç”¨å¼•æ“ç»“æœçš„è´¨é‡
                        temp_result_backup: Dict[str, Any] = {
                            'paper_info': paper,
                            'status': 'success',
                            'ai_summary': backup_result
                        }
                        
                        is_quality_ok_backup, quality_reason_backup = validate_summary_quality(temp_result_backup)
                        
                        if is_quality_ok_backup:
                            self.logger.info("å¤‡ç”¨å¼•æ“å†…å®¹è´¨é‡æ£€æŸ¥é€šè¿‡")
                            ai_result = backup_result  # ä½¿ç”¨å¤‡ç”¨å¼•æ“çš„ç»“æœ
                            # ç»§ç»­åç»­å¤„ç†
                        else:
                            self.logger.warning(f"å¤‡ç”¨å¼•æ“å†…å®¹è´¨é‡æ£€æŸ¥ä¹Ÿå¤±è´¥: {quality_reason_backup}")
                            # å¤‡ç”¨å¼•æ“ä¹Ÿå¤±è´¥ï¼Œè¿”å›å¤±è´¥ç»“æœ
                            failed_result: ProcessingResult = {
                                'paper_info': paper,
                                'status': 'failed',
                                'failure_reason': f"ä¸»å¼•æ“å’Œå¤‡ç”¨å¼•æ“éƒ½å¤±è´¥: {quality_reason}; å¤‡ç”¨å¼•æ“: {quality_reason_backup}"
                            }
                            return failed_result
                    else:
                        self.logger.error("å¤‡ç”¨å¼•æ“AIæ‘˜è¦ç”Ÿæˆå¤±è´¥")
                        # è¿”å›å¤±è´¥ç»“æœ
                        failed_result: ProcessingResult = {
                            'paper_info': paper,
                            'status': 'failed',
                            'failure_reason': f"ä¸»å¼•æ“å’Œå¤‡ç”¨å¼•æ“éƒ½å¤±è´¥: {quality_reason}; å¤‡ç”¨å¼•æ“è°ƒç”¨å¤±è´¥"
                        }
                        return failed_result
                else:
                    # æ²¡æœ‰é…ç½®å¤‡ç”¨å¼•æ“ï¼Œç›´æ¥è¿”å›å¤±è´¥
                    self.logger.info("æœªé…ç½®å¤‡ç”¨å¼•æ“ï¼Œç›´æ¥è¿”å›å¤±è´¥ä»¥è§¦å‘é‡è¯•æœºåˆ¶")
                    failed_result: ProcessingResult = {
                        'paper_info': paper,
                        'status': 'failed',
                        'failure_reason': failure_reason
                    }
                    return failed_result
            
            self.logger.info("å†…å®¹è´¨é‡æ£€æŸ¥é€šè¿‡")
            # ================================================================
            
            # =================== STAGE 1 VALIDATION (MODULAR) ===================
            if self.config and hasattr(self.config, 'getboolean') and self.config.getboolean('Performance', 'enable_stage1_validation', fallback=False):
                ai_result = validator.validate_paper_analysis(self, pdf_text, ai_result)  # type: ignore
            # ===================================================================
            
            # æ¦‚å¿µå¢å¼ºåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.concept_mode and self.concept_profile and ai_result:
                self.logger.info(f"æ­£åœ¨å¯¹ '{paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}' è¿›è¡Œæ¦‚å¿µå¢å¼ºåˆ†æ...")
                
                # è¯»å–æ¦‚å¿µåˆ†ææç¤ºè¯æ¨¡æ¿
                try:
                    with open('prompts/prompt_concept_analysis.txt', 'r', encoding='utf-8') as f:
                        concept_prompt_template = f.read()
                    self.logger.success(f"åŠ è½½æ¦‚å¿µåˆ†ææç¤ºè¯æ¨¡æ¿: {len(concept_prompt_template)}å­—ç¬¦")
                except Exception as e:
                    self.logger.warning(f"æ— æ³•åŠ è½½æ¦‚å¿µåˆ†ææç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                    concept_prompt_template = "åŸºäºæä¾›çš„èƒŒæ™¯æ¦‚å¿µä¿¡æ¯å’Œè®ºæ–‡æ‘˜è¦ï¼Œåˆ†æè¯¥è®ºæ–‡åœ¨èƒŒæ™¯æ¦‚å¿µå‘å±•ä¸­çš„ä½œç”¨ã€‚\n\nã€èƒŒæ™¯æ¦‚å¿µã€‘\n{{CONCEPT_PROFILE}}\n\nã€è®ºæ–‡æ‘˜è¦ã€‘\n{{PAPER_SUMMARY}}"
                
                # å‡†å¤‡æ¦‚å¿µåˆ†æçš„æç¤ºè¯
                concept_prompt = concept_prompt_template.replace(
                    '{{CONCEPT_PROFILE}}', json.dumps(self.concept_profile, ensure_ascii=False)
                ).replace(
                    '{{PAPER_SUMMARY}}', json.dumps(ai_result, ensure_ascii=False)
                )
                
                # è·å–å†™ä½œå¼•æ“çš„ API é…ç½®
                writer_config: Dict[str, str] = self.config.get('Writer_API', {}) if self.config else {}
                writer_api_config: APIConfig = {
                    'api_key': writer_config.get('api_key') or '',  # type: ignore
                    'model': writer_config.get('model') or '',  # type: ignore
                    'api_base': writer_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
                }

                # è°ƒç”¨æ¦‚å¿µåˆ†ææ¥å£
                concept_analysis_result = get_concept_analysis(concept_prompt, writer_api_config, logger=self.logger, config=self.config)
                
                if concept_analysis_result:
                    # å°†æ¦‚å¿µåˆ†æç»“æœåˆå¹¶åˆ°æœ€ç»ˆçš„æ‘˜è¦ä¸­
                    ai_result['concept_analysis'] = concept_analysis_result
                    self.logger.success("æ¦‚å¿µå¢å¼ºåˆ†ææˆåŠŸã€‚")
                else:
                    self.logger.warning("æ¦‚å¿µå¢å¼ºåˆ†æå¤±è´¥ã€‚")

            # =================== METADATA BACKFILL ===================
            # AIæå–çš„å…ƒæ•°æ®å›å¡«åˆ°paper_infoä¸­ï¼Œè§£å†³Direct PDF Modeä¸‹çš„å…ƒæ•°æ®æ˜¾ç¤ºé—®é¢˜
            try:
                if ai_result and 'common_core' in ai_result:
                    common_core = ai_result['common_core']
                    
                    # æå–AIåˆ†æå‡ºçš„å…ƒæ•°æ®
                    extracted_title = common_core.get('title', '').strip()
                    extracted_authors = common_core.get('authors', [])
                    extracted_year = common_core.get('year', '').strip()
                    extracted_journal = common_core.get('journal', '').strip()
                    extracted_doi = common_core.get('doi', '').strip()
                    
                    # éªŒè¯æå–çš„å…ƒæ•°æ®æ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºä¸”ä¸æ˜¯"æœªçŸ¥"ç­‰å ä½ç¬¦ï¼‰
                    valid_title = extracted_title and extracted_title not in ['', 'æœªçŸ¥', 'N/A', 'æ— æ ‡é¢˜']
                    valid_year = extracted_year and extracted_year not in ['', 'æœªçŸ¥', 'N/A', 'æœªçŸ¥å¹´ä»½']
                    valid_journal = extracted_journal and extracted_journal not in ['', 'æœªçŸ¥', 'N/A', 'æœªçŸ¥æœŸåˆŠ']
                    
                    # æ›´æ–°paper_infoä¸­çš„å…ƒæ•°æ®å­—æ®µ
                    if valid_title:
                        paper['title'] = extracted_title
                    
                    # å¤„ç†authorså­—æ®µï¼šå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
                    if extracted_authors:
                        if isinstance(extracted_authors, list):
                            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
                            if extracted_authors:  # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©º
                                paper['authors'] = extracted_authors
                        elif isinstance(extracted_authors, str):
                            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•åˆ†å‰²ä¸ºåˆ—è¡¨
                            authors_str = extracted_authors.strip()
                            if authors_str and authors_str not in ['', 'æœªçŸ¥', 'N/A']:
                                # å°è¯•æŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
                                import re
                                authors_list = re.split(r'[,ï¼Œã€;ï¼›å’Œand]\s*', authors_str)
                                authors_list = [author.strip() for author in authors_list if author.strip()]
                                if authors_list:
                                    paper['authors'] = authors_list
                    
                    # æ›´æ–°å¹´ä»½å’ŒæœŸåˆŠä¿¡æ¯
                    if valid_year:
                        paper['year'] = extracted_year
                    
                    if valid_journal:
                        paper['journal'] = extracted_journal
                    
                    # æ›´æ–°DOIï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    if extracted_doi:
                        paper['doi'] = extracted_doi
                    
                    # è®°å½•å…ƒæ•°æ®æ›´æ–°æƒ…å†µ
                    updated_fields: List[str] = []
                    if valid_title:
                        updated_fields.append('æ ‡é¢˜')
                    if extracted_authors:
                        updated_fields.append('ä½œè€…')
                    if valid_year:
                        updated_fields.append('å¹´ä»½')
                    if valid_journal:
                        updated_fields.append('æœŸåˆŠ')
                    if extracted_doi:
                        updated_fields.append('DOI')
                    
                    if updated_fields:
                        self.logger.info(f"âœ… å…ƒæ•°æ®å›å¡«æˆåŠŸï¼Œæ›´æ–°å­—æ®µ: {', '.join(updated_fields)}")
                    else:
                        self.logger.info("â„¹ï¸  æœªå‘ç°æœ‰æ•ˆçš„AIæå–å…ƒæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        
            except Exception as e:
                self.logger.warning(f"å…ƒæ•°æ®å›å¡«å¤±è´¥: {e}")
            # =============================================================

            # æ„é€ ç»“æœ
            result: ProcessingResult = {
                'paper_info': paper,
                'status': 'success',
                'ai_summary': ai_result,  # type: ignore
                'processing_time': datetime.now().isoformat(),
                'text_length': len(pdf_text) if pdf_text else 0  # type: ignore
            }
            
            return result
            
        except Exception as e:
            failure_reason = f"å¤„ç†è®ºæ–‡æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}"
            self.logger.error(failure_reason)
            traceback.print_exc()
            failed_result: ProcessingResult = {
                'paper_info': paper,
                'status': 'failed',
                'failure_reason': failure_reason
            }
            return failed_result
    
    def save_summaries(self) -> bool:
        """ä¿å­˜æ‘˜è¦åˆ°JSONæ–‡ä»¶ï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰"""
        try:
            if not self.output_dir or not self.summary_file:
                self.logger.error("è¾“å‡ºç›®å½•æˆ–æ‘˜è¦æ–‡ä»¶è·¯å¾„æœªè®¾ç½®")
                return False
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            if not ensure_dir(self.output_dir):
                self.logger.error(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {self.output_dir}")
                return False
            
            # åˆ›å»ºå¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœåŸæ–‡ä»¶å­˜åœ¨ï¼‰
            if os.path.exists(self.summary_file):
                backup_file = f"{self.summary_file}.backup"
                try:
                    import shutil
                    shutil.copy2(self.summary_file, backup_file)
                    self.logger.debug(f"å·²æ›´æ–°æ‘˜è¦æ–‡ä»¶å¤‡ä»½: {backup_file}")
                except Exception as e:
                    self.logger.debug(f"æ— æ³•æ›´æ–°æ‘˜è¦æ–‡ä»¶å¤‡ä»½: {e}")
            
            # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿çº¿ç¨‹å®‰å…¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(self, 'save_lock'):
                with self.save_lock:
                    # åŸå­æ€§å†™å…¥æ–‡ä»¶
                    temp_file = f"{self.summary_file}.tmp"
                    
                    with open(temp_file, 'w', encoding='utf-8') as f:
                        # å†™å…¥æ•°æ®
                        json.dump(self.summaries, f, ensure_ascii=False, indent=2)
                        f.flush()
                    
                    # åŸå­æ€§é‡å‘½ååˆ°ç›®æ ‡æ–‡ä»¶
                    os.replace(temp_file, self.summary_file)
            else:
                # æ— é”ç‰ˆæœ¬ï¼ˆå‘åå…¼å®¹ï¼‰
                # åŸå­æ€§å†™å…¥æ–‡ä»¶
                temp_file = f"{self.summary_file}.tmp"
                
                with open(temp_file, 'w', encoding='utf-8') as f:
                    # å†™å…¥æ•°æ®
                    json.dump(self.summaries, f, ensure_ascii=False, indent=2)
                    f.flush()
                
                # åŸå­æ€§é‡å‘½ååˆ°ç›®æ ‡æ–‡ä»¶
                os.replace(temp_file, self.summary_file)
            
            self.logger.debug(f"[ä¿å­˜] æ‘˜è¦æ–‡ä»¶å·²æ›´æ–°: {len(self.summaries)}æ¡è®°å½•")
            return True
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ‘˜è¦æ–‡ä»¶å¤±è´¥: {e}")
            self.logger.error(f"æ‘˜è¦åˆ—è¡¨ç±»å‹: {type(self.summaries)}")
            self.logger.error(f"æ‘˜è¦åˆ—è¡¨é•¿åº¦: {len(self.summaries)}")
            
            # å°è¯•ä¿å­˜é”™è¯¯æŠ¥å‘Š
            try:
                error_file = f"{self.summary_file}.error" if self.summary_file else None
                if error_file:
                    with open(error_file, 'w', encoding='utf-8') as f:
                        json.dump({
                            'error': str(e),
                            'timestamp': datetime.now().isoformat(),
                            'summaries_count': len(self.summaries),
                            'summaries_type': str(type(self.summaries))
                        }, f, ensure_ascii=False, indent=2)
            except:
                pass
            return False
    
    def generate_excel_report(self) -> bool:
        """ç”ŸæˆExcelæŠ¥å‘Š"""
        try:
            if not self.output_dir or not self.project_name:
                return False
            
            excel_file = os.path.join(self.output_dir, f'{self.project_name}_analyzed_papers.xlsx')
            
            # ç”ŸæˆExcelæŠ¥å‘Š
            success = generate_excel_report(self)
            
            if success:
                self.logger.success(f"ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {excel_file}")
                return True
            else:
                self.logger.error("ExcelæŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"ç”ŸæˆExcelæŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def generate_failure_report(self) -> bool:
        """ç”Ÿæˆå¤±è´¥æŠ¥å‘Š"""
        try:
            if not self.output_dir or not self.project_name:
                return False
            
            failure_report_file = os.path.join(self.output_dir, f'{self.project_name}_failed_papers_report.txt')
            
            # ç”Ÿæˆå¤±è´¥æŠ¥å‘Š
            success = generate_failure_report(self)
            
            if success:
                self.logger.success(f"å¤±è´¥æŠ¥å‘Šå·²ç”Ÿæˆ: {failure_report_file}")
                return True
            else:
                self.logger.error("å¤±è´¥æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¤±è´¥æŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def generate_retry_zotero_report(self) -> bool:
        """ç”ŸæˆZoteroé‡è·‘æŠ¥å‘Šï¼ˆä»…Zoteroæ¨¡å¼ï¼‰"""
        try:
            if self.mode != "zotero":
                return True  # ç›´æ¥æ¨¡å¼ä¸‹ä¸éœ€è¦ç”Ÿæˆé‡è·‘æŠ¥å‘Š
            
            if not self.output_dir or not self.project_name:
                return False
            
            # ç±»å‹å®ˆå«ï¼šç¡®ä¿output_dirå’Œproject_nameä¸æ˜¯None
            assert self.output_dir is not None and self.project_name is not None
            
            retry_report_file = os.path.join(self.output_dir, f'{self.project_name}_zotero_report_for_retry.txt')
            
            # ç”Ÿæˆé‡è·‘æŠ¥å‘Š
            success = generate_retry_zotero_report(self)
            
            if success:
                self.logger.success(f"é‡è·‘æŠ¥å‘Šå·²ç”Ÿæˆ: {retry_report_file}")
                return True
            else:
                self.logger.error("é‡è·‘æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                return False
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆé‡è·‘æŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def process_all_papers(self) -> bool:
        """å¤„ç†æ‰€æœ‰è®ºæ–‡ï¼ˆå¹¶å‘å¤„ç†ç‰ˆæœ¬ï¼‰"""
        try:
            if not self.papers:
                self.logger.error("æ²¡æœ‰è®ºæ–‡éœ€è¦å¤„ç†")
                return False
            
            total_papers = len(self.papers)
            self.logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {total_papers} ç¯‡è®ºæ–‡")
            
            # ç¡®å®šæœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
            performance_config: Dict[str, str] = self.config.get('Performance', {}) if self.config else {}
            max_workers = int(performance_config.get('max_workers', 3))
            self.logger.info(f"ä½¿ç”¨ {max_workers} ä¸ªå·¥ä½œçº¿ç¨‹")
            
            # åˆ›å»ºæ–‡ä»¶ç´¢å¼•ï¼ˆZoteroæ¨¡å¼ï¼‰
            file_index: Optional[FileIndex] = None
            if self.mode == "zotero":
                paths_config: Dict[str, str] = self.config.get('Paths', {}) if self.config else {}
                library_path: str = paths_config.get('library_path', '')
                if library_path:
                    self.logger.info("æ­£åœ¨åˆ›å»ºæ–‡ä»¶ç´¢å¼•...")
                    file_index = create_file_index(library_path)
                    self.logger.success(f"æ–‡ä»¶ç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(file_index)} ä¸ªæ–‡ä»¶")
            
            # ç¡®å®šéœ€è¦å¤„ç†çš„è®ºæ–‡ï¼ˆè·³è¿‡å·²å¤„ç†çš„ï¼‰
            papers_to_process: List[Tuple[int, 'PaperInfo']] = []
            skipped_count = 0
            
            for i, paper in enumerate(self.papers):
                paper_key = LiteratureReviewGenerator.get_paper_key(paper)  # type: ignore
                if paper_key in self._checkpoint_processed_papers or paper_key in self._checkpoint_failed_papers:
                    skipped_count += 1
                    continue
                papers_to_process.append((i, paper))
            
            self.logger.info(f"éœ€è¦å¤„ç†: {len(papers_to_process)}ç¯‡è®ºæ–‡ï¼Œè·³è¿‡: {skipped_count}ç¯‡è®ºæ–‡")
            
            if not papers_to_process:
                self.logger.success("æ‰€æœ‰è®ºæ–‡éƒ½å·²å¤„ç†å®Œæˆ")
                return True
            
            # é‡ç½®è®¡æ•°å™¨
            self.reset_counters()
            
            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = tqdm(total=len(papers_to_process), desc="[é˜¶æ®µä¸€] æ­£åœ¨åˆ†ææ–‡çŒ®")
            
            # åˆ›å»ºçº¿ç¨‹æ± å¹¶æäº¤ä»»åŠ¡
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_paper: Dict[concurrent.futures.Future['ProcessingResult | None'], Tuple[int, 'PaperInfo']] = {
                    executor.submit(self.process_paper, paper, i, file_index, total_papers): (i, paper)
                    for i, paper in papers_to_process
                }
                
                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                for future in concurrent.futures.as_completed(future_to_paper):
                    _, paper = future_to_paper[future]
                    paper_key = LiteratureReviewGenerator.get_paper_key(paper)  # type: ignore
                    
                    try:
                        result = future.result()
                        
                        if result and result.get('status') == 'success':
                            # å¤„ç†æˆåŠŸ
                            with self.save_lock:
                                self.summaries.append(result)
                                self._checkpoint_processed_papers.add(paper_key)
                            
                            # çº¿ç¨‹å®‰å…¨åœ°å¢åŠ è®¡æ•°å™¨
                            with self.save_lock:
                                self.processed_count.increment()
                            
                            # æ›´æ–°è¿›åº¦æ¡
                            progress_bar.update(1)
                            # æ›´æ–°è¿›åº¦æ¡çš„åç¼€ä¿¡æ¯
                            progress_bar.set_postfix_str(f"æˆåŠŸ: {self.processed_count.value}, å¤±è´¥: {self.failed_count.value}")
                        else:
                            # å¤„ç†å¤±è´¥
                            failure_reason = result.get('failure_reason') or 'æœªçŸ¥é”™è¯¯' if result else 'å¤„ç†è¿”å›ç©ºç»“æœ'
                            if not isinstance(failure_reason, str):  # type: ignore
                                failure_reason = 'æœªçŸ¥é”™è¯¯'
                            failed_paper = result.get('paper_info', paper) if result else paper
                            
                            self.failed_papers.append({  # type: ignore
                                    'paper_info': failed_paper,
                                    'failure_reason': failure_reason
                                })
                                # æ›´æ–°èº«ä»½åŸºæ–­ç‚¹è·Ÿè¸ª
                            self._checkpoint_failed_papers.add(paper_key)
                            
                            # çº¿ç¨‹å®‰å…¨åœ°å¢åŠ è®¡æ•°å™¨
                            with self.save_lock:
                                self.failed_count.increment()
                            
                            # æ›´æ–°è¿›åº¦æ¡
                            progress_bar.update(1)
                            # æ›´æ–°è¿›åº¦æ¡çš„åç¼€ä¿¡æ¯
                            progress_bar.set_postfix_str(f"æˆåŠŸ: {self.processed_count.value}, å¤±è´¥: {self.failed_count.value}")
                        
                        # æ¯å®Œæˆä¸€ä¸ªä»»åŠ¡å°±ç«‹å³ä¿å­˜æ•°æ®ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
                        if result and result.get('status') == 'success':
                            save_result = self.save_summaries()
                            if not save_result:
                                self.logger.error("âš ï¸ è­¦å‘Š: æ•°æ®ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç£ç›˜ç©ºé—´å’Œæƒé™")
                        else:
                            # å¤±è´¥çš„æƒ…å†µä¸‹å®šæœŸä¿å­˜ï¼ˆæ¯3ä¸ªå¤±è´¥ä¿å­˜ä¸€æ¬¡ï¼‰
                            if (self.processed_count.get_value() + self.failed_count.get_value()) % 3 == 0:
                                self.save_summaries()
                                self.save_checkpoint()
                        
                    except Exception as e:
                        # ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸
                        failure_reason = f"å¤„ç†è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}"
                        
                        with self.save_lock:
                            self.failed_papers.append({  # type: ignore
                                'paper_info': paper,
                                'failure_reason': failure_reason
                            })
                            # æ›´æ–°èº«ä»½åŸºæ–­ç‚¹è·Ÿè¸ª
                            self._checkpoint_failed_papers.add(paper_key)
                        
                        # çº¿ç¨‹å®‰å…¨åœ°å¢åŠ è®¡æ•°å™¨
                        with self.save_lock:
                            self.failed_count.increment()
                        
                        self.logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
                        self.logger.error(f"å¤±è´¥: {self.processed_count.value}æˆåŠŸ, {self.failed_count.value}å¤±è´¥ - {failure_reason}")
                        
                        # å¼‚å¸¸æƒ…å†µä¸‹ç«‹å³ä¿å­˜ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
                        save_result = self.save_summaries()
                        if not save_result:
                            self.logger.error("âš ï¸ è­¦å‘Š: å¼‚å¸¸æƒ…å†µä¸‹æ•°æ®ä¿å­˜å¤±è´¥")
                        self.save_checkpoint()
            
            # æœ€ç»ˆä¿å­˜æ‰€æœ‰æ•°æ®
            self.save_summaries()
            self.save_checkpoint()
            
            self.logger.success("\nå¹¶å‘å¤„ç†å®Œæˆï¼")
            self.logger.info(f"æ€»æ–‡çŒ®æ•°: {total_papers}")
            self.logger.info(f"æœ¬æ¬¡å¤„ç†: {len(papers_to_process)}ç¯‡")
            self.logger.info(f"è·³è¿‡å·²å¤„ç†: {skipped_count}ç¯‡")
            self.logger.info(f"æˆåŠŸå¤„ç†: {self.processed_count.value}")
            self.logger.info(f"å¤±è´¥: {self.failed_count.value}")
            self.logger.info(f"æ‘˜è¦æ–‡ä»¶: {self.summary_file}")
            
            # è‡ªåŠ¨é‡è¯•å¾ªç¯ - ç¬¬ä¸€é˜¶æ®µæœ«å°¾
            if self.failed_papers:
                self.logger.warning(f"æœ‰{len(self.failed_papers)}ç¯‡è®ºæ–‡å¤„ç†å¤±è´¥ï¼Œå¯åŠ¨è‡ªåŠ¨é‡è¯•å¾ªç¯...")
                
                # è¯»å–é‡è¯•é…ç½®
                retry_config: Dict[str, str] = self.config.get('Retry_Settings', {}) if self.config else {}
                max_retry_rounds: int = int(retry_config.get('max_retry_rounds', 2))
                base_retry_delay: int = int(retry_config.get('base_retry_delay', 30))
                max_retry_delay: int = int(retry_config.get('max_retry_delay', 120))
                
                self.logger.info(f"ğŸ”„ é‡è¯•é…ç½®: æœ€å¤§é‡è¯•è½®æ•°={max_retry_rounds}, åŸºç¡€é—´éš”={base_retry_delay}ç§’, æœ€å¤§é—´éš”={max_retry_delay}ç§’")
                
                # å®šä¹‰å¯é‡è¯•çš„å¤±è´¥ç±»å‹å…³é”®è¯
                retriable_keywords = ['api', 'network', 'http', 'timeout', '500', '502', '503', '504', '429', 'è¿æ¥', 'è¶…æ—¶', 'é”™è¯¯', 'å¤±è´¥']
                
                # åˆ†ç¦»å¯é‡è¯•å’Œæ°¸ä¹…å¤±è´¥çš„è®ºæ–‡
                retriable_failures: List['FailedPaper'] = []
                permanent_failures: List['FailedPaper'] = []
                
                for failed_item in self.failed_papers:
                    failure_reason: str = failed_item.get('failure_reason', '').lower()
                    paper_info: Dict[str, Any] = failed_item.get('paper_info', {})  # type: ignore
                    
                    # æ£€æŸ¥å¤±è´¥åŸå› æ˜¯å¦åŒ…å«å¯é‡è¯•å…³é”®è¯
                    is_retriable = any(keyword in failure_reason for keyword in retriable_keywords)
                    
                    if is_retriable:
                        retriable_failures.append(failed_item)
                    else:
                        permanent_failures.append(failed_item)
                
                self.logger.info(f"å¯é‡è¯•å¤±è´¥è®ºæ–‡: {len(retriable_failures)}ç¯‡")
                self.logger.info(f"æ°¸ä¹…å¤±è´¥è®ºæ–‡: {len(permanent_failures)}ç¯‡")
                
                # æ‰§è¡Œè‡ªåŠ¨é‡è¯•ï¼ˆä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°ï¼‰
                for retry_round in range(1, max_retry_rounds + 1):
                    if not retriable_failures:
                        self.logger.info("æ²¡æœ‰å¯é‡è¯•çš„å¤±è´¥è®ºæ–‡ï¼Œç»“æŸé‡è¯•å¾ªç¯")
                        break
                    
                    # å¦‚æœä¸æ˜¯ç¬¬ä¸€è½®é‡è¯•ï¼Œæ·»åŠ å»¶è¿Ÿç­‰å¾…APIé™åˆ¶æ¢å¤
                    if retry_round > 1:
                        # ä½¿ç”¨é…ç½®çš„é‡è¯•é—´éš”ï¼Œæ”¯æŒä¸Šé™æ§åˆ¶
                        calculated_delay = retry_round * base_retry_delay
                        retry_delay = min(calculated_delay, max_retry_delay)
                        self.logger.info(f"ç¬¬ {retry_round-1} è½®é‡è¯•å¤±è´¥ï¼Œç­‰å¾… {retry_delay} ç§’è®©APIé™åˆ¶æ¢å¤...")
                        self.logger.info(f"â° é—´éš”è®¡ç®—: {retry_round} Ã— {base_retry_delay} = {calculated_delay}ç§’ï¼Œå·²é™åˆ¶ä¸Šé™ä¸º {max_retry_delay}ç§’")
                        self.logger.info("â³ ç­‰å¾…ä¸­... è¿™æœ‰åŠ©äºé¿å…APIé¢‘ç‡é™åˆ¶ï¼Œæé«˜é‡è¯•æˆåŠŸç‡")
                        
                        # æ˜¾ç¤ºå€’è®¡æ—¶
                        for i in range(retry_delay, 0, -5):
                            if i > 5:
                                self.logger.info(f"â° å‰©ä½™ç­‰å¾…æ—¶é—´: {i} ç§’...")
                                time.sleep(5)
                            else:
                                break
                        time.sleep(retry_delay % 5)  # å®Œæˆå‰©ä½™ç­‰å¾…æ—¶é—´
                        self.logger.info("âœ… ç­‰å¾…å®Œæˆï¼Œå¼€å§‹é‡è¯•...")
                    
                    self.logger.info(f"æ­£åœ¨å¯¹ {len(retriable_failures)} ç¯‡å¤±è´¥æ–‡çŒ®è¿›è¡Œç¬¬ {retry_round} è½®è‡ªåŠ¨é‡è¯•...")
                    
                    # å‡†å¤‡é‡è¯•è®ºæ–‡æ•°æ®
                    retry_papers: List[Tuple[int, Dict[str, Any]]] = []
                    retry_indices: List[int] = []
                    for failed_item in retriable_failures:
                        paper_info: Dict[str, Any] = failed_item.get('paper_info', {})  # type: ignore
                        # æ‰¾åˆ°åŸå§‹è®ºæ–‡ç´¢å¼•

                        for i, original_paper in enumerate(self.papers):

                            if LiteratureReviewGenerator.get_paper_key(original_paper) == LiteratureReviewGenerator.get_paper_key(paper_info):
                                # è®¡ç®—è®ºæ–‡key
                                paper_key = LiteratureReviewGenerator.get_paper_key(original_paper)
                                # å…³é”®ä¿®å¤ï¼šä»å¤±è´¥é›†åˆä¸­ç§»é™¤ï¼Œé¿å…è¢«process_paperè·³è¿‡
                                if paper_key in self._checkpoint_failed_papers:
                                    self._checkpoint_failed_papers.discard(paper_key)
                                    self.logger.info(f"å·²ä»å¤±è´¥é›†åˆä¸­ç§»é™¤è®ºæ–‡ä»¥ä¾¿é‡è¯•: {original_paper.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                                
                                retry_papers.append((i, original_paper))  # type: ignore  # ä½¿ç”¨original_paperè€Œä¸æ˜¯paper_info
                                retry_indices.append(i)
                                break
                    
                    if not retry_papers:
                        self.logger.warning("æ— æ³•æ‰¾åˆ°é‡è¯•è®ºæ–‡çš„åŸå§‹ç´¢å¼•ï¼Œç»“æŸé‡è¯•")
                        break
                    
                    # é‡ç½®å½“å‰è½®æ¬¡çš„å¤±è´¥åˆ—è¡¨
                    current_round_failures: List[Dict[str, Any]] = []
                    
                    # åˆ›å»ºçº¿ç¨‹æ± è¿›è¡Œé‡è¯•å¤„ç†
                    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as retry_executor:
                        retry_futures: Dict[concurrent.futures.Future['ProcessingResult | None'], Tuple[Dict[str, Any], int]] = {}
                        for original_index, paper in retry_papers:
                            future = retry_executor.submit(self.process_paper, paper, original_index, file_index, total_papers)  # type: ignore
                            retry_futures[future] = (paper, original_index)
                        
                        # å¤„ç†é‡è¯•ç»“æœ
                        retry_progress_bar = tqdm(concurrent.futures.as_completed(retry_futures), 
                                                total=len(retry_papers), desc=f"[é‡è¯•ç¬¬{retry_round}è½®] æ­£åœ¨é‡è¯•æ–‡çŒ®")
                        
                        for future in retry_progress_bar:
                            paper, original_index = retry_futures[future]
                            paper_key = LiteratureReviewGenerator.get_paper_key(paper)  # type: ignore
                            
                            try:
                                result = future.result()
                                if result and result.get('status') == 'success':
                                    # é‡è¯•æˆåŠŸï¼Œæ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                                    with self.save_lock:
                                        self.summaries.append(result)
                                        self._checkpoint_processed_papers.add(paper_key)
                                        # ä»å¤±è´¥é›†åˆä¸­ç§»é™¤ï¼Œä¿æŒçŠ¶æ€ä¸€è‡´æ€§
                                        self._checkpoint_failed_papers.discard(paper_key)
                                        # ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤
                                        self.failed_papers = [fp for fp in self.failed_papers  # type: ignore
                                                          if LiteratureReviewGenerator.get_paper_key(fp.get('paper_info', {})) != paper_key]
                                    
                                    with self.save_lock:
                                        self.processed_count.increment()
                                    
                                    retry_progress_bar.update(1)
                                    retry_progress_bar.set_postfix_str(f"æˆåŠŸ: {self.processed_count.value}, å¤±è´¥: {self.failed_count.value}")
                                else:
                                    # é‡è¯•ä»ç„¶å¤±è´¥
                                    failure_reason = result.get('failure_reason', 'é‡è¯•å¤±è´¥') if result else 'é‡è¯•è¿”å›ç©ºç»“æœ'
                                    current_round_failures.append({
                                        'paper_info': paper,
                                        'failure_reason': failure_reason
                                    })
                                    
                                    retry_progress_bar.update(1)
                                    retry_progress_bar.set_postfix_str(f"æˆåŠŸ: {self.processed_count.value}, å¤±è´¥: {self.failed_count.value}")
                                
                                # é‡è¯•æˆåŠŸæ—¶ç«‹å³ä¿å­˜ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
                                if result and result.get('status') == 'success':
                                    save_result = self.save_summaries()
                                    if not save_result:
                                        self.logger.error("âš ï¸ è­¦å‘Š: é‡è¯•æˆåŠŸæ•°æ®ä¿å­˜å¤±è´¥")
                                else:
                                    # å¤±è´¥æƒ…å†µä¸‹å®šæœŸä¿å­˜
                                    if (self.processed_count.get_value() + self.failed_count.get_value()) % 3 == 0:
                                        self.save_summaries()
                                        self.save_checkpoint()
                                
                            except Exception as e:
                                # é‡è¯•å¼‚å¸¸
                                failure_reason = f"é‡è¯•è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {str(e)}"
                                current_round_failures.append({
                                    'paper_info': paper,
                                    'failure_reason': failure_reason
                                })
                                
                                self.logger.error(f"é‡è¯•ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
                                # é‡è¯•å¼‚å¸¸æ—¶ç«‹å³ä¿å­˜ï¼Œç¡®ä¿æ•°æ®ä¸ä¸¢å¤±
                                save_result = self.save_summaries()
                                if not save_result:
                                    self.logger.error("âš ï¸ è­¦å‘Š: é‡è¯•å¼‚å¸¸æ—¶æ•°æ®ä¿å­˜å¤±è´¥")
                                self.save_checkpoint()
                    
                    # æ›´æ–°é‡è¯•å¤±è´¥åˆ—è¡¨
                    retriable_failures = current_round_failures  # type: ignore
                    
                    if current_round_failures:
                        self.logger.warning(f"ç¬¬ {retry_round} è½®é‡è¯•åï¼Œä»æœ‰ {len(current_round_failures)} ç¯‡è®ºæ–‡å¤±è´¥")
                    else:
                        self.logger.success(f"ç¬¬ {retry_round} è½®é‡è¯•æˆåŠŸï¼Œæ‰€æœ‰è®ºæ–‡å¤„ç†å®Œæˆï¼")
                        break
                
                # åˆå¹¶æœ€ç»ˆå¤±è´¥åˆ—è¡¨
                final_failed_papers = permanent_failures + retriable_failures
                self.failed_papers = final_failed_papers  # type: ignore
                
                # æ›´æ–°å¤±è´¥è®¡æ•°
                self.failed_count.set(len(self.failed_papers))
                
                self.logger.info(f"ğŸ”„ è‡ªåŠ¨é‡è¯•å¾ªç¯å®Œæˆï¼")
                self.logger.info(f"ğŸ“Š ä½¿ç”¨é…ç½®: {max_retry_rounds}è½®é‡è¯•ï¼ŒåŸºç¡€é—´éš”{base_retry_delay}ç§’ï¼Œä¸Šé™{max_retry_delay}ç§’")
                self.logger.info(f"ğŸ“ˆ æœ€ç»ˆå¤±è´¥è®ºæ–‡æ•°: {len(self.failed_papers)}ç¯‡")
            
            # ç”Ÿæˆå¤±è´¥æŠ¥å‘Š
            if self.failed_papers:
                self.logger.warning(f"æœ‰{len(self.failed_papers)}ç¯‡è®ºæ–‡å¤„ç†å¤±è´¥ï¼Œå°†ç”Ÿæˆå¤±è´¥æŠ¥å‘Š")
            
            # æœ€ç»ˆä¿å­˜æ‰€æœ‰æ•°æ®
            self.save_summaries()
            self.save_checkpoint()
            
            return True
            
        except KeyboardInterrupt:
            self.logger.error("\n\nç”¨æˆ·ä¸­æ–­å¤„ç†")
            self.logger.info(f"å·²å¤„ç†: {self.processed_count.value}ç¯‡æ–‡çŒ®ï¼Œå¤±è´¥: {self.failed_count.value}ç¯‡")
            self.save_summaries()
            self.save_checkpoint()
            return False
        except Exception as e:
            self.logger.error(f"å¹¶å‘å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            self.logger.info(f"å·²å¤„ç†: {self.processed_count.value}ç¯‡æ–‡çŒ®ï¼Œå¤±è´¥: {self.failed_count.value}ç¯‡")
            self.save_summaries()
            self.save_checkpoint()
            return False

    def save_checkpoint(self) -> bool:
        """ä¿å­˜åŸºäºèº«ä»½çš„æ–­ç‚¹æ–‡ä»¶ - å§”æ‰˜ç»™CheckpointManager"""
        return self.checkpoint_manager.save_checkpoint(self)

    def load_checkpoint(self) -> bool:
        """åŠ è½½åŸºäºèº«ä»½çš„æ–­ç‚¹æ–‡ä»¶ - å§”æ‰˜ç»™CheckpointManager"""
        return self.checkpoint_manager.load_checkpoint(self)

    def run_stage_one(self, override_zotero_report_path: Optional[str] = None) -> bool:
        """é˜¶æ®µä¸€ï¼šæ–‡çŒ®è§£æä¸AIæ‘˜è¦ç”Ÿæˆï¼ˆåŸºäºèº«ä»½çš„æ–­ç‚¹ç»­ä¼ ç‰ˆæœ¬ï¼‰"""
        self.logger.info("=" * 60 + "\næ–‡çŒ®ç»¼è¿°è‡ªåŠ¨ç”Ÿæˆå™¨ - é˜¶æ®µä¸€ï¼ˆèº«ä»½åŸºæ–­ç‚¹ç»­ä¼ ï¼‰\n" + "=" * 60)
        try:
            # åŠ è½½é…ç½®æ–‡ä»¶
            if not self.load_configuration(): 
                return False
            # ç¡®ä¿é…ç½®å·²æ­£ç¡®åŠ è½½åˆ°å®ä¾‹å˜é‡
            if not self.config:
                self.logger.error("é…ç½®æœªæ­£ç¡®åŠ è½½")
                return False
            
            # å¦‚æœæä¾›äº†é‡å†™çš„ZoteroæŠ¥å‘Šè·¯å¾„ï¼Œåœ¨æ­¤å¤„åº”ç”¨
            if override_zotero_report_path:
                self.logger.info(f"[é‡è·‘æ¨¡å¼] å·²å°†æ–‡çŒ®æ¥æºå¼ºåˆ¶æŒ‡å®šä¸º -> {override_zotero_report_path}")
            
            if not self.setup_output_directory(): 
                return False
            
            # åŠ è½½åŸºäºèº«ä»½çš„æ–­ç‚¹æ–‡ä»¶
            checkpoint_loaded = self.load_checkpoint()
            if not checkpoint_loaded:
                self.logger.info("[å…¨æ–°å¼€å§‹] æœªæ‰¾åˆ°æœ‰æ•ˆæ–­ç‚¹ï¼Œå°†å¼€å§‹å…¨æ–°å¤„ç†")
                self.reset_counters()
                # åˆå§‹åŒ–æ–­ç‚¹è·Ÿè¸ªå˜é‡
                self._checkpoint_processed_papers = set()
                self._checkpoint_failed_papers = set()
            else:
                self.logger.info("[æ–­ç‚¹ç»­ä¼ ] å·²åŠ è½½å¤„ç†è¿›åº¦ï¼Œå°†è·³è¿‡å·²å¤„ç†çš„è®ºæ–‡")
            
            # åŠ è½½ç°æœ‰æ‘˜è¦ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
            self.load_existing_summaries()
            
            # é€»è¾‘åˆ†å‰ï¼šæ ¹æ®è¿è¡Œæ¨¡å¼é€‰æ‹©æ•°æ®æº
            if self.mode == "zotero":
                # Zoteroæ¨¡å¼ï¼šè§£æZoteroæŠ¥å‘Šï¼Œä¼ é€’è¦†ç›–è·¯å¾„
                if not self.parse_zotero_report(override_zotero_report_path): 
                    return False
            else:
                # ç›´æ¥æ¨¡å¼ï¼šæ‰«æPDFæ–‡ä»¶å¤¹
                if not self.scan_pdf_folder(): 
                    return False
            
            # éªŒè¯è®ºæ–‡æ•°æ®å®Œæ•´æ€§
            if not self.papers:
                self.logger.error("æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡æ•°æ®")
                return False
            
            self.logger.info(f"è®ºæ–‡æ•°æ®åŠ è½½å®Œæˆ: {len(self.papers)}ç¯‡è®ºæ–‡")
            
            # å¤„ç†æ‰€æœ‰è®ºæ–‡ï¼ˆä½¿ç”¨èº«ä»½åŸºæ–­ç‚¹ç»­ä¼ ï¼‰
            success = self.process_all_papers()
            
            # å¦‚æœå¤„ç†æˆåŠŸï¼Œç”ŸæˆæŠ¥å‘Š
            if success:
                # æ¸…é™¤æ–­ç‚¹æ–‡ä»¶ï¼ˆè¡¨ç¤ºå…¨éƒ¨å®Œæˆï¼‰
                if self.output_dir and self.project_name:
                    # ç±»å‹å®ˆå«ï¼šç¡®ä¿output_dirå’Œproject_nameä¸æ˜¯None
                    assert self.output_dir is not None and self.project_name is not None
                    checkpoint_file = os.path.join(self.output_dir, f'{self.project_name}_checkpoint.json')
                    if os.path.exists(checkpoint_file):
                        try:
                            os.remove(checkpoint_file)
                            self.logger.info("å·²æ¸…é™¤æ–­ç‚¹æ–‡ä»¶ï¼Œæ‰€æœ‰è®ºæ–‡å¤„ç†å®Œæˆ")
                        except Exception as e:
                            self.logger.warning(f"æ— æ³•æ¸…é™¤æ–­ç‚¹æ–‡ä»¶: {e}")
                
                # è°ƒç”¨ç»Ÿä¸€çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•
                self.generate_all_reports()
            
            return success
            
        except Exception as e:
            self.logger.error(f"é˜¶æ®µä¸€è¿è¡Œå¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿè¦ä¿å­˜æ–­ç‚¹
            self.save_checkpoint()
            return False

    
    



    def generate_all_reports(self) -> None:
        """ç”Ÿæˆæ‰€æœ‰åˆ†æé˜¶æ®µçš„æŠ¥å‘Š - å§”æ‰˜ç»™ReportingService"""
        self.reporting_service.generate_all_reports(self)
    
    def extract_section_title_from_outline(self, outline_content: str, section_number: int) -> Optional[str]:
        """ä»å¤§çº²å†…å®¹ä¸­æå–æŒ‡å®šç« èŠ‚çš„æ ‡é¢˜"""
        try:
            lines = outline_content.split('\n')
            current_section = 0
            
            for line in lines:
                # æŸ¥æ‰¾äºŒçº§æ ‡é¢˜ï¼ˆ##ï¼‰
                if line.startswith('## '):
                    current_section += 1
                    if current_section == section_number:
                        return line[3:].strip()
            
            return None
        except Exception as e:
            self.logger.error(f"æå–ç« èŠ‚æ ‡é¢˜å¤±è´¥: {e}")
            return None

    def create_literature_review_section(self, section_number: int, section_title: str, outline_content: str) -> bool:
        """åˆ›å»ºæ–‡çŒ®ç»¼è¿°çš„æŒ‡å®šç« èŠ‚å†…å®¹"""
        try:
            section_content = self.generate_review_section_content(section_title, outline_content)
            if not section_content:
                self.logger.error(f"ç¬¬{section_number}ç« å†…å®¹ç”Ÿæˆå¤±è´¥")
                return False
            
            # section_contentåº”è¯¥æ˜¯çº¯æ–‡æœ¬å­—ç¬¦ä¸²
            if not isinstance(section_content, str):  # type: ignore
                self.logger.warning("é¢„æœŸæ”¶åˆ°çº¯æ–‡æœ¬ï¼Œä½†æ”¶åˆ°å…¶ä»–æ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢...")
                section_text = str(section_content)
            else:
                section_text = section_content
            
            # ç”ŸæˆWordæ–‡æ¡£è·¯å¾„ï¼ˆæ·»åŠ é¡¹ç›®åç§°å‰ç¼€ï¼‰
            if not self.output_dir:
                self.logger.error("è¾“å‡ºç›®å½•æœªè®¾ç½®")
                return False
                
            if self.project_name:
                word_file = os.path.join(self.output_dir, f'{self.project_name}_literature_review.docx')
            else:
                word_file = os.path.join(self.output_dir, 'literature_review.docx')
            
            # å°†ç« èŠ‚å†…å®¹è¿½åŠ åˆ°Wordæ–‡æ¡£
            success = self.append_section_to_word_document(section_number, section_title, section_text, word_file)
            
            if success:
                self.logger.success(f"ç¬¬{section_number}ç« å·²è¿½åŠ åˆ°æ–‡çŒ®ç»¼è¿°: {word_file}")
                return True
            else:
                return False
                
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ–‡çŒ®ç»¼è¿°ç« èŠ‚å¤±è´¥: {e}")
            return False

    def generate_review_section_content(self, section_title: str, outline_content: str) -> Optional[str]:
        """ç”ŸæˆæŒ‡å®šç« èŠ‚çš„å†…å®¹ï¼ˆå¸¦æ™ºèƒ½ç»­å†™å¾ªç¯å’Œä¸Šä¸‹æ–‡ä¼˜åŒ–ï¼‰"""
        try:
            # ğŸ†• ä½¿ç”¨context_managerä¼˜åŒ–ä¸Šä¸‹æ–‡æ•°æ®
            self.logger.info("æ­£åœ¨ä¼˜åŒ–ç»¼è¿°ç”Ÿæˆä¸Šä¸‹æ–‡...")
            
            # ä¼˜åŒ–ä¸Šä¸‹æ–‡å¹¶æ™ºèƒ½æˆªæ–­
            # Gemini 3 Proæœ‰1M tokenä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨950000ä½œä¸ºå®‰å…¨é˜ˆå€¼ï¼ˆä»…åœ¨æœ€æç«¯æƒ…å†µä¸‹è§¦å‘æˆªæ–­ï¼‰
            optimized_context: str = optimize_context_for_synthesis(
                self.summaries, 
                outline_content, 
                max_tokens=950000
            )
            
            self.logger.info(f"ä¸Šä¸‹æ–‡ä¼˜åŒ–å®Œæˆï¼šåŸå§‹æ•°æ® -> ä¼˜åŒ–åæ ¼å¼")
            
            # ç›´æ¥ä½¿ç”¨ä¼˜åŒ–çš„promptæ–‡ä»¶
            with open('prompts/optimized_prompt_synthesize_section.txt', 'r', encoding='utf-8') as f:
                prompt_template = f.read()
            
            # æ›¿æ¢å ä½ç¬¦
            section_prompt: str = prompt_template.replace('{{SUMMARIES_JSON_ARRAY}}', optimized_context)
            section_prompt = section_prompt.replace('{{SECTION_TITLE}}', section_title)
            section_prompt = section_prompt.replace('{{REVIEW_OUTLINE}}', outline_content)

            self.logger.info(f"ç”Ÿæˆç»¼è¿°æç¤ºè¯: {len(section_prompt)}å­—ç¬¦")

            # æå–å†™ä½œå¼•æ“APIé…ç½®
            writer_config: Dict[str, Any] = (self.config or {}).get('Writer_API', {})  # type: ignore
            writer_api_config: APIConfig = {
                'api_key': writer_config.get('api_key') or '',  # type: ignore
                'model': writer_config.get('model') or '',  # type: ignore
                'api_base': writer_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
            }

            self.logger.info(f"æ­£åœ¨è°ƒç”¨å†™ä½œå¼•æ“ç”Ÿæˆç« èŠ‚å†…å®¹: {section_title}")

            # æ™ºèƒ½ç»­å†™å¾ªç¯å®ç°
            partial_section_content = ""  # å­˜å‚¨å·²ç”Ÿæˆçš„ç« èŠ‚å†…å®¹
            continuation_attempts = 0  # ç»­å†™è®¡æ•°å™¨
            max_continuation_attempts = 5  # æœ€å¤§ç»­å†™æ¬¡æ•°ï¼ˆå®‰å…¨ç†”æ–­ï¼‰

            while continuation_attempts <= max_continuation_attempts:
                if continuation_attempts == 0:
                    # é¦–æ¬¡è°ƒç”¨ï¼Œä½¿ç”¨ä¼˜åŒ–åçš„æç¤ºè¯
                    self.logger.info(f"[ç« èŠ‚ç”Ÿæˆ] é¦–æ¬¡è°ƒç”¨ç”Ÿæˆç« èŠ‚: {section_title}")
                    result = self._call_section_api_optimized(
                        section_prompt,
                        writer_api_config, 
                        is_continuation=False
                    )
                else:
                    # ç»­å†™è°ƒç”¨ï¼Œä½¿ç”¨ç»­å†™æç¤ºè¯
                    self.logger.info(f"[ç« èŠ‚ç»­å†™] ç¬¬{continuation_attempts}æ¬¡ç»­å†™: {section_title}")
                    result = self._call_section_api_optimized(
                        section_prompt,
                        writer_api_config, 
                        is_continuation=True,
                        partial_content=partial_section_content
                    )

                if not result:
                    self.logger.error(f"[ç« èŠ‚ç”Ÿæˆ] APIè°ƒç”¨å¤±è´¥ï¼Œç« èŠ‚ç”Ÿæˆä¸­æ–­")
                    return None

                # è§£æè¿”å›ç»“æœ

                section_content = result.get('content', '')  # type: ignore

                finish_reason = result.get('finish_reason', 'stop')  # type: ignore

                if not section_content or len(section_content.strip()) < 100:
                    self.logger.warning(f"[ç« èŠ‚ç”Ÿæˆ] è¿”å›å†…å®¹è¿‡çŸ­({len(section_content)}å­—ç¬¦)ï¼Œé‡è¯•...")
                    continuation_attempts += 1
                    continue

                # å°†æ–°å†…å®¹è¿½åŠ åˆ°å·²ç”Ÿæˆå†…å®¹ä¸­
                if continuation_attempts == 0:
                    partial_section_content = section_content
                else:
                    partial_section_content += section_content

                self.logger.success(f"[ç« èŠ‚ç”Ÿæˆ] æœ¬æ¬¡ç”Ÿæˆ {len(section_content)} å­—ç¬¦ï¼Œç´¯è®¡ {len(partial_section_content)} å­—ç¬¦")

                # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­ç»­å†™
                if finish_reason == 'stop':
                    self.logger.success(f"[ç« èŠ‚ç”Ÿæˆ] ç« èŠ‚ç”Ÿæˆå®Œæˆï¼Œæ— éœ€ç»­å†™")
                    return partial_section_content
                elif finish_reason == 'length':
                    self.logger.info(f"[ç« èŠ‚ç”Ÿæˆ] å†…å®¹è¢«æˆªæ–­ï¼Œå‡†å¤‡ç»­å†™...")
                    continuation_attempts += 1
                    if continuation_attempts > max_continuation_attempts:
                        self.logger.warning(f"[ç« èŠ‚ç”Ÿæˆ] è¾¾åˆ°æœ€å¤§ç»­å†™æ¬¡æ•°({max_continuation_attempts})ï¼Œè¿”å›éƒ¨åˆ†ç”Ÿæˆçš„å†…å®¹")
                        return partial_section_content
                else:
                    self.logger.warning(f"[ç« èŠ‚ç”Ÿæˆ] æœªçŸ¥çš„finish_reason: {finish_reason}ï¼Œå‡è®¾å®Œæˆ")
                    return partial_section_content

            # è¾¾åˆ°æœ€å¤§ç»­å†™æ¬¡æ•°
            self.logger.warning(f"[ç« èŠ‚ç”Ÿæˆ] è¾¾åˆ°æœ€å¤§ç»­å†™æ¬¡æ•°({max_continuation_attempts})ï¼Œè¿”å›éƒ¨åˆ†ç”Ÿæˆçš„å†…å®¹")
            return partial_section_content

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return None

    def _call_section_api(self, section_title: str, summaries_string: str, outline_string: str, 
                         writer_api_config: 'APIConfig', is_continuation: bool = False, 
                         partial_content: str = "") -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ç« èŠ‚ç”ŸæˆAPIçš„ç§æœ‰æ–¹æ³•"""
        try:
            # Determine system prompt
            try:
                with open('prompts/prompt_system_section.txt', 'r', encoding='utf-8') as f:
                    system_prompt = f.read()
                self.logger.success(f"åŠ è½½ç« èŠ‚ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿: {len(system_prompt)}å­—ç¬¦")
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½ç« èŠ‚ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æ–‡çŒ®ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„æ–‡çŒ®åˆ†æç»“æœå’Œå®Œæ•´å¤§çº²ï¼Œæ’°å†™æŒ‡å®šç« èŠ‚çš„æ­£æ–‡å†…å®¹ã€‚

è¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬æ ¼å¼çš„ç« èŠ‚æ­£æ–‡å†…å®¹
2. ä¸è¦åŒ…å«ç« èŠ‚æ ‡é¢˜
3. å†…å®¹éœ€è¦ä¸“ä¸šã€å®¢è§‚ã€å…¨é¢
4. é€‚å½“å¼•ç”¨å…·ä½“æ–‡çŒ®ä»¥æ”¯æŒè®ºç‚¹
5. è¯­è¨€é£æ ¼éœ€ä¸“ä¸šã€å­¦æœ¯
6. åªæ’°å†™æŒ‡å®šç« èŠ‚çš„å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–ç« èŠ‚"""

            # Determine final prompt
            if is_continuation:
                try:
                    with open('prompts/prompt_continue_section.txt', 'r', encoding='utf-8') as f:
                        section_prompt_template = f.read()
                    self.logger.success(f"åŠ è½½ç« èŠ‚ç»­å†™æç¤ºè¯æ¨¡æ¿: {len(section_prompt_template)}å­—ç¬¦")
                except Exception as e:
                    self.logger.warning(f"æ— æ³•åŠ è½½ç« èŠ‚ç»­å†™æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                    section_prompt_template = "ã€è§’è‰²ã€‘ä½ æ˜¯ä¸€ä½æ­£åœ¨æ’°å†™ç»¼è¿°ç‰¹å®šç« èŠ‚çš„å­¦è€…ï¼Œåˆšæ‰æ€è·¯è¢«æ‰“æ–­äº†ã€‚\nã€ä»»åŠ¡ã€‘è¯·ä½ ç»§ç»­å®Œæˆä¸€ä»½æœªå†™å®Œçš„ç« èŠ‚æ­£æ–‡ã€‚\n\nã€å…¨éƒ¨è®ºæ–‡åˆ†ææ•°æ®ã€‘\n{{SUMMARIES_JSON_ARRAY}}\n\nã€ç»¼è¿°å®Œæ•´å¤§çº²ã€‘\n{{REVIEW_OUTLINE}}\n\nã€å½“å‰éœ€è¦æ’°å†™çš„ç« èŠ‚æ ‡é¢˜ã€‘\n{{SECTION_TITLE}}\n\nã€å·²å®Œæˆçš„ç« èŠ‚è‰ç¨¿ã€‘\n{{PARTIAL_SECTION_CONTENT}}"

                final_prompt = section_prompt_template.replace('{{SUMMARIES_JSON_ARRAY}}', summaries_string)
                final_prompt = final_prompt.replace('{{REVIEW_OUTLINE}}', outline_string)
                final_prompt = final_prompt.replace('{{SECTION_TITLE}}', section_title)
                final_prompt = final_prompt.replace('{{PARTIAL_SECTION_CONTENT}}', partial_content)
            else:
                try:
                    with open('prompts/prompt_synthesize_section.txt', 'r', encoding='utf-8') as f:
                        section_prompt_template = f.read()
                    self.logger.success(f"åŠ è½½ç« èŠ‚æç¤ºè¯æ¨¡æ¿: {len(section_prompt_template)}å­—ç¬¦")
                except Exception as e:
                    self.logger.warning(f"æ— æ³•åŠ è½½ç« èŠ‚æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                    section_prompt_template = "åŸºäºä»¥ä¸‹æ–‡çŒ®æ‘˜è¦ä¿¡æ¯å’Œå¤§çº²ï¼Œè¯·æ’°å†™æŒ‡å®šç« èŠ‚çš„å†…å®¹ã€‚\n\nã€å…¨éƒ¨è®ºæ–‡åˆ†ææ•°æ®ã€‘\n{{SUMMARIES_JSON_ARRAY}}\n\nã€ç»¼è¿°å®Œæ•´å¤§çº²ã€‘\n{{REVIEW_OUTLINE}}\n\nã€å½“å‰éœ€è¦æ’°å†™çš„ç« èŠ‚æ ‡é¢˜ã€‘\n{{SECTION_TITLE}}"

                final_prompt = section_prompt_template.replace('{{SUMMARIES_JSON_ARRAY}}', summaries_string)
                final_prompt = final_prompt.replace('{{REVIEW_OUTLINE}}', outline_string)
                final_prompt = final_prompt.replace('{{SECTION_TITLE}}', section_title)

            self.logger.success(f"ç”Ÿæˆæœ€ç»ˆç« èŠ‚æç¤ºè¯: {len(final_prompt)}å­—ç¬¦")

            # Call unified AI API function
            ai_response = _call_ai_api(
                prompt=final_prompt,
                api_config=writer_api_config,
                system_prompt=system_prompt,
                max_tokens=6000,
                temperature=0.7,
                response_format="text" # Expecting plain text
            )

            if ai_response:
                # _call_ai_api returns content directly for text format
                return {
                    'content': ai_response,
                    'finish_reason': 'stop' # _call_ai_api doesn't return finish_reason for text, assume stop
                }
            else:
                self.logger.error(f"ç« èŠ‚å†…å®¹ç”Ÿæˆå¤±è´¥: _call_ai_api è¿”å›ç©ºå€¼")
                return None

        except Exception as e:
            self.logger.error(f"è°ƒç”¨ç« èŠ‚APIå¤±è´¥: {e}")
            return None

    def _call_section_api_optimized(self, section_prompt: str, writer_api_config: 'APIConfig', 
                                   is_continuation: bool = False, partial_content: str = "") -> Optional[Dict[str, Any]]:
        """ğŸ†• ä¼˜åŒ–çš„ç« èŠ‚ç”ŸæˆAPIè°ƒç”¨ï¼ˆä½¿ç”¨é¢„å¤„ç†çš„æç¤ºè¯ï¼‰"""
        try:
            # Determine system prompt
            try:
                with open('prompts/prompt_system_section.txt', 'r', encoding='utf-8') as f:
                    system_prompt = f.read()
                self.logger.success(f"åŠ è½½ç« èŠ‚ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿: {len(system_prompt)}å­—ç¬¦")
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½ç« èŠ‚ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æ–‡çŒ®ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„æ–‡çŒ®åˆ†æç»“æœå’Œå®Œæ•´å¤§çº²ï¼Œæ’°å†™æŒ‡å®šç« èŠ‚çš„æ­£æ–‡å†…å®¹ã€‚

è¦æ±‚ï¼š
1. æ·±åº¦ç»¼åˆä¸åŒå­¦è€…çš„è§‚ç‚¹ï¼Œå¯¹æ¯”å¼‚åŒ
2. æ¯ä¸ªè®ºç‚¹å¿…é¡»å¼•ç”¨è‡³å°‘1-2ç¯‡æ–‡çŒ®ï¼Œæ ¼å¼ä¸º(ä½œè€…, å¹´ä»½)
3. é€»è¾‘è¿è´¯ï¼Œæ®µè½é—´æœ‰è¿‡æ¸¡
4. é¿å…æµæ°´è´¦å¼å†™æ³•ï¼ŒæŒ‰ä¸»é¢˜ç»„ç»‡å†…å®¹"""

            # å¯¹äºç»­å†™è°ƒç”¨ï¼Œæ·»åŠ ç»­å†™æ ‡è®°
            if is_continuation and partial_content:
                continuation_prompt = f"""è¯·ç»§ç»­æ’°å†™ä¸Šæ–‡çš„ç« èŠ‚å†…å®¹ã€‚ä¸Šæ–‡å†…å®¹ï¼š
{partial_content}

è¯·ç»§ç»­ä¸Šæ–‡çš„å†…å®¹ï¼Œä¿æŒé€»è¾‘è¿è´¯ï¼Œç¡®ä¿ï¼š
1. ä¸ä¸Šæ–‡é£æ ¼ä¸€è‡´
2. å†…å®¹è‡ªç„¶è¡”æ¥
3. ç»§ç»­æ·±åŒ–ä¸»é¢˜åˆ†æ

ç»§ç»­å†…å®¹ï¼š"""
                final_prompt = f"{section_prompt}\n\n{continuation_prompt}"
            else:
                final_prompt = section_prompt

            self.logger.success(f"ç”Ÿæˆæœ€ç»ˆç« èŠ‚æç¤ºè¯: {len(final_prompt)}å­—ç¬¦")

            # Call unified AI API function
            ai_response = _call_ai_api(
                prompt=final_prompt,
                api_config=writer_api_config,
                system_prompt=system_prompt,
                max_tokens=6000,
                temperature=0.7,
                response_format="text" # Expecting plain text
            )

            if ai_response:
                # _call_ai_api returns content directly for text format
                return {
                    'content': ai_response,
                    'finish_reason': 'stop' # _call_ai_api doesn't return finish_reason for text, assume stop
                }
            else:
                self.logger.error(f"ç« èŠ‚å†…å®¹ç”Ÿæˆå¤±è´¥: _call_ai_api è¿”å›ç©ºå€¼")
                return None

        except Exception as e:
            self.logger.error(f"è°ƒç”¨ä¼˜åŒ–ç« èŠ‚APIå¤±è´¥: {e}")
            return None

    def append_section_to_word_document(self, section_number: int, section_title: str, section_text: str, word_file: str) -> bool:
        """å°†ç« èŠ‚å†…å®¹è¿½åŠ åˆ°Wordæ–‡æ¡£ï¼ˆå¸¦æ ·å¼é…ç½®ï¼‰"""
        return append_section_to_word_document(self, section_number, section_title, section_text, word_file)

    def generate_full_review_from_outline(self) -> bool:
        """ä»å¤§çº²ç”Ÿæˆå®Œæ•´æ–‡çŒ®ç»¼è¿°"""
        self.logger.info("=" * 60 + "\næ–‡çŒ®ç»¼è¿°è‡ªåŠ¨ç”Ÿæˆå™¨ - é˜¶æ®µäºŒï¼šç»¼è¿°ç”Ÿæˆ\n" + "=" * 60)
        try:
            if not self.load_configuration(): 
                return False
            if not self.setup_output_directory(): 
                return False
            if not self.load_existing_summaries():
                self.logger.error("æ— æ³•åŠ è½½æ‘˜è¦æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œé˜¶æ®µä¸€")
                return False
            if not self.summaries:
                self.logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ‘˜è¦ï¼Œè¯·å…ˆè¿è¡Œé˜¶æ®µä¸€")
                return False
            
            writer_config: Dict[str, Any] = (self.config or {}).get('Writer_API', {})  # type: ignore
            if 'dummy' in (writer_config.get('api_key') or ''):  # type: ignore
                if not self.output_dir:
                    self.logger.error("è¾“å‡ºç›®å½•æœªè®¾ç½®")
                    return False
                    
                if self.project_name:
                    word_file = os.path.join(self.output_dir, f'{self.project_name}_literature_review.docx')
                else:
                    word_file = os.path.join(self.output_dir, 'literature_review.docx')
                doc = Document()  # type: ignore
                doc.add_heading('Dummy Literature Review', 0)
                doc.add_paragraph('This is a dummy literature review.')
                doc.save(word_file)
                self.logger.success(f"Dummy review saved to {word_file}")
                return True
            
            # åŠ è½½å¤§çº²æ–‡ä»¶
            if not self.output_dir:
                self.logger.error("è¾“å‡ºç›®å½•æœªè®¾ç½®")
                return False
                
            if self.project_name:
                outline_file = os.path.join(self.output_dir, f'{self.project_name}_literature_review_outline.md')  # type: ignore
                review_checkpoint_file = os.path.join(self.output_dir, f'{self.project_name}_review_checkpoint.json')
                word_file = os.path.join(self.output_dir, f'{self.project_name}_literature_review.docx')
            else:
                outline_file = os.path.join(self.output_dir, 'literature_review_outline.md')
                review_checkpoint_file = os.path.join(self.output_dir, 'review_checkpoint.json')
                word_file = os.path.join(self.output_dir, 'literature_review.docx')
            
            if not os.path.exists(outline_file):
                self.logger.error(f"å¤§çº²æ–‡ä»¶ä¸å­˜åœ¨: {outline_file}ï¼Œè¯·å…ˆè¿è¡Œ --generate-outline ç”Ÿæˆå¤§çº²")
                return False
            
            with open(outline_file, 'r', encoding='utf-8') as f:
                outline_content = f.read()
            
            # è§£æå¤§çº²ä¸­çš„æ‰€æœ‰ç« èŠ‚
            import re
            section_matches = re.findall(r"^##\s*(\d+)\.\s*(.*)", outline_content, re.MULTILINE)
            
            if not section_matches:
                self.logger.error("å¤§çº²ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç« èŠ‚ï¼ˆæ ¼å¼ï¼š## æ•°å­—. æ ‡é¢˜ï¼‰")
                return False
            
            self.logger.info(f"ä»å¤§çº²ä¸­è§£æåˆ° {len(section_matches)} ä¸ªç« èŠ‚")
            
            # éªŒè¯ç« èŠ‚ç¼–å·è¿ç»­æ€§
            section_numbers = [int(match[0]) for match in section_matches]
            section_numbers.sort()
            for i in range(1, len(section_numbers)):
                if section_numbers[i] != section_numbers[i-1] + 1:
                    self.logger.error(f"å¤§çº²ç« èŠ‚ç¼–å·ä¸è¿ç»­ï¼šå‘ç°ç¬¬{section_numbers[i-1]}ç« åç›´æ¥æ˜¯ç¬¬{section_numbers[i]}ç« ")
                    self.logger.error("è¯·æ£€æŸ¥å¤§çº²æ–‡ä»¶ï¼Œç¡®ä¿ç« èŠ‚ç¼–å·è¿ç»­ï¼ˆå¦‚1, 2, 3...ï¼‰")
                    return False
            self.logger.success("å¤§çº²ç« èŠ‚ç¼–å·éªŒè¯é€šè¿‡ï¼šç¼–å·è¿ç»­")
            
            # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ æ–‡ä»¶
            last_completed_section = 0
            if os.path.exists(review_checkpoint_file):
                try:
                    with open(review_checkpoint_file, 'r', encoding='utf-8') as f:
                        checkpoint = json.load(f)
                        last_completed_section = checkpoint.get('last_completed_section', 0)
                    
                    if last_completed_section > 0:
                        self.logger.info(f"[æ–­ç‚¹ç»­ä¼ ] å‘ç°ç»¼è¿°ç”Ÿæˆæ–­ç‚¹ï¼Œå°†ä»ç¬¬ {last_completed_section + 1} ç« å¼€å§‹ç»§ç»­...")
                    else:
                        self.logger.info("[å…¨æ–°å¼€å§‹] æœªå‘ç°æœ‰æ•ˆæ–­ç‚¹ï¼Œå°†ä»ç¬¬1ç« å¼€å§‹ç”Ÿæˆ")
                except Exception as e:
                    self.logger.warning(f"è¯»å–æ–­ç‚¹æ–‡ä»¶å¤±è´¥ï¼Œå°†ä»å¤´å¼€å§‹: {e}")
                    last_completed_section = 0
            else:
                self.logger.info("[å…¨æ–°å¼€å§‹] æœªå‘ç°æ–­ç‚¹æ–‡ä»¶ï¼Œå°†ä»ç¬¬1ç« å¼€å§‹ç”Ÿæˆ")
            
            # æ´å‡€å¯åŠ¨æœºåˆ¶ï¼šå…¨æ–°ä»»åŠ¡æ—¶åˆ é™¤æ—§æ–‡ä»¶
            if last_completed_section == 0 and os.path.exists(word_file):
                self.logger.info(f"æ£€æµ‹åˆ°å·²å­˜åœ¨çš„æ—§ç»¼è¿°æ–‡ä»¶ï¼Œå°†åˆ›å»ºå…¨æ–°ç‰ˆæœ¬: {word_file}")
                try:
                    os.remove(word_file)
                except Exception as e:
                    self.logger.error(f"æ— æ³•åˆ é™¤æ—§çš„ç»¼è¿°æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™: {e}")
                    return False
            
            # åˆ›å»ºæˆ–åŠ è½½Wordæ–‡æ¡£
            doc = None
            if os.path.exists(word_file) and last_completed_section > 0:
                # æ–­ç‚¹ç»­ä¼ ï¼šåŠ è½½ç°æœ‰æ–‡æ¡£
                try:
                    doc = Document(word_file)  # type: ignore
                    self.logger.info(f"[æ–­ç‚¹ç»­ä¼ ] å·²åŠ è½½ç°æœ‰æ–‡æ¡£: {word_file}")
                except Exception as e:
                    self.logger.error(f"åŠ è½½ç°æœ‰æ–‡æ¡£å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ–‡æ¡£: {e}")
                    doc = Document()  # type: ignore
            else:
                # å…¨æ–°å¼€å§‹ï¼šåˆ›å»ºæ–°æ–‡æ¡£
                doc = Document()  # type: ignore
                
                # åŠ è½½æ ·å¼é…ç½®
                style_config = self.config.get('Styling') if self.config else {}  # type: ignore
                font_name = style_config.get('font_name', 'Times New Roman')  # type: ignore
                font_size_body = int(style_config.get('font_size_body', '12'))  # type: ignore
                font_size_heading1 = int(style_config.get('font_size_heading1', '16'))  # type: ignore
                font_size_heading2 = int(style_config.get('font_size_heading2', '14'))  # type: ignore
                
                # è®¾ç½®é»˜è®¤å­—ä½“
                doc.styles['Normal'].font.name = font_name  # type: ignore
                doc.styles['Normal'].font.size = Pt(font_size_body)  # type: ignore
                
                # è®¾ç½®ä¸­æ–‡å­—ä½“
                doc.styles['Normal']._element  # type: ignore.rPr.rFonts.set(qn('w:eastAsia'), font_name)  # type: ignore
                
                # è®¾ç½®æ ‡é¢˜æ ·å¼
                doc.styles['Heading 1'].font.name = font_name  # type: ignore
                doc.styles['Heading 1'].font.size = Pt(font_size_heading1)  # type: ignore
                doc.styles['Heading 1']._element  # type: ignore.rPr.rFonts.set(qn('w:eastAsia'), font_name)  # type: ignore
                
                doc.styles['Heading 2'].font.name = font_name  # type: ignore
                doc.styles['Heading 2'].font.size = Pt(font_size_heading2)  # type: ignore
                doc.styles['Heading 2']._element  # type: ignore.rPr.rFonts.set(qn('w:eastAsia'), font_name)  # type: ignore
                
                title = doc.add_heading('æ–‡çŒ®ç»¼è¿°', level=0)
                if title is not None:  # type: ignore
                    title.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER  # type: ignore

                # åº”ç”¨æ ‡é¢˜æ ·å¼
                for run in title.runs:

                    run.font.name = font_name  # type: ignore

                    run.font.size = Pt(font_size_heading1 + 2)  # ä¸»æ ‡é¢˜ç¨å¤§  # type: ignore
                
                # æ·»åŠ ç”Ÿæˆæ—¶é—´
                date_para = doc.add_paragraph(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
                date_para.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER  # type: ignore
                
                # åº”ç”¨æ—¥æœŸæ ·å¼

                for run in date_para.runs:

                    run.font.name = font_name  # type: ignore

                    run.font.size = Pt(font_size_body)  # type: ignore
            
            # ç”¨tqdmåŒ…è£…ç« èŠ‚åˆ—è¡¨ï¼Œæ˜¾ç¤ºè¿›åº¦æ¡
            progress_bar = tqdm(enumerate(section_matches, 1), total=len(section_matches), desc="[é˜¶æ®µäºŒ] æ­£åœ¨ç”Ÿæˆç»¼è¿°ç« èŠ‚")
            
            # é€ç« ç”Ÿæˆå†…å®¹ï¼ˆä»æ–­ç‚¹å¼€å§‹ï¼‰
            for i, (section_num, section_title) in progress_bar:
                # è·³è¿‡å·²å®Œæˆçš„ç« èŠ‚
                if i <= last_completed_section:
                    self.logger.info(f"[è·³è¿‡] ç¬¬{section_num}ç« å·²å®Œæˆï¼Œç»§ç»­ä¸‹ä¸€ç« ...")
                    continue
                
                # æ–°å¢ï¼šè·³è¿‡å‚è€ƒæ–‡çŒ®å’Œé™„å½•ç« èŠ‚
                if "å‚è€ƒæ–‡çŒ®" in section_title or "é™„å½•" in section_title:
                    self.logger.info(f"[è·³è¿‡] ç¬¬{section_num}ç«  '{section_title}' å°†åœ¨æœ€åç”±ç¨‹åºè‡ªåŠ¨ç”Ÿæˆã€‚")
                    continue
                
                # æ›´æ–°è¿›åº¦æ¡çš„å½“å‰ç« èŠ‚ä¿¡æ¯
                progress_bar.set_postfix_str(f"å½“å‰ç« èŠ‚: {section_num}. {section_title[:30]}...")
                
                self.logger.info(f"æ­£åœ¨ç”Ÿæˆç¬¬{section_num}ç« : {section_title}")
                
                # ç”Ÿæˆç« èŠ‚å†…å®¹
                section_content = self.generate_review_section_content(section_title, outline_content)
                if not section_content:
                    self.logger.error(f"ç¬¬{section_num}ç« å†…å®¹ç”Ÿæˆå¤±è´¥")
                    continue
                
                # æ·»åŠ ç« èŠ‚æ ‡é¢˜å’Œå†…å®¹åˆ°Wordæ–‡æ¡£
                doc.add_paragraph()  # ç©ºè¡Œåˆ†éš”
                
                heading = doc.add_heading(f'ç¬¬{section_num}ç«  {section_title}', level=1)
                heading.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT  # type: ignore
                
                # åŠ è½½æ ·å¼é…ç½®
                style_config = self.config.get('Styling') if self.config else {}  # type: ignore
                font_name = style_config.get('font_name', 'Times New Roman')  # type: ignore
                font_size_body = int(style_config.get('font_size_body', '12'))  # type: ignore
                font_size_heading1 = int(style_config.get('font_size_heading1', '16'))  # type: ignore
                
                # åº”ç”¨æ ‡é¢˜æ ·å¼
                for run in heading.runs:

                    run.font.name = font_name  # type: ignore

                    run.font.size = Pt(font_size_heading1)  # type: ignore
                
                # å°†ç« èŠ‚å†…å®¹æŒ‰æ®µè½åˆ†å‰²å¹¶æ·»åŠ åˆ°æ–‡æ¡£
                paragraphs = section_content.split('\n\n')
                for para in paragraphs:
                    para = para.strip()
                    if para:
                        p = doc.add_paragraph(para)
                        # åº”ç”¨æ­£æ–‡å­—ä½“æ ·å¼

                        for run in p.runs:

                            run.font.name = font_name  # type: ignore

                            run.font.size = Pt(font_size_body)  # type: ignore
                
                # æ›´æ–°æ–­ç‚¹æ–‡ä»¶ï¼ˆæ¯å®Œæˆä¸€ç« å°±æ›´æ–°æ–­ç‚¹ï¼Œä½†ä¸ç«‹å³ä¿å­˜æ–‡æ¡£ï¼‰
                checkpoint_data: Dict[str, Any] = {  # type: ignore
                    'last_completed_section': i,
                    'last_section_title': section_title,
                    'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                with open(review_checkpoint_file, 'w', encoding='utf-8') as f:
                    json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
                
                self.logger.success(f"ç¬¬{section_num}ç« å·²å¤„ç†å¹¶æ›´æ–°æ–­ç‚¹")
            
            # åœ¨æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆåï¼Œä¸€æ¬¡æ€§ä¿å­˜æ–‡æ¡£
            doc.save(word_file)
            self.logger.success(f"å®Œæ•´æ–‡çŒ®ç»¼è¿°å·²ä¿å­˜: {word_file}")
            
            # ç”ŸæˆAPAå‚è€ƒæ–‡çŒ®ï¼ˆæ€»æ˜¯æ‰§è¡Œï¼Œç¡®ä¿å‚è€ƒæ–‡çŒ®æ€»æ˜¯å­˜åœ¨ï¼‰
            self.logger.info("æ­£åœ¨ç”ŸæˆAPAå‚è€ƒæ–‡çŒ®...")
            references = self.generate_apa_references()
            if references:
                doc.add_paragraph()  # ç©ºè¡Œåˆ†éš”
                ref_heading = doc.add_heading('å‚è€ƒæ–‡çŒ®', level=1)
                ref_heading.alignment = WD_PARAGRAPH_ALIGNMENT.LEFT  # type: ignore
                
                # åŠ è½½æ ·å¼é…ç½®
                style_config = self.config.get('Styling') if self.config else {}  # type: ignore
                font_name = style_config.get('font_name', 'Times New Roman')  # type: ignore
                font_size_body = int(style_config.get('font_size_body', '12'))  # type: ignore
                
                # åº”ç”¨å‚è€ƒæ–‡çŒ®æ ‡é¢˜æ ·å¼
                for run in ref_heading.runs:

                    run.font.name = font_name  # type: ignore
                
                # æ·»åŠ å‚è€ƒæ–‡çŒ®ï¼Œåº”ç”¨APAæ‚¬æŒ‚ç¼©è¿›
                for ref in references:
                    p = doc.add_paragraph(ref)
                    # åº”ç”¨æ­£æ–‡å­—ä½“æ ·å¼

                    for run in p.runs:

                        run.font.name = font_name  # type: ignore

                        run.font.size = Pt(font_size_body)  # type: ignore
                    # è®¾ç½®APAæ‚¬æŒ‚ç¼©è¿›ï¼šé¦–è¡Œä¸ç¼©è¿›ï¼Œåç»­è¡Œç¼©è¿›1.27å˜ç±³ï¼ˆ0.5è‹±å¯¸ï¼‰
                    p.paragraph_format.first_line_indent = 0
                    p.paragraph_format.left_indent = Pt(36)  # type: ignore
                
                self.logger.success(f"å·²æ·»åŠ  {len(references)} æ¡å‚è€ƒæ–‡çŒ®ï¼ˆAPAæ ¼å¼ï¼‰")
            else:
                self.logger.warning("æœªç”Ÿæˆä»»ä½•å‚è€ƒæ–‡çŒ®ï¼Œè¯·æ£€æŸ¥æ‘˜è¦æ•°æ®æ˜¯å¦å®Œæ•´")
            
            # æœ€ç»ˆä¿å­˜
            doc.save(word_file)
            
            # æ¸…é™¤æ–­ç‚¹æ–‡ä»¶ï¼ˆè¡¨ç¤ºå…¨éƒ¨å®Œæˆï¼‰
            if os.path.exists(review_checkpoint_file):
                os.remove(review_checkpoint_file)
                self.logger.info("å·²æ¸…é™¤æ–­ç‚¹æ–‡ä»¶ï¼Œæ‰€æœ‰ç« èŠ‚ç”Ÿæˆå®Œæˆ")
            
            # ç”Ÿæˆç›®å½•ï¼ˆåœ¨æœ€ç»ˆä¿å­˜å‰ï¼‰
            if last_completed_section < len(section_matches) or not os.path.exists(review_checkpoint_file):
                self.logger.info("æ­£åœ¨ç”ŸæˆWordæ–‡æ¡£ç›®å½•...")
                self.generate_word_table_of_contents(doc)  # type: ignore
                self.logger.success("ç›®å½•å·²ç”Ÿæˆ")
                
                # æœ€ç»ˆä¿å­˜
                doc.save(word_file)
            
            self.logger.success(f"å®Œæ•´æ–‡çŒ®ç»¼è¿°å·²ç”Ÿæˆ: {word_file}")
            
            # ç¬¬äºŒé˜¶æ®µéªŒè¯ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦è‡ªåŠ¨è¿è¡Œï¼‰
            try:
                if self.config and self.config.getboolean('Performance', 'enable_stage2_validation', fallback=False):
                    self.logger.info("æ ¹æ®é…ç½®æ–‡ä»¶è‡ªåŠ¨å¯åŠ¨ç¬¬äºŒé˜¶æ®µéªŒè¯...")
                    from validator import run_review_validation
                    validation_success = run_review_validation(self)
                    if validation_success:
                        self.logger.success("ç¬¬äºŒé˜¶æ®µéªŒè¯å®Œæˆï¼éªŒè¯æŠ¥å‘Šå·²ç”Ÿæˆã€‚")
                    else:
                        self.logger.warning("ç¬¬äºŒé˜¶æ®µéªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥éªŒè¯æŠ¥å‘Šæ–‡ä»¶ã€‚")
                else:
                    self.logger.info("ç¬¬äºŒé˜¶æ®µéªŒè¯æœªåœ¨é…ç½®ä¸­å¯ç”¨ã€‚å¦‚éœ€è¿è¡ŒéªŒè¯ï¼Œè¯·ä½¿ç”¨: --validate-review")
            except Exception as e:
                self.logger.error(f"ç¬¬äºŒé˜¶æ®µéªŒè¯è¿è¡Œæ—¶å‡ºé”™: {e}")
                self.logger.info("æ‚¨å¯ä»¥æ‰‹åŠ¨è¿è¡ŒéªŒè¯å‘½ä»¤: python main.py --validate-review")
            
            return True
            
        except Exception as e:
            self.logger.error(f"ä»å¤§çº²ç”Ÿæˆæ–‡çŒ®ç»¼è¿°å¤±è´¥: {e}")
            return False

    def generate_word_table_of_contents(self, doc: Any) -> bool:  # type: ignore
        """ä¸ºWordæ–‡æ¡£ç”Ÿæˆè‡ªåŠ¨ç›®å½•"""
        return generate_word_table_of_contents(doc)

    def generate_apa_references(self) -> List[str]:
        """ç”ŸæˆAPAæ ¼å¼çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨"""
        return generate_apa_references(self)

    

    def generate_literature_review_outline(self) -> bool:
        """ç”Ÿæˆæ–‡çŒ®ç»¼è¿°å¤§çº²ï¼ˆå¸¦æ™ºèƒ½ç»­å†™å¾ªç¯ï¼‰"""
        self.logger.info("=" * 60 + "\næ–‡çŒ®ç»¼è¿°è‡ªåŠ¨ç”Ÿæˆå™¨ - é˜¶æ®µäºŒï¼šå¤§çº²ç”Ÿæˆ\n" + "=" * 60)
        try:
            if not self.load_configuration(): 
                return False
            if not self.setup_output_directory(): 
                return False
            if not self.load_existing_summaries():
                self.logger.error("æ— æ³•åŠ è½½æ‘˜è¦æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œé˜¶æ®µä¸€")
                return False
            if not self.summaries:
                self.logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ‘˜è¦ï¼Œè¯·å…ˆè¿è¡Œé˜¶æ®µä¸€")
                return False
            
            writer_config: Dict[str, Any] = (self.config or {}).get('Writer_API', {})  # type: ignore
            if 'dummy' in (writer_config.get('api_key') or ''):  # type: ignore
                outline_content = "# Dummy Outline\n\n## Introduction\n\n## Body Paragraph\n\n## Conclusion"
                if self.project_name:
                    outline_file = os.path.join(self.output_dir, f'{self.project_name}_literature_review_outline.md')  # type: ignore
                else:
                    outline_file = os.path.join(self.output_dir, 'literature_review_outline.md')  # type: ignore
                with open(outline_file, 'w', encoding='utf-8') as f:  # type: ignore
                    f.write(outline_content)
                self.logger.success(f"Dummy outline saved to {outline_file}")
                return True
            
            self.logger.info(f"å·²åŠ è½½{len(self.summaries)}ä¸ªæ–‡çŒ®æ‘˜è¦")
            return self.create_literature_review_outline()
        except Exception as e:
            self.logger.error(f"é˜¶æ®µäºŒè¿è¡Œå¤±è´¥: {e}")
            return False

    def create_literature_review_outline(self) -> bool:
        """åˆ›å»ºæ–‡çŒ®ç»¼è¿°å¤§çº²ï¼Œé€‚é…æ–°çš„çº¯æ–‡æœ¬è¾“å‡ºæ ¼å¼"""
        try:
            review_data = self.prepare_review_data()
            outline_content = self.generate_review_outline(review_data)
            if not outline_content:
                self.logger.error("æ–‡çŒ®ç»¼è¿°å¤§çº²ç”Ÿæˆå¤±è´¥")
                return False
            
            # outline_contentåº”è¯¥æ˜¯çº¯æ–‡æœ¬å­—ç¬¦ä¸²
            if not isinstance(outline_content, str):  # type: ignore
                self.logger.warning("é¢„æœŸæ”¶åˆ°çº¯æ–‡æœ¬ï¼Œä½†æ”¶åˆ°å…¶ä»–æ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢...")
                outline_text = str(outline_content)
            else:
                outline_text = outline_content
            
            # ç”Ÿæˆå¤§çº²æ–‡ä»¶è·¯å¾„ï¼ˆæ·»åŠ é¡¹ç›®åç§°å‰ç¼€ï¼‰
            if self.project_name:
                outline_file = os.path.join(self.output_dir, f'{self.project_name}_literature_review_outline.md')  # type: ignore
            else:
                outline_file = os.path.join(self.output_dir, 'literature_review_outline.md')  # type: ignore
            
            # ä¿å­˜å¤§çº²æ–‡ä»¶
            with open(outline_file, 'w', encoding='utf-8') as f:  # type: ignore
                f.write(outline_text)
            
            self.logger.success(f"æ–‡çŒ®ç»¼è¿°å¤§çº²å·²ç”Ÿæˆ: {outline_file}")
            # æ ¹æ®æ¨¡å¼æä¾›ä¸åŒçš„å‘½ä»¤æç¤º
            if self.mode == "direct":
                self.logger.info("å¤§çº²å·²ç”Ÿæˆã€‚è¯·æ£€æŸ¥å¹¶ä¿®æ”¹ã€‚ç„¶åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆå®Œæ•´ç»¼è¿°ï¼š")
                self.logger.info(f"å‘½ä»¤: python main.py --pdf-folder \"{self.pdf_folder}\" --generate-review")
            else:
                self.logger.info("å¤§çº²å·²ç”Ÿæˆã€‚è¯·æ£€æŸ¥å¹¶ä¿®æ”¹ã€‚ç„¶åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆå®Œæ•´ç»¼è¿°ï¼š")
                self.logger.info(f"å‘½ä»¤: python main.py --project-name \"{self.project_name}\" --generate-review")
            return True
                
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ–‡çŒ®ç»¼è¿°å¤§çº²å¤±è´¥: {e}")
            return False

    def generate_review_outline(self, review_data: Dict[str, Any]) -> Optional[str]:
        """ç”Ÿæˆç»¼è¿°å¤§çº²å†…å®¹ï¼Œé€‚é…æ–°çš„ä¸¤æ®µå¼JSONè¾“å…¥ï¼ˆæ™ºèƒ½ç»­å†™å¾ªç¯ç‰ˆæœ¬ï¼‰"""
        try:
            # ä»æç¤ºè¯æ¨¡æ¿æ–‡ä»¶è¯»å–å¤§çº²æç¤ºè¯
            outline_prompt_template = ""
            try:
                with open('prompts/prompt_synthesize_outline.txt', 'r', encoding='utf-8') as f:
                    outline_prompt_template = f.read()
                self.logger.success(f"åŠ è½½å¤§çº²æç¤ºè¯æ¨¡æ¿: {len(outline_prompt_template)}å­—ç¬¦")
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½å¤§çº²æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                try:
                    with open('prompts/prompt_default_outline.txt', 'r', encoding='utf-8') as f:
                        outline_prompt_template = f.read()
                    self.logger.success(f"åŠ è½½é»˜è®¤å¤§çº²æç¤ºè¯æ¨¡æ¿: {len(outline_prompt_template)}å­—ç¬¦")
                except Exception as e2:
                    self.logger.error(f"æ— æ³•åŠ è½½é»˜è®¤å¤§çº²æç¤ºè¯æ¨¡æ¿: {e2}")
                    outline_prompt_template = "åŸºäºä»¥ä¸‹æ–‡çŒ®æ‘˜è¦ä¿¡æ¯ï¼Œè¯·ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ–‡çŒ®ç»¼è¿°å¤§çº²ã€‚\n\n{{SUMMARIES_JSON_ARRAY}}"

            # ä»æç¤ºè¯æ¨¡æ¿æ–‡ä»¶è¯»å–ç»­å†™æç¤ºè¯
            continue_prompt_template = ""
            try:
                with open('prompts/prompt_continue_outline.txt', 'r', encoding='utf-8') as f:
                    continue_prompt_template = f.read()
                self.logger.success(f"åŠ è½½ç»­å†™æç¤ºè¯æ¨¡æ¿: {len(continue_prompt_template)}å­—ç¬¦")
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½ç»­å†™æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                try:
                    with open('prompts/prompt_default_continue_outline.txt', 'r', encoding='utf-8') as f:
                        continue_prompt_template = f.read()
                    self.logger.success(f"åŠ è½½é»˜è®¤ç»­å†™æç¤ºè¯æ¨¡æ¿: {len(continue_prompt_template)}å­—ç¬¦")
                except Exception as e2:
                    self.logger.error(f"æ— æ³•åŠ è½½é»˜è®¤ç»­å†™æç¤ºè¯æ¨¡æ¿: {e2}")
                    continue_prompt_template = "è¯·ç»§ç»­å®Œæˆè¿™ä»½æœªå†™å®Œçš„æ–‡çŒ®ç»¼è¿°å¤§çº²ã€‚\n\nã€å…¨éƒ¨è®ºæ–‡åˆ†ææ•°æ®ã€‘\n{{SUMMARIES_JSON_ARRAY}}\n\nã€å·²å®Œæˆçš„å¤§çº²è‰ç¨¿ã€‘\n{{PARTIAL_OUTLINE}}"

            # å°†æ•´ä¸ªsummariesåˆ—è¡¨è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²ï¼ˆåŒ…å«ä¸¤æ®µå¼ç»“æ„ï¼‰
            summaries_string = json.dumps(self.summaries, ensure_ascii=False, indent=2)
            self.logger.success(f"ç”Ÿæˆæ‘˜è¦JSONå­—ç¬¦ä¸²: {len(summaries_string)}å­—ç¬¦")

            # å§‹ç»ˆä½¿ç”¨ä¼˜åŒ–åçš„é«˜å¯†åº¦æ ¼å¼ï¼ˆå»é™¤JSONç»“æ„å¼€é”€ï¼‰ï¼Œä»…åœ¨æœ€æç«¯æƒ…å†µä¸‹è§¦å‘æˆªæ–­
            # Gemini 3 Proæœ‰1M tokenä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨950000ä½œä¸ºå®‰å…¨é˜ˆå€¼
            estimated_tokens = estimate_tokens(summaries_string)
            max_tokens_for_optimization = 950000  # ä¼˜åŒ–æ—¶æœ€å¤§tokenæ•°ï¼ˆä»…åœ¨æœ€æç«¯æƒ…å†µä¸‹è§¦å‘æˆªæ–­ï¼‰
            
            self.logger.info(f"ä¸Šä¸‹æ–‡tokenæ•°({estimated_tokens})ï¼Œä½¿ç”¨é«˜å¯†åº¦å‹ç¼©æ ¼å¼...")
            optimized_context = optimize_context_for_outline(self.summaries, max_tokens=max_tokens_for_optimization)
            self.logger.success(f"ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(optimized_context)}å­—ç¬¦ (åŸé•¿åº¦: {len(summaries_string)}å­—ç¬¦)")
            self.logger.info(f"å‹ç¼©ç‡: {len(optimized_context)/len(summaries_string):.1%}")
            
            # ä½¿ç”¨ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡
            summaries_string = optimized_context

            # æå–å†™ä½œå¼•æ“APIé…ç½®
            writer_config: Dict[str, Any] = (self.config or {}).get('Writer_API', {})  # type: ignore
            writer_api_config: APIConfig = {
                'api_key': writer_config.get('api_key') or '',  # type: ignore
                'model': writer_config.get('model') or '',  # type: ignore
                'api_base': writer_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
            }

            self.logger.info("æ­£åœ¨è°ƒç”¨å†™ä½œå¼•æ“ç”Ÿæˆæ–‡çŒ®ç»¼è¿°å¤§çº²ï¼ˆæ™ºèƒ½ç»­å†™å¾ªç¯æ¨¡å¼ï¼‰...")

            # ===== æ™ºèƒ½ç»­å†™å¾ªç¯æ ¸å¿ƒé€»è¾‘ =====
            partial_outline = ""  # å­˜å‚¨å·²ç”Ÿæˆçš„å¤§çº²å†…å®¹
            continuation_attempts = 0  # ç»­å†™è®¡æ•°å™¨
            max_continuation_attempts = 5  # æœ€å¤§ç»­å†™æ¬¡æ•°ï¼ˆå®‰å…¨ç†”æ–­æœºåˆ¶ï¼‰
            
            while continuation_attempts <= max_continuation_attempts:
                try:
                    # æ ¹æ®æ˜¯å¦ä¸ºé¦–æ¬¡è°ƒç”¨é€‰æ‹©ä¸åŒçš„æç¤ºè¯
                    if continuation_attempts == 0:
                        # é¦–æ¬¡è°ƒç”¨ï¼šä½¿ç”¨åŸå§‹å¤§çº²æç¤ºè¯
                        final_prompt = outline_prompt_template.replace('{{SUMMARIES_JSON_ARRAY}}', summaries_string)
                        self.logger.info(f"é¦–æ¬¡å¤§çº²ç”Ÿæˆï¼Œæç¤ºè¯é•¿åº¦: {len(final_prompt)}å­—ç¬¦")
                    else:
                        # ç»­å†™è°ƒç”¨ï¼šä½¿ç”¨ç»­å†™æç¤ºè¯
                        final_prompt = continue_prompt_template.replace('{{SUMMARIES_JSON_ARRAY}}', summaries_string)
                        final_prompt = final_prompt.replace('{{PARTIAL_OUTLINE}}', partial_outline)  # type: ignore
                        self.logger.info(f"ç»­å†™å¤§çº²ç”Ÿæˆ(ç¬¬{continuation_attempts}æ¬¡)ï¼Œæç¤ºè¯é•¿åº¦: {len(final_prompt)}å­—ç¬¦")  # type: ignore

                    # è°ƒç”¨AI API
                    # åŠ è½½ç³»ç»Ÿæç¤ºè¯
                    try:
                        with open('prompts/prompt_system_outline.txt', 'r', encoding='utf-8') as f:
                            system_prompt = f.read()
                        self.logger.success(f"åŠ è½½å¤§çº²ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿: {len(system_prompt)}å­—ç¬¦")
                    except Exception as e:
                        self.logger.warning(f"æ— æ³•åŠ è½½å¤§çº²ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                        system_prompt = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æ–‡çŒ®ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„æ–‡çŒ®åˆ†æç»“æœç”Ÿæˆä¸€ä»½è¯¦ç»†çš„æ–‡çŒ®ç»¼è¿°å¤§çº²ã€‚

è¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼çš„å¤§çº²å†…å®¹
2. ä½¿ç”¨Markdownçš„æ ‡é¢˜æ ¼å¼ï¼ˆ# ä¸»è¦æ ‡é¢˜, ## ç« èŠ‚æ ‡é¢˜, ### å°èŠ‚æ ‡é¢˜ï¼‰
3. æ¯ä¸ªç« èŠ‚æ ‡é¢˜ä¸‹ï¼Œç”¨é¡¹ç›®ç¬¦å·ï¼ˆ-ï¼‰åˆ—å‡ºè¯¥ç« èŠ‚åº”åŒ…å«çš„æ ¸å¿ƒè®ºç‚¹æˆ–åˆ†æè¦ç‚¹
4. å¤§çº²åº”è¯¥ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥è°¨
5. ä¸è¦åŒ…å«ä»»ä½•æ­£æ–‡å†…å®¹ï¼Œåªè¾“å‡ºå¤§çº²"""

                    ai_response_text = _call_ai_api(
                        prompt=final_prompt,
                        api_config=writer_api_config,
                        system_prompt=system_prompt,
                        max_tokens=8192,
                        temperature=0.7,
                        response_format="text",
                        logger=self.logger  # æ·»åŠ loggerå‚æ•°ä»¥è®°å½•è¯¦ç»†é”™è¯¯ä¿¡æ¯
                    )
                    
                    if ai_response_text is None:
                        self.logger.error("APIè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå¤§çº²")
                        return None
                    
                    # æ¨¡æ‹Ÿæ—§APIçš„è¿”å›ç»“æ„ä»¥é€‚é…åç»­é€»è¾‘
                    ai_response = {'choices': [{'message': {'content': ai_response_text}, 'finish_reason': 'stop'}]}  # type: ignore
                    
                    # æå–AIå›å¤å†…å®¹å’Œå®ŒæˆåŸå› 
                    outline_content = ai_response['choices'][0]['message']['content']  # type: ignore
                    finish_reason = ai_response['choices'][0]['finish_reason']  # type: ignore
                    
                    if outline_content and len(outline_content) > 100:  # type: ignore
                        # å°†æœ¬æ¬¡ç”Ÿæˆçš„å†…å®¹è¿½åŠ åˆ°éƒ¨åˆ†å¤§çº²ä¸­
                        if continuation_attempts == 0:
                            partial_outline = outline_content  # type: ignore
                        else:
                            partial_outline += "\n\n" + outline_content  # type: ignore
                        
                        self.logger.success(f"å¤§çº²ç‰‡æ®µç”ŸæˆæˆåŠŸï¼Œå½“å‰æ€»é•¿åº¦: {len(partial_outline)}å­—ç¬¦")  # type: ignore
                        self.logger.info(f"å®ŒæˆåŸå› : {finish_reason}")
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­ç»­å†™
                        if finish_reason == 'stop':  # type: ignore
                            self.logger.success("å¤§çº²ç”Ÿæˆå®Œæˆï¼Œæ— éœ€ç»­å†™")
                            return partial_outline  # type: ignore
                        elif finish_reason == 'length':
                            self.logger.info("å¤§çº²è¢«æˆªæ–­ï¼Œå‡†å¤‡ç»­å†™...")
                            continuation_attempts += 1
                            continue
                        else:
                            self.logger.warning(f"æœªçŸ¥çš„å®ŒæˆåŸå› : {finish_reason}ï¼Œå°è¯•ç»­å†™...")
                            continuation_attempts += 1
                            continue
                    else:
                        self.logger.warning(f"å¤§çº²å†…å®¹è¿‡çŸ­({len(outline_content) if outline_content else 0}å­—ç¬¦)ï¼Œé‡è¯•...")  # type: ignore
                        continuation_attempts += 1
                        continue

                except Exception as e:
                    self.logger.error(f"å¤§çº²ç”Ÿæˆè¿‡ç¨‹å‡ºé”™: {str(e)}")
                    continuation_attempts += 1
                    if continuation_attempts <= max_continuation_attempts:
                        self.logger.info(f"å‡†å¤‡é‡è¯•ç¬¬{continuation_attempts}æ¬¡...")
                        continue
                    else:
                        break
            
            # å®‰å…¨ç†”æ–­ï¼šè¾¾åˆ°æœ€å¤§ç»­å†™æ¬¡æ•°
            if continuation_attempts > max_continuation_attempts:
                self.logger.error(f"[ERROR] å¤§çº²ç”Ÿæˆç»­å†™æ¬¡æ•°è¿‡å¤š({continuation_attempts}æ¬¡)ï¼Œæˆ–å·²é™·å…¥æ­»å¾ªç¯ã€‚è¯·æ£€æŸ¥è¾“å…¥æ•°æ®æˆ–Promptã€‚")
                if partial_outline and len(partial_outline) > 100:  # type: ignore  # åªæœ‰éƒ¨åˆ†å†…å®¹è¶³å¤Ÿé•¿æ‰è¿”å›
                    self.logger.warning("è¿”å›éƒ¨åˆ†ç”Ÿæˆçš„å¤§çº²å†…å®¹")
                    return partial_outline  # type: ignore
                self.logger.error("å¤§çº²ç”Ÿæˆå¤±è´¥ï¼Œå†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º")
                return None
            
            # æœ€ç»ˆæ£€æŸ¥ï¼šåªæœ‰å†…å®¹è¶³å¤Ÿé•¿æ‰è®¤ä¸ºæˆåŠŸ
            if partial_outline and len(partial_outline) > 100:  # type: ignore
                return partial_outline  # type: ignore
            else:
                self.logger.error("å¤§çº²ç”Ÿæˆå¤±è´¥ï¼Œå†…å®¹è¿‡çŸ­æˆ–ä¸ºç©º")
                return None

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆå¤§çº²å†…å®¹å¤±è´¥: {e}")
            return None

    

    def create_literature_review(self) -> bool:
        """åˆ›å»ºæ–‡çŒ®ç»¼è¿°ï¼Œé€‚é…æ–°çš„çº¯æ–‡æœ¬è¾“å‡ºæ ¼å¼"""
        try:
            review_data = self.prepare_review_data()
            review_content = self.generate_review_content(review_data)
            if not review_content:
                self.logger.error("æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå¤±è´¥")
                return False
            
            # review_contentç°åœ¨åº”è¯¥æ˜¯çº¯æ–‡æœ¬å­—ç¬¦ä¸²
            if not isinstance(review_content, str):  # type: ignore
                self.logger.warning("é¢„æœŸæ”¶åˆ°çº¯æ–‡æœ¬ï¼Œä½†æ”¶åˆ°å…¶ä»–æ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢...")
                review_text = str(review_content)
            else:
                review_text = review_content
            
            # ç”ŸæˆWordæ–‡æ¡£è·¯å¾„ï¼ˆæ·»åŠ é¡¹ç›®åç§°å‰ç¼€ï¼‰
            if not self.output_dir:
                self.logger.error("è¾“å‡ºç›®å½•æœªè®¾ç½®")
                return False
                
            if self.project_name:
                word_file = os.path.join(self.output_dir, f'{self.project_name}_literature_review.docx')
            else:
                word_file = os.path.join(self.output_dir, 'literature_review.docx')
            
            # åˆ›å»ºWordæ–‡æ¡£
            success = self.create_word_document(review_text, word_file)
            
            if success:
                self.logger.success(f"æ–‡çŒ®ç»¼è¿°Wordæ–‡æ¡£å·²ç”Ÿæˆ: {word_file}")
                return True
            else:
                return False
                
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ–‡çŒ®ç»¼è¿°å¤±è´¥: {e}")
            return False

    def prepare_review_data(self) -> Dict[str, Any]:
        review_data: Dict[str, Any] = {  # type: ignore
            'total_papers': len(self.summaries),
            'successful_papers': len([s for s in self.summaries if s.get('status') == 'success']),
            'failed_papers': len([s for s in self.summaries if s.get('status') != 'success']), 
            'papers': [],
            'research_areas': {}, 
            'methodologies': {}, 
            'key_findings': [], 
            'common_themes': []
        }
        
        for summary in self.summaries:
            if summary.get('status') != 'success': 
                continue
                
            paper_info = summary.get('paper_info', {})
            ai_summary: Union[AISummary, Dict[str, Any], None] = summary.get('ai_summary', {})
            
            # é€‚é…æ–°çš„ä¸¤æ®µå¼ç»“æ„
            if 'common_core' in ai_summary:  # type: ignore
                # æ–°çš„ä¸¤æ®µå¼ç»“æ„
                common_core = ai_summary['common_core']  # type: ignore
            else:
                # å…¼å®¹æ—§çš„å•æ®µå¼ç»“æ„
                common_core = ai_summary  # type: ignore
            
            paper_data: Dict[str, Any] = {  # type: ignore
                'title': paper_info.get('title', 'æœªçŸ¥æ ‡é¢˜'),
                'authors': paper_info.get('authors', []),
                'year': paper_info.get('year', 'æœªçŸ¥å¹´ä»½'),
                'journal': paper_info.get('journal', 'æœªçŸ¥æœŸåˆŠ'),
                'summary': common_core.get('summary', ''),  # type: ignore
                'key_points': common_core.get('key_points', []),  # type: ignore
                'methodology': common_core.get('methodology', ''),  # type: ignore
                'findings': common_core.get('findings', ''),  # type: ignore
                'conclusions': common_core.get('conclusions', ''),  # type: ignore
                'relevance': common_core.get('relevance', ''),  # type: ignore
                'limitations': common_core.get('limitations', '')  # type: ignore
            }
            
            review_data['papers'].append(paper_data)  # type: ignore
            
            methodology = paper_data['methodology']
            if methodology: 
                review_data['methodologies'][methodology] = review_data['methodologies'].get(methodology, 0) + 1  # type: ignore
                
            findings = paper_data['findings']  # type: ignore
            if findings: 
                review_data['key_findings'].append(findings)  # type: ignore
                
        return review_data

    def generate_review_content(self, review_data: Dict[str, Any]) -> Optional[str]:
        """ç”Ÿæˆç»¼è¿°å†…å®¹ï¼Œé€‚é…æ–°çš„ä¸¤æ®µå¼JSONè¾“å…¥"""
        try:
            # ä»æç¤ºè¯æ¨¡æ¿æ–‡ä»¶è¯»å–ç»¼è¿°æç¤ºè¯
            synthesize_prompt_template = ""
            try:
                with open('prompts/prompt_synthesize.txt', 'r', encoding='utf-8') as f:
                    synthesize_prompt_template = f.read()
                self.logger.success(f"åŠ è½½ç»¼è¿°æç¤ºè¯æ¨¡æ¿: {len(synthesize_prompt_template)}å­—ç¬¦")
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½ç»¼è¿°æç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                try:
                    with open('prompts/prompt_default_synthesize.txt', 'r', encoding='utf-8') as f:
                        synthesize_prompt_template = f.read()
                    self.logger.success(f"åŠ è½½é»˜è®¤ç»¼è¿°æç¤ºè¯æ¨¡æ¿: {len(synthesize_prompt_template)}å­—ç¬¦")
                except Exception as e2:
                    self.logger.error(f"æ— æ³•åŠ è½½é»˜è®¤ç»¼è¿°æç¤ºè¯æ¨¡æ¿: {e2}")
                    synthesize_prompt_template = "åŸºäºä»¥ä¸‹æ–‡çŒ®æ‘˜è¦ä¿¡æ¯ï¼Œè¯·ç”Ÿæˆä¸€ä»½å®Œæ•´çš„æ–‡çŒ®ç»¼è¿°æŠ¥å‘Šã€‚\n\n{{SUMMARIES_JSON_ARRAY}}"

            # å°†æ•´ä¸ªsummariesåˆ—è¡¨è½¬æ¢ä¸ºæ ¼å¼åŒ–çš„JSONå­—ç¬¦ä¸²ï¼ˆåŒ…å«ä¸¤æ®µå¼ç»“æ„ï¼‰
            summaries_string = json.dumps(self.summaries, ensure_ascii=False, indent=2)
            self.logger.success(f"ç”Ÿæˆæ‘˜è¦JSONå­—ç¬¦ä¸²: {len(summaries_string)}å­—ç¬¦")

            # å°†å®Œæ•´çš„JSONå­—ç¬¦ä¸²æ³¨å…¥åˆ°æ¨¡æ¿ä¸­
            final_prompt = synthesize_prompt_template.replace('{{SUMMARIES_JSON_ARRAY}}', summaries_string)
            self.logger.success(f"ç”Ÿæˆæœ€ç»ˆç»¼è¿°æç¤ºè¯: {len(final_prompt)}å­—ç¬¦")

            # æå–å†™ä½œå¼•æ“APIé…ç½®
            writer_config: Dict[str, Any] = (self.config or {}).get('Writer_API', {})  # type: ignore
            writer_api_config: APIConfig = {
                'api_key': writer_config.get('api_key') or '',  # type: ignore
                'model': writer_config.get('model') or '',  # type: ignore
                'api_base': writer_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
            }

            self.logger.info("æ­£åœ¨è°ƒç”¨å†™ä½œå¼•æ“ç”Ÿæˆæ–‡çŒ®ç»¼è¿°...")

            # ===== ä¸“é—¨ä¸ºç»¼è¿°è°ƒç”¨è®¾è®¡çš„APIæ¥å£ï¼ˆä¸å¼ºåˆ¶JSONæ ¼å¼ï¼‰=====
            import requests  # type: ignore

            api_key = writer_api_config.get('api_key') or ''
            api_base = writer_api_config.get('api_base', 'https://api.openai.com/v1')
            model_name = writer_api_config.get('model') or ''

            api_url = f"{api_base.rstrip('/')}/chat/completions"  # type: ignore

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }

            # ä¸“é—¨ä¸ºç»¼è¿°è®¾è®¡çš„ç³»ç»Ÿæç¤ºè¯ï¼ˆè¿”å›çº¯æ–‡æœ¬ï¼‰
            try:
                with open('prompts/prompt_system_synthesize.txt', 'r', encoding='utf-8') as f:
                    system_prompt = f.read()
                self.logger.success(f"åŠ è½½ç»¼è¿°ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿: {len(system_prompt)}å­—ç¬¦")
            except Exception as e:
                self.logger.warning(f"æ— æ³•åŠ è½½ç»¼è¿°ç³»ç»Ÿæç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {e}")
                system_prompt = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯æ–‡çŒ®ç»¼è¿°ä¸“å®¶ã€‚è¯·åŸºäºæä¾›çš„æ–‡çŒ®åˆ†æç»“æœç”Ÿæˆä¸€ä»½å®Œæ•´çš„ä¸­æ–‡å­¦æœ¯ç»¼è¿°æŠ¥å‘Šã€‚

è¦æ±‚ï¼š
1. ç›´æ¥è¾“å‡ºçº¯æ–‡æœ¬æ ¼å¼çš„ç»¼è¿°å†…å®¹ï¼Œä¸è¦ä½¿ç”¨JSONæ ¼å¼
2. ä½¿ç”¨markdownæ ¼å¼ç»„ç»‡ç»“æ„ï¼ˆæ ‡é¢˜ç”¨#, ##ç­‰ï¼‰
3. å†…å®¹éœ€è¦ä¸“ä¸šã€å®¢è§‚ã€å…¨é¢
4. é€‚å½“å¼•ç”¨å…·ä½“æ–‡çŒ®ä»¥æ”¯æŒè®ºç‚¹
5. æ€»å­—æ•°æ§åˆ¶åœ¨3000-5000å­—"""

            payload: Dict[str, Any] = {  # type: ignore
                "model": model_name,
                "messages": [
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": final_prompt
                    }
                ],
                "temperature": 0.7,
                "max_tokens": 8000  # ç»¼è¿°éœ€è¦æ›´é•¿çš„å“åº”
            }

            # é‡è¯•é€»è¾‘
            max_retries = 5
            for attempt in range(max_retries):
                try:
                    self.logger.info(f"ç»¼è¿°ç”Ÿæˆå°è¯• {attempt + 1}/{max_retries}")

                    response = requests.post(
                        api_url,
                        headers=headers,
                        json=payload,
                        timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                    )

                    response.raise_for_status()
                    response_data = response.json()

                    # æå–AIå›å¤å†…å®¹
                    review_content = response_data['choices'][0]['message']['content']

                    if review_content and len(review_content) > 100:
                        self.logger.success("å†™ä½œå¼•æ“è¿”å›ç»¼è¿°æ–‡æœ¬")
                        return review_content
                    else:
                        self.logger.warning(f"ç»¼è¿°å†…å®¹è¿‡çŸ­({len(review_content)}å­—ç¬¦)ï¼Œé‡è¯•...")

                except requests.exceptions.HTTPError as e:
                    if attempt < max_retries - 1:
                        wait_time = 2 * (2 ** attempt)
                        self.logger.warning(f"HTTPé”™è¯¯ {response.status_code if 'response' in locals() else '?'}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")  # type: ignore
                        time.sleep(wait_time)
                        continue
                    else:
                        self.logger.error(f"ç»¼è¿°ç”Ÿæˆå¤±è´¥: {str(e)}")
                        return None

                except Exception as e:
                    if attempt < max_retries - 1:
                        wait_time = 2 * (2 ** attempt)
                        self.logger.warning(f"é”™è¯¯: {str(e)}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        self.logger.error(f"ç»¼è¿°ç”Ÿæˆæœ€ç»ˆå¤±è´¥: {str(e)}")
                        return None

            return None

        except Exception as e:
            self.logger.error(f"ç”Ÿæˆç»¼è¿°å†…å®¹å¤±è´¥: {e}")
            return None

    @staticmethod
    def build_review_prompt(review_data: Dict[str, Any]) -> str:
        papers_info = []
        for i, paper in enumerate(review_data['papers'], 1):
            paper_text = f"æ–‡çŒ® {i}: {paper['title']}\nä½œè€…: {', '.join(paper['authors']) if paper['authors'] else 'æœªçŸ¥'}\nå¹´ä»½: {paper['year']}\næœŸåˆŠ: {paper['journal']}\n\næ‘˜è¦: {paper['summary']}\n\nç ”ç©¶æ–¹æ³•: {paper['methodology']}\nä¸»è¦å‘ç°: {paper['findings']}\nç»“è®º: {paper['conclusions']}\nç›¸å…³æ€§: {paper['relevance']}\nå±€é™æ€§: {paper['limitations']}\n\nå…³é”®è¦ç‚¹:\n{chr(10).join(['- ' + point for point in paper['key_points']])}"
            papers_info.append(paper_text)  # type: ignore
        all_papers_text = '\n'.join(papers_info)  # type: ignore
        prompt = f"åŸºäºä»¥ä¸‹{review_data['total_papers']}ç¯‡å­¦æœ¯æ–‡çŒ®çš„æ‘˜è¦ä¿¡æ¯ï¼Œè¯·ç”Ÿæˆä¸€ä»½å®Œæ•´çš„æ–‡çŒ®ç»¼è¿°æŠ¥å‘Šã€‚\n\næ–‡çŒ®ä¿¡æ¯:\n{all_papers_text}\n\nè¯·æŒ‰ç…§ä»¥ä¸‹ç»“æ„ç”Ÿæˆæ–‡çŒ®ç»¼è¿°ï¼š\n\n# æ–‡çŒ®ç»¼è¿°æŠ¥å‘Š\n\n## 1. å¼•è¨€\n- ç ”ç©¶é¢†åŸŸæ¦‚è¿°\n- ç ”ç©¶èƒŒæ™¯å’Œæ„ä¹‰\n- æ–‡çŒ®ç»¼è¿°çš„ç›®çš„å’ŒèŒƒå›´\n\n## 2. ç ”ç©¶ç°çŠ¶åˆ†æ\n- ä¸»è¦ç ”ç©¶ä¸»é¢˜å’Œè¶‹åŠ¿\n- ç ”ç©¶æ–¹æ³•çš„åˆ†æå’Œæ¯”è¾ƒ\n- å…³é”®å‘ç°çš„æ€»ç»“\n\n## 3. ç ”ç©¶çƒ­ç‚¹å’Œå‰æ²¿\n- å½“å‰ç ”ç©¶çš„çƒ­ç‚¹é—®é¢˜\n- æ–°å…´çš„ç ”ç©¶æ–¹å‘\n- å°šæœªè§£å†³çš„é—®é¢˜\n\n## 4. ç ”ç©¶æ–¹æ³•å’Œè´¨é‡åˆ†æ\n- å¸¸ç”¨ç ”ç©¶æ–¹æ³•çš„è¯„ä»·\n- ç ”ç©¶è´¨é‡çš„æ€»ä½“è¯„ä¼°\n- ç ”ç©¶çš„å±€é™æ€§åˆ†æ\n\n## 5. ç»¼åˆè®¨è®º\n- ä¸»è¦å…±è¯†å’Œåˆ†æ­§\n- ç ”ç©¶çš„ç†è®ºè´¡çŒ®\n- å®è·µæ„ä¹‰å’Œåº”ç”¨å‰æ™¯\n\n## 6. æœªæ¥ç ”ç©¶æ–¹å‘\n- åŸºäºç°æœ‰ç ”ç©¶ç©ºç™½çš„å»ºè®®\n- æ–¹æ³•å­¦æ”¹è¿›çš„å»ºè®®\n- ç†è®ºå’Œå®è·µçš„å‘å±•æ–¹å‘\n\n## 7. ç»“è®º\n- ä¸»è¦å‘ç°æ€»ç»“\n- å¯¹é¢†åŸŸçš„è´¡çŒ®\n- ç»¼è¿°çš„å±€é™æ€§\n\n## å‚è€ƒæ–‡çŒ®\n- æŒ‰ç…§å­¦æœ¯è§„èŒƒåˆ—å‡ºæ‰€æœ‰æ–‡çŒ®\n\nè¦æ±‚ï¼š\n1. å†…å®¹è¦å…¨é¢ã€å®¢è§‚ã€å‡†ç¡®\n2. è¦æœ‰æ‰¹åˆ¤æ€§æ€ç»´å’Œåˆ†æ\n3. è¦æŒ‡å‡ºç ”ç©¶è¶‹åŠ¿å’Œæœªæ¥æ–¹å‘\n4. è¯­è¨€è¦ä¸“ä¸šã€ç®€æ´ã€æ¸…æ™°\n5. æ€»å­—æ•°åœ¨3000-5000å­—ä¹‹é—´"
        return prompt

    @staticmethod
    def format_review_content(review_content: Dict[str, Any], review_data: Dict[str, Any]) -> str:
        header = f"# æ–‡çŒ®ç»¼è¿°æŠ¥å‘Š\n\n**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}\n**æ–‡çŒ®æ•°é‡**: {review_data['total_papers']}ç¯‡\n**æˆåŠŸå¤„ç†**: {review_data['successful_papers']}ç¯‡\n**å¤±è´¥å¤„ç†**: {review_data['failed_papers']}ç¯‡\n\n---\n\n"
        review_text = review_content if isinstance(review_content, str) else review_content.get('summary', json.dumps(  # type: ignore
            review_content, ensure_ascii=False, indent=2))
        references = "\n\n## å‚è€ƒæ–‡çŒ®\n\n"
        for i, paper in enumerate(review_data['papers'], 1):
            authors = ', '.join(paper['authors']) if paper['authors'] else 'æœªçŸ¥ä½œè€…';
            year = f" ({paper['year']})" if paper['year'] != 'æœªçŸ¥å¹´ä»½' else '';
            journal = f". {paper['journal']}" if paper['journal'] != 'æœªçŸ¥æœŸåˆŠ' else ''
            references += f"{i}. {authors}{year}. {paper['title']}{journal}.\n"
        return header + review_text + references

    def create_word_document(self, markdown_text: str, output_path: str) -> bool:
        """å°†Markdownæ–‡æœ¬è§£æå¹¶åˆ›å»ºWordæ–‡æ¡£ï¼ˆå¸¦æ ·å¼é…ç½®ï¼‰"""
        return create_word_document(self, markdown_text, output_path)

    def run_priming_phase(self, concept_name: str, seed_folder: str) -> bool:
        """æ¦‚å¿µå­¦ä¹ é˜¶æ®µï¼šåˆ†ææ ¸å¿ƒè®ºæ–‡ä»¥å»ºç«‹æ¦‚å¿µç†è§£"""
        self.logger.info("=" * 60 + "\næ¦‚å¿µå­¦ä¹ é˜¶æ®µï¼šå»ºç«‹æ¦‚å¿µç†è§£\n" + "=" * 60)
        try:
            if not self.load_configuration():
                return False
            if not self.setup_output_directory():
                return False
            
            # éªŒè¯ç§å­æ–‡ä»¶å¤¹
            if not os.path.exists(seed_folder):
                self.logger.error(f"ç§å­æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {seed_folder}")
                return False
            
            # æ‰«æç§å­è®ºæ–‡
            seed_papers = []
            for root, _, files in os.walk(seed_folder):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        seed_papers.append(os.path.join(root, file))  # type: ignore
            
            if not seed_papers:
                self.logger.error(f"ç§å­æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶: {seed_folder}")
                return False
            
            self.logger.info(f"æ‰¾åˆ° {len(seed_papers)} ç¯‡ç§å­è®ºæ–‡")  # type: ignore

            # å¤„ç†ç§å­è®ºæ–‡ - ä¿æŒå®Œæ•´ä¿¡æ¯é‡ï¼Œä½¿ç”¨å¹¶å‘å¤„ç†
            concept_papers = []

            # ä½¿ç”¨å¹¶å‘å¤„ç†æé«˜é€Ÿåº¦ï¼Œä½†ä¿æŒå®Œæ•´ä¿¡æ¯é‡
            max_workers = min(2, len(seed_papers))  # type: ignore  # æœ€å¤š2ä¸ªå¹¶å‘ï¼Œé¿å…APIé™åˆ¶
            
            def process_seed_paper(pdf_path: str) -> Optional[Dict[str, Any]]:  # type: ignore
                """å¤„ç†å•ä¸ªç§å­è®ºæ–‡"""
                try:
                    self.logger.info(f"æ­£åœ¨åˆ†æç§å­è®ºæ–‡: {os.path.basename(pdf_path)}")  # type: ignore
                    
                    # æå–å®Œæ•´æ–‡æœ¬
                    pdf_text = extract_text_from_pdf(pdf_path)  # type: ignore
                    if not pdf_text or len(pdf_text.strip()) < 500:  # type: ignore
                        self.logger.warning(f"ç§å­è®ºæ–‡æ–‡æœ¬æå–å¤±è´¥: {os.path.basename(pdf_path)}")  # type: ignore
                        return None
                    
                    # åˆ›å»ºè®ºæ–‡ä¿¡æ¯
                    paper_info: Dict[str, Any] = {
                        'title': os.path.splitext(os.path.basename(pdf_path))[0],
                        'authors': [],
                        'year': 'æœªçŸ¥å¹´ä»½',
                        'journal': 'æœªçŸ¥æœŸåˆŠ',
                        'doi': '',
                        'pdf_path': pdf_path
                    }  # type: ignore
                    
                    # è·å–APIé…ç½®
                    primary_config: Dict[str, str] = self.config.get('Primary_Reader_API', {}) if self.config else {}  # type: ignore
                    backup_config: Dict[str, str] = self.config.get('Backup_Reader_API', {}) if self.config else {}  # type: ignore
                    
                    reader_api_config: APIConfig = {
                        'api_key': primary_config.get('api_key', ''),  # type: ignore
                        'model': primary_config.get('model', ''),  # type: ignore
                        'api_base': primary_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
                    }
                    
                    backup_api_config: APIConfig = {
                        'api_key': backup_config.get('api_key', ''),  # type: ignore
                        'model': backup_config.get('model', ''),  # type: ignore
                        'api_base': backup_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
                    }
                    
                    # æ„å»ºå®Œæ•´çš„åˆ†ææç¤ºè¯
                    try:
                        with open('prompts/prompt_analyze.txt', 'r', encoding='utf-8') as f:
                            prompt_template: str = f.read()  # type: ignore
                        
                        # æ›¿æ¢å ä½ç¬¦
                        analysis_prompt: str = prompt_template.replace('{{PAPER_FULL_TEXT}}', pdf_text)  # type: ignore
                        
                    except Exception as e:
                        self.logger.warning(f"æ— æ³•åŠ è½½åˆ†ææç¤ºè¯æ¨¡æ¿ï¼Œä½¿ç”¨ç®€åŒ–æç¤ºè¯: {e}")
                        # ç®€åŒ–æç¤ºè¯
                        analysis_prompt = f"è¯·åˆ†æä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œç”Ÿæˆç»“æ„åŒ–æ‘˜è¦ï¼š\n\n{pdf_text}"
                    
                    # è°ƒç”¨AIåˆ†æ
                    ai_result = get_summary_from_ai_with_fallback(analysis_prompt, reader_api_config, backup_api_config, logger=self.logger, config=self.config)
                    if ai_result:
                        self.logger.success(f"ç§å­è®ºæ–‡åˆ†ææˆåŠŸ: {os.path.basename(pdf_path)}")
                        return {
                            'paper_info': paper_info,
                            'ai_summary': ai_result
                        }
                    else:
                        self.logger.warning(f"ç§å­è®ºæ–‡åˆ†æå¤±è´¥: {os.path.basename(pdf_path)}")
                        return None
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†ç§å­è®ºæ–‡æ—¶å‡ºé”™ {os.path.basename(pdf_path)}: {e}")
                    return None
            
            for future in concurrent.futures.as_completed(future_to_pdf):  # type: ignore
                result: Optional[Dict[str, Any]] = future.result()  # type: ignore
                if result:
                    concept_papers.append(result)  # type: ignore
            
            if not concept_papers:
                self.logger.error("æ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•ç§å­è®ºæ–‡")
                return False
            
            # ç”Ÿæˆæ¦‚å¿µé…ç½®
            self.logger.info(f"æ­£åœ¨ç”Ÿæˆæ¦‚å¿µé…ç½®: {concept_name}")
            concept_profile: Dict[str, Any] = self._generate_concept_profile(concept_name, concept_papers)  # type: ignore
            
            if not concept_profile:
                self.logger.error("æ¦‚å¿µé…ç½®ç”Ÿæˆå¤±è´¥")
                return False
            
            # ä¿å­˜æ¦‚å¿µé…ç½®
            concept_profile_file: str = os.path.join(self.output_dir, f"{self.project_name}_concept_profile.json")  # type: ignore
            with open(concept_profile_file, 'w', encoding='utf-8') as f:  # type: ignore
                json.dump(concept_profile, f, ensure_ascii=False, indent=2)
            
            self.logger.success(f"æ¦‚å¿µé…ç½®å·²ä¿å­˜: {concept_profile_file}")
            return True
            
        except Exception as e:
            self.logger.error(f"æ¦‚å¿µå­¦ä¹ é˜¶æ®µå¤±è´¥: {e}")
            return False
    
    
    def _fix_json_string(self, json_str: str) -> str:
        """ä¿®å¤å¸¸è§çš„JSONå­—ç¬¦ä¸²é—®é¢˜"""
        try:
            # ç§»é™¤å¯èƒ½çš„æ³¨é‡Š
            import re
            json_str = re.sub(r'//.*', '', json_str)  # ç§»é™¤å•è¡Œæ³¨é‡Š
            json_str = re.sub(r'/\*.*?\*/', '', json_str, flags=re.DOTALL)  # ç§»é™¤å¤šè¡Œæ³¨é‡Š
            
            # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
            json_str = json_str.strip()
            
            # å¦‚æœå­—ç¬¦ä¸²ä»¥å¼•å·å¼€å§‹ä½†ä¸ä»¥å¼•å·ç»“æŸï¼Œæ·»åŠ ç»“æŸå¼•å·
            if json_str.startswith('"') and not json_str.endswith('"'):
                json_str += '"'
            elif json_str.startswith("'") and not json_str.endswith("'"):
                json_str += "'"
            
            return json_str
        except Exception as e:
            self.logger.error(f"ä¿®å¤JSONå­—ç¬¦ä¸²å¤±è´¥: {e}")
            return json_str
    
    def _generate_concept_profile(self, concept_name: str, concept_papers: list[Dict[str, Any]]) -> Dict[str, Any]:  # type: ignore
        """æ ¹æ®å·²åˆ†æçš„ç§å­è®ºæ–‡æ‘˜è¦ï¼Œç”Ÿæˆæ¦‚å¿µé…ç½®æ–‡ä»¶ã€‚"""
        try:
            self.logger.info(f"å¼€å§‹ç”Ÿæˆæ¦‚å¿µå­¦ä¹ ç¬”è®°: {concept_name}")
            self.logger.info(f"ç§å­è®ºæ–‡æ•°é‡: {len(concept_papers)}")
            
            # 1. åŠ è½½æ¦‚å¿µåˆ†æçš„ Prompt æ¨¡æ¿
            try:
                with open('prompts/prompt_prime_concept.txt', 'r', encoding='utf-8') as f:
                    prompt_template = f.read()
                self.logger.success(f"åŠ è½½æ¦‚å¿µåˆ†ææç¤ºè¯æ¨¡æ¿: {len(prompt_template)}å­—ç¬¦")
            except Exception as e:
                self.logger.error(f"æ— æ³•åŠ è½½æ¦‚å¿µåˆ†ææç¤ºè¯æ¨¡æ¿: {e}")
                return {}  # type: ignore

            # 2. å‡†å¤‡è®ºæ–‡æ•°æ® (ç›´æ¥ä»ä¼ å…¥çš„ concept_papers æ„å»º)
            papers_data: list[Dict[str, Any]] = []  # type: ignore
            for paper in concept_papers:  # type: ignore
                # ä» paper['ai_summary'] æå–æ‰€éœ€å­—æ®µï¼Œæ„å»º papers_data
                papers_data.append({
                    'file_name': paper.get('file_name', 'æœªçŸ¥æ–‡ä»¶'),  # type: ignore
                    'ai_summary': paper.get('ai_summary', {})  # type: ignore
                })
            
            # 3. æ„å»ºæœ€ç»ˆçš„ Prompt
            papers_json = json.dumps(papers_data, ensure_ascii=False, indent=2)
            final_prompt = prompt_template.replace('{{CONCEPT_NAME}}', concept_name).replace('{{SEED_PAPERS}}', papers_json)
            
            # è°ƒç”¨AIç”Ÿæˆæ¦‚å¿µå­¦ä¹ ç¬”è®°
            writer_config: Dict[str, Any] = (self.config or {}).get('Writer_API', {})  # type: ignore
            writer_api_config: APIConfig = {
                'api_key': writer_config.get('api_key') or '',  # type: ignore
                'model': writer_config.get('model') or '',  # type: ignore
                'api_base': writer_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
            }
            
            # è®¾ç½®ç³»ç»Ÿæç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä½å­¦æœ¯ç ”ç©¶ä¸“å®¶ï¼Œä¸“é—¨ç ”ç©¶æ¦‚å¿µçš„å†å²å‘å±•å’Œç†è®ºæ¼”åŒ–ã€‚è¯·åŸºäºæä¾›çš„ç§å­è®ºæ–‡ï¼Œç”Ÿæˆä¸€ä¸ªå…³äºæŒ‡å®šæ¦‚å¿µçš„å…¨é¢å­¦ä¹ ç¬”è®°ï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚"""
            
            self.logger.info("æ­£åœ¨è°ƒç”¨AIç”Ÿæˆæ¦‚å¿µå­¦ä¹ ç¬”è®°...")
            
            # ä½¿ç”¨ai_interface.pyä¸­çš„å¥å£®APIè°ƒç”¨å‡½æ•°
            from ai_interface import _call_ai_api  # type: ignore
            concept_profile = _call_ai_api(
                prompt=final_prompt,
                api_config=writer_api_config,
                system_prompt=system_prompt,
                max_tokens=4000,
                temperature=0.7,
                response_format="json"
            )
            
            if concept_profile:
                self.logger.success(f"æ¦‚å¿µå­¦ä¹ ç¬”è®°ç”ŸæˆæˆåŠŸ")
                return concept_profile  # type: ignore
            else:
                self.logger.error("æ¦‚å¿µå­¦ä¹ ç¬”è®°ç”Ÿæˆå¤±è´¥")
                return {}  # type: ignore
            
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæ¦‚å¿µé…ç½®å¤±è´¥: {e}")
            return {}  # type: ignore
    
    
    
    
    def run_concept_priming(self, seed_papers_folder: str, concept_name: str) -> bool:
        """è¿è¡Œæ¦‚å¿µå­¦ä¹ é˜¶æ®µï¼ˆä¿ç•™æ—§å‡½æ•°åä»¥å…¼å®¹ï¼‰"""
        return self.run_priming_phase(concept_name, seed_papers_folder)
        
        
        


def sanitize_path_component(path_component: str) -> str:
    """æ¸…ç†è·¯å¾„ç»„ä»¶ï¼Œç§»é™¤æˆ–æ›¿æ¢éæ³•å­—ç¬¦"""
    import re
    if not path_component:
        return "unnamed"
    
    # ç§»é™¤æˆ–æ›¿æ¢Windowsè·¯å¾„ä¸­çš„éæ³•å­—ç¬¦
    # Windowsä¸å…è®¸çš„å­—ç¬¦: < > : " | ? * ä»¥åŠæ§åˆ¶å­—ç¬¦
    sanitized = re.sub(r'[<>:"|?*\x00-\x1f]', '_', path_component)
    
    # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºæ ¼å’Œç‚¹ï¼ˆWindowsä¸å…è®¸ï¼‰
    sanitized = sanitized.strip(' .')
    
    # ç¡®ä¿åç§°ä¸ä¸ºç©º
    if not sanitized:
        sanitized = "unnamed"
    
    # é™åˆ¶é•¿åº¦ï¼ˆWindowsè·¯å¾„é™åˆ¶ï¼‰
    if len(sanitized) > 100:
        sanitized = sanitized[:100]
    
    return sanitized

def dispatch_command(args: argparse.Namespace):  # type: ignore
    """å‘½ä»¤åˆ†æ´¾å™¨ - æ ¹æ®å‚æ•°è°ƒç”¨ç›¸åº”çš„å¤„ç†å‡½æ•°"""
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºå®‰è£…æ¨¡å¼
        if args.setup:
            run_setup_wizard()
            return
        
        # æ¦‚å¿µå­¦ä¹ æ¨¡å¼ï¼ˆPriming Phaseï¼‰
        if args.prime_with_folder and args.concept:
            # æ£€æŸ¥æ˜¯å¦æä¾›äº†é¡¹ç›®åç§°
            if not args.project_name:
                logging.error("æ¦‚å¿µå­¦ä¹ æ¨¡å¼éœ€è¦æŒ‡å®š --project-name å‚æ•°")
                sys.exit(1)
            
            generator = LiteratureReviewGenerator(args.config, args.project_name, None)
            generator.logger.info("*** æ¦‚å¿µå­¦ä¹ æ¨¡å¼å·²å¯åŠ¨ ***")
            generator.logger.info("=" * 60)
            
            if not generator.load_configuration():
                generator.logger.error("é…ç½®åŠ è½½å¤±è´¥")
                sys.exit(1)
            
            # è®¾ç½®è¾“å‡ºç›®å½•
            if not generator.setup_output_directory():
                generator.logger.error("è¾“å‡ºç›®å½•è®¾ç½®å¤±è´¥")
                sys.exit(1)
            
            # æ‰§è¡Œæ¦‚å¿µå­¦ä¹ é˜¶æ®µ
            success = generator.run_priming_phase(args.concept, args.prime_with_folder)
            if success:
                generator.logger.success("æ¦‚å¿µå­¦ä¹ é˜¶æ®µå®Œæˆï¼æ¦‚å¿µé…ç½®æ–‡ä»¶å·²ç”Ÿæˆ")
            else:
                generator.logger.error("æ¦‚å¿µå­¦ä¹ é˜¶æ®µå¤±è´¥")
                sys.exit(1)
            return
        
        # é‡è¯•æ¨¡å¼
        if args.retry_failed:
            handle_retry_failed(args)
            return
        
        # åˆå¹¶æ¨¡å¼
        if args.merge:
            handle_merge_mode(args)
            return
        
        # æ­£å¸¸æ‰§è¡Œæ¨¡å¼ - éªŒè¯å‚æ•°
        if not args.project_name and not args.pdf_folder:
            logging.error("å¿…é¡»æŒ‡å®š--project-nameæˆ–--pdf-folderå‚æ•°ä¸­çš„ä¸€ä¸ª")
            sys.exit(1)
        
        # éªŒè¯project_nameæ ¼å¼
        if args.project_name:
            # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯å®Œæ•´è·¯å¾„ï¼ˆå¸¸è§é”™è¯¯ï¼‰
            if len(args.project_name) > 100 or '\\' in args.project_name or '/' in args.project_name:
                logging.error("âŒ --project-name å‚æ•°é”™è¯¯")
                logging.error("ğŸ’¡ è¯·ä¸è¦ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼Œåº”è¯¥ä½¿ç”¨ç®€æ´çš„é¡¹ç›®åç§°")
                logging.error("ğŸ“ ç¤ºä¾‹ï¼š--project-name \"æ¡ˆä¾‹åˆ†æ\" è€Œé --project-name \"C:\\Users\\123\\Desktop\\æˆ‘çš„é¡¹ç›®\"")
                logging.error("ğŸ”„ æˆ–è€…ä½¿ç”¨ --pdf-folder æŒ‡å®šPDFæ–‡ä»¶å¤¹è·¯å¾„")
                sys.exit(1)
            
            # æ£€æŸ¥project_nameé•¿åº¦
            if len(args.project_name) > 50:
                logging.warning(f"âš ï¸  é¡¹ç›®åç§°è¿‡é•¿ï¼ˆ{len(args.project_name)}å­—ç¬¦ï¼‰ï¼Œå»ºè®®ä½¿ç”¨æ›´ç®€æ´çš„åç§°")
            
        generator = LiteratureReviewGenerator(args.config, args.project_name, args.pdf_folder)
        
        # å…ˆåŠ è½½é…ç½®å’Œè®¾ç½®è¾“å‡ºç›®å½•
        if not generator.load_configuration():
            generator.logger.error("é…ç½®åŠ è½½å¤±è´¥")
            sys.exit(1)
        
        if not generator.setup_output_directory():
            generator.logger.error("è¾“å‡ºç›®å½•è®¾ç½®å¤±è´¥")
            sys.exit(1)
        
        # æ¦‚å¿µæ¨¡å¼éªŒè¯
        if args.concept and not args.prime_with_folder:
            generator.logger.info(f"æ£€æµ‹åˆ°æ¦‚å¿µæ¨¡å¼ï¼Œæ¦‚å¿µåç§°: {args.concept}")
            # è®¾ç½®æ¦‚å¿µæ¨¡å¼æ ‡å¿—
            generator.concept_mode = True
            
            # å°è¯•åŠ è½½æ¦‚å¿µé…ç½®æ–‡ä»¶
            concept_profile_file: str = os.path.join(generator.output_dir or '', f'{generator.project_name or "concept"}_concept_profile.json')  # type: ignore
            if os.path.exists(concept_profile_file):  # type: ignore
                try:
                    with open(concept_profile_file, 'r', encoding='utf-8') as f:  # type: ignore
                        generator.concept_profile = json.load(f)
                    generator.logger.success(f"æ¦‚å¿µé…ç½®æ–‡ä»¶å·²åŠ è½½: {concept_profile_file}")
                except Exception as e:
                    generator.logger.error(f"åŠ è½½æ¦‚å¿µé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                    generator.concept_profile = None
            else:
                generator.logger.warning(f"æœªæ‰¾åˆ°æ¦‚å¿µé…ç½®æ–‡ä»¶: {concept_profile_file}")
                generator.logger.warning("æ¦‚å¿µå¢å¼ºåˆ†æå°†æ— æ³•æ‰§è¡Œï¼Œè¯·å…ˆè¿è¡Œæ¦‚å¿µå­¦ä¹ é˜¶æ®µ")
                generator.concept_profile = None
        
        # ä¸€é”®æ‰§è¡Œæ¨¡å¼
        if args.run_all:
            handle_run_all_mode(generator)
        # åŸæœ‰çš„å•ç‹¬æ‰§è¡Œæ¨¡å¼
        elif args.generate_outline:
            handle_generate_outline_mode(generator, args)
        elif args.generate_review:
            handle_generate_review_mode(generator)
        elif args.validate_review:
            if generator.load_existing_summaries():
                 validator.run_review_validation(generator)  # type: ignore
            else:
                generator.logger.error("æ— æ³•åŠ è½½æ‘˜è¦æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œé˜¶æ®µä¸€")
                sys.exit(1)
        else:
            # é»˜è®¤æ‰§è¡Œé˜¶æ®µä¸€
            handle_stage_one_mode(generator, args)
            
    except KeyboardInterrupt:
        logging.info("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(1)
    except Exception as e:
        logging.error(f"ç¨‹åºè¿è¡Œå¤±è´¥: {e}")
        logging.error("=" * 60)
        logging.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        logging.error(traceback.format_exc())
        logging.error("=" * 60)

        # æ£€æŸ¥æ˜¯å¦ä¸ºç½‘ç»œç›¸å…³å¼‚å¸¸
        import requests  # type: ignore
        if isinstance(e, (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.RequestException)):
            logging.error("æ£€æµ‹åˆ°ç½‘ç»œè¿æ¥ä¸­æ–­ã€‚")
            logging.error("ä¸ç”¨æ‹…å¿ƒï¼Œæ‚¨çš„è¿›åº¦å·²è¢«ä¿å­˜ã€‚")
            logging.error("è¯·åœ¨ç½‘ç»œæ¢å¤åï¼Œé‡æ–°è¿è¡Œæ‚¨åˆšæ‰ä½¿ç”¨çš„å‘½ä»¤ï¼Œç¨‹åºå°†ä»ä¸­æ–­çš„åœ°æ–¹ç»§ç»­ã€‚")
        else:
            logging.error("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ã€ç½‘ç»œè¿æ¥å’Œæ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

        sys.exit(1)

def parse_failure_report(failure_report_file: str, pdf_folder: Optional[str] = None) -> List[PaperInfo]:  # type: ignore
    """ä»å¤±è´¥æŠ¥å‘Šæ–‡ä»¶ä¸­è§£æå¤±è´¥çš„è®ºæ–‡ä¿¡æ¯"""
    try:
        with open(failure_report_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        papers: List[PaperInfo] = []
        
        # æŸ¥æ‰¾è®ºæ–‡æ ‡é¢˜
        import re
        title_pattern = r'ğŸ“„ æ ‡é¢˜:\s*(.+?)(?:\r?\n|$)'
        title_matches: List[str] = re.findall(title_pattern, content)
        
        for title in title_matches:
            title = title.strip()
            if title:
                logging.info(f"ä»å¤±è´¥æŠ¥å‘Šä¸­æå–åˆ°è®ºæ–‡æ ‡é¢˜: {title}")
                
                # PDFæ–‡ä»¶å¤¹è·¯å¾„å·²ç»ä½œä¸ºå‚æ•°ä¼ å…¥
                logging.info(f"PDFæ–‡ä»¶å¤¹è·¯å¾„: {pdf_folder}")
                
                # å¦‚æœæ‰¾åˆ°äº†PDFæ–‡ä»¶å¤¹ï¼Œåœ¨å…¶ä¸­æœç´¢
                pdf_path = None
                if pdf_folder and os.path.exists(pdf_folder):
                    import glob
                    # åœ¨æ–‡ä»¶å¤¹ä¸­æœç´¢åŒ…å«æ ‡é¢˜çš„PDFæ–‡ä»¶
                    pattern = os.path.join(pdf_folder, '**', '*.pdf')
                    all_pdfs = glob.glob(pattern, recursive=True)
                    
                    logging.info(f"åœ¨PDFæ–‡ä»¶å¤¹ä¸­æ‰¾åˆ° {len(all_pdfs)} ä¸ªPDFæ–‡ä»¶")
                    
                    for pdf_file in all_pdfs:
                        pdf_filename = os.path.splitext(os.path.basename(pdf_file))[0]
                        
                        # é€šç”¨çš„åŒ¹é…é€»è¾‘ï¼šæ£€æŸ¥æ ‡é¢˜å’ŒPDFæ–‡ä»¶åçš„ç›¸ä¼¼åº¦
                        # æ–¹æ³•1ï¼šæ£€æŸ¥ä½œè€…å§“åï¼ˆå¦‚æœæ ‡é¢˜ä¸­æœ‰ä¸‹åˆ’çº¿åˆ†éš”ä½œè€…ï¼‰
                        author_match = False
                        if '_' in title:
                            possible_authors = title.split('_')[-1].strip()
                            if possible_authors and possible_authors in pdf_filename:
                                author_match = True
                                logging.info(f"åŸºäºä½œè€…å§“ååŒ¹é…: {possible_authors}")
                        
                        # æ–¹æ³•2ï¼šæå–æ ‡é¢˜ä¸­çš„å…³é”®è¯è¿›è¡ŒåŒ¹é…
                        # å»é™¤å¸¸è§åœç”¨è¯ï¼Œæå–æœ‰æ„ä¹‰çš„è¯æ±‡
                        def extract_keywords(text: str) -> List[str]:
                            """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼ˆå»é™¤åœç”¨è¯ï¼‰"""
                            # å¸¸è§åœç”¨è¯ï¼ˆä¸­è‹±æ–‡ï¼‰
                            stop_words = {'çš„', 'ä¸', 'å’Œ', 'åŠ', 'åœ¨', 'å¯¹', 'ä¸º', 'äº†', 'ä¸­', 'æ˜¯', 'æœ‰', 'ä¹Ÿ', 'å°±', 'éƒ½',
                                         'the', 'and', 'or', 'in', 'on', 'at', 'for', 'to', 'of', 'a', 'an', 'the',
                                         'ç ”ç©¶', 'åˆ†æ', 'æ¢è®¨', 'åˆæ¢', 'æ€è€ƒ', 'åŸºäº', 'è§†è§’'}
                            
                            # åˆ†å‰²æˆè¯æ±‡ï¼ˆæŒ‰éå­—æ¯æ•°å­—å­—ç¬¦åˆ†å‰²ï¼‰
                            import re
                            words = re.findall(r'[\u4e00-\u9fff]+|[a-zA-Z]+', text)
                            
                            # è¿‡æ»¤åœç”¨è¯ï¼Œä¿ç•™é•¿åº¦>=2çš„è¯æ±‡
                            keywords = [word for word in words if len(word) >= 2 and word not in stop_words]
                            return keywords
                        
                        title_keywords = extract_keywords(title)
                        filename_keywords = extract_keywords(pdf_filename)
                        
                        # è®¡ç®—å…³é”®è¯é‡å åº¦ï¼ˆä½¿ç”¨æå–çš„å…³é”®è¯è¿›è¡Œæ›´å‡†ç¡®çš„åŒ¹é…ï¼‰
                        keyword_overlap = 0
                        matched_words: List[str] = []
                        for keyword in title_keywords:
                            # æ–¹æ³•1ï¼šæ£€æŸ¥ç²¾ç¡®åŒ¹é…ï¼ˆå…³é”®è¯åœ¨æ–‡ä»¶åå…³é”®è¯åˆ—è¡¨ä¸­ï¼‰
                            if keyword in filename_keywords:
                                keyword_overlap += 1
                                matched_words.append(f"[ç²¾ç¡®]{keyword}")
                            # æ–¹æ³•2ï¼šæ£€æŸ¥å­å­—ç¬¦ä¸²åŒ¹é…ï¼ˆå…³é”®è¯åœ¨PDFæ–‡ä»¶åä¸­ï¼‰
                            elif keyword in pdf_filename:
                                keyword_overlap += 1
                                matched_words.append(f"[åŒ…å«]{keyword}")
                        
                        # æ–¹æ³•3ï¼šè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
                        def calculate_similarity(str1: str, str2: str) -> float:
                            """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºé‡å å­—ç¬¦ï¼‰"""
                            # è½¬æ¢ä¸ºé›†åˆï¼ˆå»é™¤é‡å¤å­—ç¬¦ï¼‰
                            set1 = set(str1)
                            set2 = set(str2)
                            if not set1 or not set2:
                                return 0.0
                            # Jaccardç›¸ä¼¼åº¦
                            intersection = len(set1.intersection(set2))
                            union = len(set1.union(set2))
                            return intersection / union if union > 0 else 0.0
                        
                        similarity_score = calculate_similarity(title, pdf_filename)
                        
                        # åŒ¹é…æ¡ä»¶ï¼šä½œè€…åŒ¹é… æˆ– å…³é”®è¯åŒ¹é…>=2 æˆ– ç›¸ä¼¼åº¦>0.5
                        if author_match or keyword_overlap >= 2 or similarity_score > 0.5:
                            pdf_path = pdf_file
                            logging.info(f"æˆåŠŸåŒ¹é…PDFæ–‡ä»¶: {pdf_file}")
                            if author_match:
                                logging.info("åŒ¹é…åŸå› : ä½œè€…å§“å")
                            if keyword_overlap > 0:
                                logging.info(f"åŒ¹é…åˆ° {keyword_overlap} ä¸ªå…³é”®è¯: {matched_words}")
                            if similarity_score > 0.5:
                                logging.info(f"æ–‡æœ¬ç›¸ä¼¼åº¦: {similarity_score:.2f}")
                            break
                        
                        # æ–¹æ³•4ï¼šç›´æ¥åŒ…å«æ£€æŸ¥ï¼ˆå¦‚æœPDFæ–‡ä»¶ååŒ…å«æ ‡é¢˜çš„ä¸»è¦éƒ¨åˆ†ï¼‰
                        else:
                            clean_title = title.replace('â€”â€”', '').replace('_', '').replace('"', '').replace('ï¼ˆ', '').replace('ï¼‰', '')
                            clean_filename = pdf_filename.replace('_', '').replace('"', '').replace('ï¼ˆ', '').replace('ï¼‰', '')
                            
                            # å¦‚æœæ ‡é¢˜é•¿åº¦>10ä¸”è¢«æ–‡ä»¶ååŒ…å«ï¼Œæˆ–ç›¸å
                            if len(clean_title) > 10 and clean_title in clean_filename:
                                pdf_path = pdf_file
                                logging.info(f"åŸºäºæ•´ä½“å­—ç¬¦ä¸²åŒ¹é…æ‰¾åˆ°PDFæ–‡ä»¶: {pdf_file}")
                                break
                            elif len(clean_filename) > 10 and clean_filename in clean_title:
                                pdf_path = pdf_file
                                logging.info(f"åŸºäºåå‘åŒ…å«åŒ¹é…æ‰¾åˆ°PDFæ–‡ä»¶: {pdf_file}")
                                break
                
                # å¦‚æœæ‰¾åˆ°äº†PDFæ–‡ä»¶ï¼Œåˆ›å»ºè®ºæ–‡ä¿¡æ¯
                if pdf_path and os.path.exists(pdf_path):
                    paper_info: PaperInfo = {
                        'title': title,
                        'authors': [],
                        'year': 'æœªçŸ¥å¹´ä»½',
                        'journal': 'æœªçŸ¥æœŸåˆŠ',
                        'doi': '',
                        'pdf_path': pdf_path,
                        'file_index': 0
                    }
                    papers.append(paper_info)
                    logging.info(f"æˆåŠŸåˆ›å»ºå¤±è´¥è®ºæ–‡çš„é‡è¯•ä¿¡æ¯: {title}")
                else:
                    logging.warning(f"æœªæ‰¾åˆ°è®ºæ–‡æ ‡é¢˜å¯¹åº”çš„PDFæ–‡ä»¶: {title}")
                    logging.info(f"PDFæ–‡ä»¶å¤¹: {pdf_folder}")
                    logging.info(f"PDFæ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨: {os.path.exists(pdf_folder) if pdf_folder else 'None'}")
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°PDFè·¯å¾„ï¼ŒæŸ¥æ‰¾PDFæ–‡ä»¶è·¯å¾„çš„æ¨¡å¼
        if not papers:
            pdf_pattern = r'PDFæ–‡ä»¶ä¸å­˜åœ¨:\s*(.+\.pdf)'
            pdf_matches: List[str] = re.findall(pdf_pattern, content)
            
            for pdf_path in pdf_matches:
                pdf_path = pdf_path.strip()
                if pdf_path and os.path.exists(pdf_path):
                    title = os.path.splitext(os.path.basename(pdf_path))[0]
                    
                    paper_info: PaperInfo = {
                        'title': title,
                        'authors': [],
                        'year': 'æœªçŸ¥å¹´ä»½',
                        'journal': 'æœªçŸ¥æœŸåˆŠ',
                        'doi': '',
                        'pdf_path': pdf_path,
                        'file_index': 0
                    }
                    papers.append(paper_info)
        
        return papers
        
    except Exception as e:
        logging.error(f"è§£æå¤±è´¥æŠ¥å‘Šæ–‡ä»¶å‡ºé”™: {e}")
        return []

def handle_retry_failed(args: argparse.Namespace):  # type: ignore
    """å¤„ç†é‡è¯•å¤±è´¥è®ºæ–‡æ¨¡å¼"""
    if not args.project_name and not args.pdf_folder:
        logging.error("ä½¿ç”¨--retry-failedå‘½ä»¤æ—¶å¿…é¡»æä¾›--project-nameæˆ–--pdf-folderå‚æ•°ä¸­çš„ä¸€ä¸ª")
        sys.exit(1)
    
    # éªŒè¯project_nameæ ¼å¼
    if args.project_name:
        if len(args.project_name) > 100 or '\\' in args.project_name or '/' in args.project_name:
            logging.error("âŒ --project-name å‚æ•°é”™è¯¯")
            logging.error("ğŸ’¡ è¯·ä¸è¦ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼Œåº”è¯¥ä½¿ç”¨ç®€æ´çš„é¡¹ç›®åç§°")
            logging.error("ğŸ“ ç¤ºä¾‹ï¼š--project-name \"æ¡ˆä¾‹åˆ†æ\" è€Œé --project-name \"C:\\Users\\123\\Desktop\\æˆ‘çš„é¡¹ç›®\"")
            sys.exit(1)

    generator = LiteratureReviewGenerator(args.config, args.project_name, args.pdf_folder)
    generator.logger.info("*** å¤±è´¥è®ºæ–‡é‡è¯•æ¨¡å¼å·²å¯åŠ¨ ***")
    
    if not generator.load_configuration() or not generator.setup_output_directory():
        sys.exit(1)

    if not generator.load_existing_summaries():
        generator.logger.error("æœªæ‰¾åˆ°æ‘˜è¦æ–‡ä»¶ï¼Œæ— æ³•è¿›è¡Œé‡è¯•ã€‚è¯·å…ˆè¿è¡Œä¸€æ¬¡å®Œæ•´çš„åˆ†æã€‚")
        sys.exit(1)

    papers_to_retry = []
    retry_report_file = ''  # åˆå§‹åŒ–å˜é‡
    if generator.mode == "zotero":
        retry_report_file: str = os.path.join(generator.output_dir or '', f'{generator.project_name or "project"}_zotero_report_for_retry.txt')  # type: ignore
        if not os.path.exists(retry_report_file):  # type: ignore:
            generator.logger.error(f"Zoteroæ¨¡å¼é‡è¯•å¤±è´¥ï¼šæœªæ‰¾åˆ°é‡è·‘æŠ¥å‘Šæ–‡ä»¶ '{retry_report_file}'")
            sys.exit(1)
        papers_to_retry = parse_zotero_report(retry_report_file)  # type: ignore
    else:  # direct mode
        generator.logger.info("ç›´æ¥PDFæ¨¡å¼ï¼šæ­£åœ¨ä»æ‘˜è¦æ–‡ä»¶å’Œå¤±è´¥æŠ¥å‘Šä¸­è¯†åˆ«å¤±è´¥çš„è®ºæ–‡...")
        
        # é¦–å…ˆå°è¯•ä»summaries.jsonä¸­æŸ¥æ‰¾å¤±è´¥çš„è®ºæ–‡
        failed_summaries = [s for s in generator.summaries if s.get('status') == 'failed']  # type: ignore
        papers_to_retry = [s.get('paper_info') for s in failed_summaries if s.get('paper_info')]  # type: ignore
        
        # å¦‚æœæ²¡æœ‰åœ¨summaries.jsonä¸­æ‰¾åˆ°å¤±è´¥çš„è®ºæ–‡ï¼Œå°è¯•ä»å¤±è´¥æŠ¥å‘Šæ–‡ä»¶ä¸­è¯»å–
        if not papers_to_retry:
            generator.logger.info("åœ¨summaries.jsonä¸­æœªæ‰¾åˆ°å¤±è´¥çš„è®ºæ–‡ï¼Œæ­£åœ¨æ£€æŸ¥å¤±è´¥æŠ¥å‘Š...")
            failure_report_file = os.path.join(generator.output_dir or '', f'{generator.project_name or "project"}_failed_papers_report.txt')
            
            if os.path.exists(failure_report_file):
                generator.logger.info(f"æ‰¾åˆ°å¤±è´¥æŠ¥å‘Šæ–‡ä»¶: {failure_report_file}")
                try:
                    # è§£æå¤±è´¥æŠ¥å‘Šæ–‡ä»¶ï¼Œä¼ å…¥PDFæ–‡ä»¶å¤¹è·¯å¾„
                    failed_papers_from_report = parse_failure_report(failure_report_file, generator.pdf_folder)
                    if failed_papers_from_report:
                        papers_to_retry = failed_papers_from_report
                        generator.logger.info(f"ä»å¤±è´¥æŠ¥å‘Šä¸­æå–åˆ° {len(papers_to_retry)} ç¯‡éœ€è¦é‡è¯•çš„è®ºæ–‡")
                    else:
                        generator.logger.warning("å¤±è´¥æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨ä½†æ— æ³•è§£æ")
                except Exception as e:
                    generator.logger.error(f"è¯»å–å¤±è´¥æŠ¥å‘Šæ–‡ä»¶å¤±è´¥: {e}")
            else:
                generator.logger.warning(f"æœªæ‰¾åˆ°å¤±è´¥æŠ¥å‘Šæ–‡ä»¶: {failure_report_file}")

    if not papers_to_retry:
        generator.logger.success("æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡è¯•çš„å¤±è´¥è®ºæ–‡ã€‚")
        return

    generator.logger.info(f"è¯†åˆ«åˆ° {len(papers_to_retry)} ç¯‡è®ºæ–‡éœ€è¦é‡è¯•ã€‚")
    
    original_summary_count = len(generator.summaries)
    file_index_path: str = generator.config.get('Paths', {}).get('library_path', '') if generator.mode == 'zotero' and generator.config else generator.pdf_folder or ''  # type: ignore
    file_index = create_file_index(file_index_path)  # type: ignore
    performance_config = generator.config.get('Performance') or {}  # type: ignore
    max_workers = int(performance_config.get('max_workers', 3))  # type: ignore

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:  # type: ignore
        future_to_paper = {executor.submit(generator.process_paper, paper, i, file_index, len(papers_to_retry)): paper for i, paper in enumerate(papers_to_retry)}  # type: ignore
        progress_bar = tqdm(concurrent.futures.as_completed(future_to_paper), total=len(papers_to_retry), desc="[é‡è¯•æ¨¡å¼] æ­£åœ¨å¤„ç†")  # type: ignore
        for future in progress_bar:
            result: Optional[Dict[str, Any]] = future.result()  # type: ignore
            if result and result.get('status') == 'success':  # type: ignore
                # åœ¨ç›´æ¥PDFæ¨¡å¼ä¸‹ï¼Œæ›´æ–°åŸå§‹æ¡ç›®è€Œä¸æ˜¯æ·»åŠ æ–°æ¡ç›®
                if generator.mode == "direct":
                    paper_key = LiteratureReviewGenerator.get_paper_key(result.get('paper_info', {}))  # type: ignore
                    # æŸ¥æ‰¾å¹¶æ›´æ–°åŸå§‹æ¡ç›®
                    for i, summary in enumerate(generator.summaries):
                        if LiteratureReviewGenerator.get_paper_key(summary.get('paper_info', {})) == paper_key:  # type: ignore
                            generator.summaries[i] = result  # type: ignore
                            break
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŸå§‹æ¡ç›®ï¼Œåˆ™æ·»åŠ æ–°æ¡ç›®
                        generator.summaries.append(result)  # type: ignore
                else:
                    # Zoteroæ¨¡å¼ä¸‹ï¼Œç›´æ¥æ·»åŠ æ–°æ¡ç›®
                    generator.summaries.append(result)  # type: ignore
            else:
                # å¤„ç†å¤±è´¥çš„è®ºæ–‡
                failed_paper: Dict[str, Any] = result or {'paper_info': future_to_paper[future], 'failure_reason': 'æœªçŸ¥é‡è¯•é”™è¯¯'}  # type: ignore
                if generator.mode == "direct":
                    paper_key = LiteratureReviewGenerator.get_paper_key(failed_paper.get('paper_info', {}))
                    # æŸ¥æ‰¾å¹¶æ›´æ–°åŸå§‹æ¡ç›®
                    for i, summary in enumerate(generator.summaries):
                        if LiteratureReviewGenerator.get_paper_key(summary.get('paper_info', {})) == paper_key:
                            generator.summaries[i] = failed_paper  # type: ignore
                            break
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŸå§‹æ¡ç›®ï¼Œåˆ™æ·»åŠ æ–°æ¡ç›®
                        generator.summaries.append(failed_paper)  # type: ignore
                    
                    # ç¡®ä¿å¤±è´¥çš„è®ºæ–‡ä¹Ÿè¢«æ·»åŠ åˆ°failed_papersåˆ—è¡¨ï¼Œä»¥ä¾¿ç”Ÿæˆå¤±è´¥æŠ¥å‘Š
                    generator.failed_papers.append(failed_paper)  # type: ignore
                else:
                    # Zoteroæ¨¡å¼ä¸‹ï¼Œç›´æ¥æ·»åŠ åˆ°å¤±è´¥åˆ—è¡¨
                    generator.failed_papers.append(failed_paper)  # type: ignore

    generator.save_summaries()
    
    # è°ƒç”¨ç»Ÿä¸€çš„æŠ¥å‘Šç”Ÿæˆæ–¹æ³•
    generator.generate_all_reports()

    # è®¡ç®—æ–°å¢æˆåŠŸçš„è®ºæ–‡æ•°é‡
    success_count = len([s for s in generator.summaries if s.get('status') == 'success'])  # type: ignore
    original_success = len([s for s in generator.summaries[:original_summary_count] if s.get('status') == 'success'])  # type: ignore
    newly_succeeded = success_count - original_success
    failed_count = len([s for s in generator.summaries if s.get('status') == 'failed'])  # type: ignore
    generator.logger.success(f"é‡è¯•å®Œæˆï¼æ–°å¢æˆåŠŸ {newly_succeeded} ç¯‡ï¼Œä»ç„¶å¤±è´¥ {failed_count} ç¯‡ã€‚")  # type: ignore
    
    if not generator.failed_papers and generator.mode == 'zotero' and os.path.exists(retry_report_file):
        try:
            os.remove(retry_report_file)
            generator.logger.info(f"æ‰€æœ‰å¤±è´¥è®ºæ–‡å‡å·²æˆåŠŸé‡è¯•ï¼Œå·²è‡ªåŠ¨åˆ é™¤é‡è·‘æŠ¥å‘Šæ–‡ä»¶: {retry_report_file}")
        except Exception as e:
            generator.logger.warning(f"æ— æ³•è‡ªåŠ¨åˆ é™¤é‡è·‘æŠ¥å‘Šæ–‡ä»¶: {e}")

def handle_merge_mode(args: argparse.Namespace):  # type: ignore
    """å¤„ç†åˆå¹¶æ¨¡å¼"""
    # éªŒè¯å‚æ•°ï¼šå¿…é¡»æä¾›project_nameæˆ–pdf_folderä¸­çš„ä¸€ä¸ª
    if not args.project_name and not args.pdf_folder:
        logging.error("ä½¿ç”¨--mergeå‘½ä»¤æ—¶å¿…é¡»æä¾›--project-nameæˆ–--pdf-folderå‚æ•°ä¸­çš„ä¸€ä¸ª")
        sys.exit(1)
    
    # éªŒè¯project_nameæ ¼å¼
    if args.project_name:
        if len(args.project_name) > 100 or '\\' in args.project_name or '/' in args.project_name:
            logging.error("âŒ --project-name å‚æ•°é”™è¯¯")
            logging.error("ğŸ’¡ è¯·ä¸è¦ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼Œåº”è¯¥ä½¿ç”¨ç®€æ´çš„é¡¹ç›®åç§°")
            logging.error("ğŸ“ ç¤ºä¾‹ï¼š--project-name \"æ¡ˆä¾‹åˆ†æ\" è€Œé --project-name \"C:\\Users\\123\\Desktop\\æˆ‘çš„é¡¹ç›®\"")
            sys.exit(1)
    
    generator = LiteratureReviewGenerator(args.config, args.project_name, args.pdf_folder)
    generator.logger.info("*** åˆå¹¶æ¨¡å¼å·²å¯åŠ¨ ***")
    generator.logger.info("=" * 60)
    
    # æ ¹æ®æ¨¡å¼ç¡®å®šé¡¹ç›®åç§°å’Œæ–‡ä»¶è·¯å¾„
    try:
        # åŠ è½½é…ç½®ä»¥è·å–è¾“å‡ºè·¯å¾„
        if not generator.load_configuration():
            generator.logger.error("é…ç½®åŠ è½½å¤±è´¥")
            sys.exit(1)
        
        # è®¾ç½®è¾“å‡ºç›®å½•ä»¥ç¡®å®šé¡¹ç›®åç§°
        if not generator.setup_output_directory():
            generator.logger.error("è¾“å‡ºç›®å½•è®¾ç½®å¤±è´¥")
            sys.exit(1)
        
        # ç¡®å®šä¸»æ–‡ä»¶è·¯å¾„
        main_file = generator.summary_file
        merge_file = args.merge
        
        if not main_file or not os.path.exists(main_file):
            generator.logger.error(f"ä¸»æ–‡ä»¶ä¸å­˜åœ¨: {main_file}")
            return
        
        if not os.path.exists(merge_file):
            generator.logger.error(f"åˆå¹¶æ–‡ä»¶ä¸å­˜åœ¨: {merge_file}")
            return
        
        # è¯»å–ä¸¤ä¸ªæ–‡ä»¶ï¼ˆæ·»åŠ robustç¼–ç å¤„ç†ï¼‰
        try:
            with open(main_file, 'r', encoding='utf-8') as f:  # type: ignore
                main_data = json.load(f)  # type: ignore
        except UnicodeDecodeError:
            try:
                with open(main_file, 'r', encoding='gbk') as f:  # type: ignore
                    content = f.read()
                content = content.encode('gbk').decode('utf-8')
                main_data = json.loads(content)  # type: ignore
            except (UnicodeDecodeError, UnicodeError):
                with open(main_file, 'r', encoding='utf-8', errors='ignore') as f:  # type: ignore
                    main_data = json.load(f)  # type: ignore
        
        try:
            with open(merge_file, 'r', encoding='utf-8') as f:  # type: ignore
                merge_data = json.load(f)  # type: ignore
        except UnicodeDecodeError:
            try:
                with open(merge_file, 'r', encoding='gbk') as f:  # type: ignore
                    content = f.read()
                content = content.encode('gbk').decode('utf-8')
                merge_data = json.loads(content)  # type: ignore
            except (UnicodeDecodeError, UnicodeError):
                with open(merge_file, 'r', encoding='utf-8', errors='ignore') as f:  # type: ignore
                    merge_data = json.load(f)  # type: ignore
        
        if not isinstance(main_data, list) or not isinstance(merge_data, list):  # type: ignore
            generator.logger.error("æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå¿…é¡»æ˜¯JSONæ•°ç»„")
            return
        
        # æ™ºèƒ½åˆå¹¶ï¼šä»¥åˆå¹¶æ–‡ä»¶ä¸­çš„è®°å½•ä¸ºå‡†
        generator.logger.info(f"ä¸»æ–‡ä»¶åŒ…å« {len(main_data)} ç¯‡è®ºæ–‡")  # type: ignore
        generator.logger.info(f"åˆå¹¶æ–‡ä»¶åŒ…å« {len(merge_data)} ç¯‡è®ºæ–‡")  # type: ignore
        
        # åˆ›å»ºåŸºäºDOIçš„ç´¢å¼•ï¼ˆå¦‚æœæ²¡æœ‰DOIåˆ™ä½¿ç”¨æ ‡é¢˜+ä½œè€…ï¼‰
        def get_paper_key(paper: 'Dict[str, Any] | PaperInfo'):  # type: ignore
            paper_info = paper.get('paper_info', {})  # type: ignore
            return paper_info.get('doi', f"{paper_info.get('title', '')}_{paper_info.get('authors', [])}")  # type: ignore
        
        # æ„å»ºä¸»æ–‡ä»¶çš„ç´¢å¼•
        main_index = {get_paper_key(paper): i for i, paper in enumerate(main_data)}  # type: ignore
        
        # åˆå¹¶æ•°æ®
        merged_count = 0
        added_count = 0
        
        for merge_paper in merge_data:  # type: ignore
            merge_key = get_paper_key(merge_paper)  # type: ignore
            
            if merge_key in main_index:
                # æ›´æ–°ç°æœ‰è®°å½•
                main_index_pos = main_index[merge_key]
                main_data[main_index_pos] = merge_paper  # type: ignore
                merged_count += 1
            else:
                # æ·»åŠ æ–°è®°å½•
                main_data.append(merge_paper)  # type: ignore
                added_count += 1
        
        # ä¿å­˜åˆå¹¶ç»“æœ
        backup_file: str = f"{main_file}.backup.{int(time.time())}"  # type: ignore
        os.rename(main_file, backup_file)  # type: ignore
        generator.logger.info(f"å·²åˆ›å»ºå¤‡ä»½æ–‡ä»¶: {backup_file}")  # type: ignore
        
        with open(main_file, 'w', encoding='utf-8') as f:  # type: ignore
            json.dump(main_data, f, ensure_ascii=False, indent=2)  # type: ignore
        
        generator.logger.success("åˆå¹¶å®Œæˆï¼")  # type: ignore
        generator.logger.info(f"æ›´æ–°è®°å½•: {merged_count} ç¯‡")  # type: ignore
        generator.logger.info(f"æ–°å¢è®°å½•: {added_count} ç¯‡")  # type: ignore
        generator.logger.info(f"æ€»è®°å½•æ•°: {len(main_data)} ç¯‡")  # type: ignore
        
    except Exception as e:
        generator.logger.error(f"åˆå¹¶è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        traceback.print_exc()

def handle_run_all_mode(generator: 'LiteratureReviewGenerator'):  # type: ignore
    """å¤„ç†ä¸€é”®æ‰§è¡Œæ¨¡å¼"""
    generator.logger.info("*** 'ä¸€é”®æ‰§è¡Œ'æ¨¡å¼å·²å¯åŠ¨ ***")
    generator.logger.info("=" * 60)
    
    # æ‰§è¡Œé˜¶æ®µä¸€
    generator.logger.info("å¼€å§‹æ‰§è¡Œé˜¶æ®µä¸€ï¼šæ–‡çŒ®åˆ†æ...")
    stage1_success = generator.run_stage_one()
    
    if stage1_success:
        generator.logger.success("\né˜¶æ®µä¸€æ‰§è¡ŒæˆåŠŸï¼")
        generator.logger.info("å¼€å§‹æ‰§è¡Œé˜¶æ®µäºŒï¼šæ–‡çŒ®ç»¼è¿°ç”Ÿæˆ...")
        
        # æ‰§è¡Œé˜¶æ®µäºŒï¼šå…ˆç”Ÿæˆå¤§çº²ï¼Œå†ç”Ÿæˆå…¨æ–‡
        generator.logger.info("å¼€å§‹æ‰§è¡Œé˜¶æ®µäºŒç¬¬ä¸€æ­¥ï¼šç”Ÿæˆå¤§çº²...")
        outline_success = generator.generate_literature_review_outline()
        
        if outline_success:
            generator.logger.success("å¤§çº²ç”ŸæˆæˆåŠŸï¼")
            generator.logger.info("å¼€å§‹æ‰§è¡Œé˜¶æ®µäºŒç¬¬äºŒæ­¥ï¼šä»å¤§çº²ç”Ÿæˆå…¨æ–‡...")
            stage2_success = generator.generate_full_review_from_outline()
        else:
            stage2_success = False
        
        if stage2_success:
            generator.logger.success("\nä¸€é”®æ‰§è¡Œæ¨¡å¼å®Œæˆï¼æ‰€æœ‰ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
        else:
            generator.logger.error("\né˜¶æ®µäºŒæ‰§è¡Œå¤±è´¥ï¼")
            sys.exit(1)
    else:
        generator.logger.error("\né˜¶æ®µä¸€æ‰§è¡Œå¤±è´¥ï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œé˜¶æ®µäºŒï¼")
        sys.exit(1)

def handle_generate_outline_mode(generator: 'LiteratureReviewGenerator', args: argparse.Namespace):  # type: ignore
    """å¤„ç†ç”Ÿæˆå¤§çº²æ¨¡å¼"""
    success = generator.generate_literature_review_outline()
    if success:
        generator.logger.success("\nå¤§çº²ç”ŸæˆæˆåŠŸï¼æ–‡çŒ®ç»¼è¿°å¤§çº²å·²ç”Ÿæˆå®Œæˆ")
        generator.logger.info(f"æ‚¨å¯ä»¥ç¼–è¾‘å¤§çº²æ–‡ä»¶ï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤ç”Ÿæˆå®Œæ•´ç»¼è¿°ï¼š")
        if args.project_name:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¦‚å¿µæ¨¡å¼
            if args.concept:
                generator.logger.info(f"å‘½ä»¤: python main.py --project-name \"{args.project_name}\" --concept \"{args.concept}\" --generate-review")
            else:
                generator.logger.info(f"å‘½ä»¤: python main.py --project-name \"{args.project_name}\" --generate-review")
        elif args.pdf_folder:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¦‚å¿µæ¨¡å¼
            if args.concept:
                generator.logger.info(f"å‘½ä»¤: python main.py --pdf-folder \"{args.pdf_folder}\" --concept \"{args.concept}\" --generate-review")
            else:
                generator.logger.info(f"å‘½ä»¤: python main.py --pdf-folder \"{args.pdf_folder}\" --generate-review")
    else:
        generator.logger.error("\nå¤§çº²ç”Ÿæˆå¤±è´¥ï¼")
        sys.exit(1)

def handle_generate_review_mode(generator: 'LiteratureReviewGenerator'):  # type: ignore
    """å¤„ç†ç”Ÿæˆç»¼è¿°æ¨¡å¼"""
    success = generator.generate_full_review_from_outline()
    if success:
        generator.logger.success("\næ–‡çŒ®ç»¼è¿°ç”ŸæˆæˆåŠŸï¼å®Œæ•´ç»¼è¿°å·²ç”Ÿæˆå®Œæˆ")
    else:
        generator.logger.error("\næ–‡çŒ®ç»¼è¿°ç”Ÿæˆå¤±è´¥ï¼")
        sys.exit(1)

def handle_stage_one_mode(generator: 'LiteratureReviewGenerator', args: argparse.Namespace):  # type: ignore
    """å¤„ç†é˜¶æ®µä¸€æ¨¡å¼ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰"""
    generator.logger.info("*** é˜¶æ®µä¸€æ¨¡å¼å·²å¯åŠ¨ ***")
    generator.logger.info("=" * 60)
    
    # æ‰§è¡Œé˜¶æ®µä¸€
    generator.logger.info("å¼€å§‹æ‰§è¡Œé˜¶æ®µä¸€ï¼šæ–‡çŒ®åˆ†æ...")
    stage1_success = generator.run_stage_one()
    
    if stage1_success:
        generator.logger.success("\né˜¶æ®µä¸€æ‰§è¡ŒæˆåŠŸï¼")
        generator.logger.info("æ‚¨ç°åœ¨å¯ä»¥ç»§ç»­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š")
        if args.project_name:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¦‚å¿µæ¨¡å¼
            if args.concept:
                generator.logger.info(f"ç”Ÿæˆå¤§çº²: python main.py --project-name \"{args.project_name}\" --concept \"{args.concept}\" --generate-outline")
                generator.logger.info(f"ä¸€é”®ç”Ÿæˆç»¼è¿°: python main.py --project-name \"{args.project_name}\" --concept \"{args.concept}\" --run-all")
            else:
                generator.logger.info(f"ç”Ÿæˆå¤§çº²: python main.py --project-name \"{args.project_name}\" --generate-outline")
                generator.logger.info(f"ä¸€é”®ç”Ÿæˆç»¼è¿°: python main.py --project-name \"{args.project_name}\" --run-all")
        elif args.pdf_folder:
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ¦‚å¿µæ¨¡å¼
            if args.concept:
                generator.logger.info(f"ç”Ÿæˆå¤§çº²: python main.py --pdf-folder \"{args.pdf_folder}\" --concept \"{args.concept}\" --generate-outline")
                generator.logger.info(f"ä¸€é”®ç”Ÿæˆç»¼è¿°: python main.py --pdf-folder \"{args.pdf_folder}\" --concept \"{args.concept}\" --run-all")
            else:
                generator.logger.info(f"ç”Ÿæˆå¤§çº²: python main.py --pdf-folder \"{args.pdf_folder}\" --generate-outline")
                generator.logger.info(f"ä¸€é”®ç”Ÿæˆç»¼è¿°: python main.py --pdf-folder \"{args.pdf_folder}\" --run-all")
    else:
        generator.logger.error("\né˜¶æ®µä¸€æ‰§è¡Œå¤±è´¥ï¼")
        sys.exit(1)


def main() -> None:  # type: ignore
    """ä¸»å‡½æ•°ï¼Œå¤„ç†å‘½ä»¤è¡Œå‚æ•°å’Œæ‰§è¡Œç›¸åº”æ“ä½œ"""
    
    parser = argparse.ArgumentParser(
        description="llm_reviewer_generator - æ–‡çŒ®ç»¼è¿°è‡ªåŠ¨ç”Ÿæˆå™¨",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument(
        '--config',
        type=str,
        default='config.ini',
        help='Path to the configuration file.'
    )
    parser.add_argument(
        '--project-name', 
        type=str, 
        help='ä¸ºæ‚¨çš„é¡¹ç›®æŒ‡å®šä¸€ä¸ªå”¯ä¸€çš„åç§°ï¼Œç”¨äºåˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºæ–‡ä»¶å¤¹ã€‚'
    )
    parser.add_argument(
        '--pdf-folder', 
        type=str, 
        help='ç›´æ¥æŒ‡å®šåŒ…å«PDFæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ï¼Œllm_reviewer_generatorå°†æ‰«æå¹¶å¤„ç†è¿™äº›æ–‡ä»¶ã€‚'
    )
    parser.add_argument(
        '--run-all', 
        action='store_true', 
        help='ä¸€é”®è¿è¡Œæ‰€æœ‰é˜¶æ®µï¼šä»æ–‡çŒ®åˆ†æåˆ°æœ€ç»ˆç”ŸæˆWordç‰ˆæ–‡çŒ®ç»¼è¿°ã€‚'
    )
    parser.add_argument(
        '--analyze-only', 
        action='store_true', 
        help='ä»…è¿è¡Œé˜¶æ®µä¸€ï¼šåˆ†ææ–‡çŒ®å¹¶ç”Ÿæˆæ‘˜è¦ã€‚'
    )
    parser.add_argument(
        '--generate-outline', 
        action='store_true', 
        help='ä»…è¿è¡Œé˜¶æ®µäºŒï¼šæ ¹æ®ç°æœ‰æ‘˜è¦ç”Ÿæˆæ–‡çŒ®ç»¼è¿°å¤§çº²ã€‚'
    )
    parser.add_argument(
        '--generate-review', 
        action='store_true', 
        help='ä»…è¿è¡Œé˜¶æ®µä¸‰ï¼šæ ¹æ®ç°æœ‰å¤§çº²å’Œæ‘˜è¦ç”Ÿæˆå®Œæ•´çš„Wordç‰ˆæ–‡çŒ®ç»¼è¿°ã€‚'
    )
    parser.add_argument(
        '--validate-review',
        action='store_true',
        help='ï¼ˆåœ¨ç»¼è¿°ç”Ÿæˆåè¿è¡Œï¼‰å¯¹ç”Ÿæˆçš„Wordç»¼è¿°è¿›è¡Œå¼•ç”¨å’Œè§‚ç‚¹éªŒè¯ã€‚'
    )
    parser.add_argument(
        '--setup', 
        action='store_true', 
        help='è¿è¡Œäº¤äº’å¼è®¾ç½®å‘å¯¼ï¼Œåˆ›å»ºæˆ–æ›´æ–°config.iniæ–‡ä»¶ã€‚'
    )
    parser.add_argument('--prime-with-folder', type=str, help='Path to a folder with seed papers for concept priming.')
    parser.add_argument('--concept', type=str, help='The name of the concept to be primed.')
    parser.add_argument('--retry-failed', action='store_true', help='Retry processing failed papers from a previous run.')
    parser.add_argument('--merge', type=str, help='Path to a summaries.json file to merge into the main project.')

    args = parser.parse_args()
    dispatch_command(args)

if __name__ == "__main__":
    main()
