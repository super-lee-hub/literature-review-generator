#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éªŒè¯ä¸ä¿®æ­£æ¨¡å—
è´Ÿè´£å¯¹AIç”Ÿæˆçš„å†…å®¹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚
"""
import os
import json
import re
import traceback
from typing import Optional, Dict, Any, List
from datetime import datetime
import configparser

# å¯¼å…¥ç±»å‹å®šä¹‰
from models import APIConfig  # type: ignore

# ä¼˜é›…åœ°å¤„ç†å¯é€‰ä¾èµ–ï¼Œç¡®ä¿æ¨¡å—çš„ç‹¬ç«‹å¥å£®æ€§
try:
    from docx import Document  # type: ignore
    DOCX_AVAILABLE = True  # type: ignore
except ImportError:
    DOCX_AVAILABLE = False  # type: ignore
    Document = None  # type: ignore

try:
    from tqdm import tqdm  # type: ignore
    TQDM_AVAILABLE = True  # type: ignore
except ImportError:
    TQDM_AVAILABLE = False  # type: ignore
    from typing import Any, Optional, Iterator
    class tqdm:
        def __init__(self, iterable: Optional[Any] = None, **kwargs: Any):
            self.iterable: Any = iterable if iterable else []  # type: ignore
        def __iter__(self) -> Iterator[Any]:
            return iter(self.iterable)
        def set_postfix_str(self, s: str) -> None:
            pass

# å¯¼å…¥ä¸»ç¨‹åºä¸­çš„AIæ¥å£è°ƒç”¨å‡½æ•°
from ai_interface import _call_ai_api  # type: ignore

def validate_paper_analysis(generator_instance: Any, pdf_text: str, ai_result: Dict[str, Any],
                           use_cache: bool = True) -> Dict[str, Any]:
    """
    [ç¬¬ä¸€é˜¶æ®µéªŒè¯] å¯¹å•ç¯‡è®ºæ–‡çš„AIåˆ†æç»“æœè¿›è¡Œäº¤å‰éªŒè¯å’Œä¿®æ­£ã€‚
    å¢å¼ºå¼‚å¸¸å¤„ç†å’Œè¾“å…¥éªŒè¯ï¼Œæ”¯æŒéªŒè¯ç»“æœç¼“å­˜

    Args:
        generator_instance: æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨å®ä¾‹
        pdf_text: PDFå…¨æ–‡å†…å®¹
        ai_result: AIåˆ†æç»“æœ
        use_cache: æ˜¯å¦ä½¿ç”¨éªŒè¯ç»“æœç¼“å­˜ï¼ˆæé«˜æ€§èƒ½ï¼‰

    Returns:
        ä¿®æ­£åçš„AIåˆ†æç»“æœ
    """
    # è¾“å…¥éªŒè¯
    if not pdf_text:
        generator_instance.logger.warning("PDFæ–‡æœ¬ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡éªŒè¯")
        return ai_result

    if not ai_result:
        generator_instance.logger.warning("AIåˆ†æç»“æœä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡éªŒè¯")
        return ai_result

    # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºç¼“å­˜
    content_hash: Optional[str] = None
    cache_file_path: Optional[str] = None
    if use_cache:
        import hashlib
        paper_info: Any = ai_result.get('paper_info') or {}  # type: ignore
        content_str = pdf_text[:1000] + str(paper_info.get('title', '')) + str(paper_info.get('authors', []))  # type: ignore
        content_hash = hashlib.md5(content_str.encode('utf-8')).hexdigest()

        # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_dir = os.path.join(generator_instance.output_dir, 'cache')  # type: ignore
        try:
            os.makedirs(cache_dir, exist_ok=True)
            cache_file_path = os.path.join(cache_dir, f'{content_hash}.json')
        except Exception as _:  # type: ignore
            generator_instance.logger.warning(f"åˆ›å»ºç¼“å­˜ç›®å½•å¤±è´¥: {_}ï¼Œå°†è·³è¿‡ç¼“å­˜")  # type: ignore
            cache_file_path = None

    # æ£€æŸ¥ç¼“å­˜
    if use_cache and content_hash and cache_file_path and os.path.exists(cache_file_path):
        try:
            with open(cache_file_path, 'r', encoding='utf-8') as f:
                cached_result = json.load(f)
            generator_instance.logger.info("ä»ç¼“å­˜ä¸­åŠ è½½éªŒè¯ç»“æœ")
            return cached_result
        except Exception as e:
            generator_instance.logger.warning(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°éªŒè¯")

    generator_instance.logger.info("å¯åŠ¨ç¬¬ä¸€é˜¶æ®µäº¤å‰éªŒè¯...")

    # é¢„æ£€æŸ¥ï¼šå¦‚æœæ‘˜è¦åŒ…å«å ä½ç¬¦'...'ï¼Œè·³è¿‡éªŒè¯ï¼ˆå› ä¸ºéªŒè¯AIä¼šé”™è¯¯åœ°å¡«å……å®ƒï¼‰
    try:
        common_core = ai_result.get('common_core', {})
        placeholder_fields: List[str] = []
        
        # æ£€æŸ¥æ‰€æœ‰å­—æ®µæ˜¯å¦åŒ…å«'...'
        for field, value in common_core.items():
            if isinstance(value, str) and '...' in value:
                placeholder_fields.append(field)
            elif isinstance(value, list):
                for i, item in enumerate(value):  # type: ignore
                    if isinstance(item, str) and '...' in item:
                        placeholder_fields.append(f"{field}[{i}]")
        
        if placeholder_fields:
            generator_instance.logger.warning(f"å‘ç°å ä½ç¬¦'...'åœ¨å­—æ®µ: {', '.join(placeholder_fields)}ï¼Œè·³è¿‡éªŒè¯ä»¥é¿å…é”™è¯¯å¡«å……")
            generator_instance.logger.info("å†…å®¹è´¨é‡æ£€æŸ¥é€šè¿‡ï¼ˆè·³è¿‡éªŒè¯ï¼‰")
            return ai_result
    except Exception as e:
        generator_instance.logger.warning(f"é¢„æ£€æŸ¥å ä½ç¬¦æ—¶å‡ºé”™: {e}ï¼Œç»§ç»­æ­£å¸¸éªŒè¯æµç¨‹")

    try:
        # å®‰å…¨è·å–é…ç½®
        validator_config: Dict[str, str] = generator_instance.config.get('Validator_API', {})
        if not validator_config:
            generator_instance.logger.error("æœªæ‰¾åˆ°[Validator_API]é…ç½®æ®µï¼Œè·³è¿‡éªŒè¯ã€‚")  # type: ignore
            return ai_result

        validator_api_config: Dict[str, str] = {  # type: ignore
            'api_key': validator_config.get('api_key', ''),  # type: ignore
            'model': validator_config.get('model', ''),  # type: ignore
            'api_base': validator_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
        }  # type: ignore

        # éªŒè¯é…ç½®å®Œæ•´æ€§
        if not validator_api_config['api_key'] or not validator_api_config['api_key'].strip():
            generator_instance.logger.error("Validator_APIçš„api_keyæœªé…ç½®æˆ–ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        if not validator_api_config['model'] or not validator_api_config['model'].strip():
            generator_instance.logger.error("Validator_APIçš„modelæœªé…ç½®æˆ–ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        # ä½¿ç”¨ä¸¥æ ¼éªŒè¯æç¤ºè¯ï¼Œåªæ£€æŸ¥å®¢è§‚äº‹å®é”™è¯¯
        prompt_file_path: str = 'prompts/prompt_validate_analysis_strict.txt'
        try:
            with open(prompt_file_path, 'r', encoding='utf-8') as f:
                prompt_template = f.read()
        except FileNotFoundError:
            generator_instance.logger.error(f"æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_file_path}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result
        except UnicodeDecodeError:
            generator_instance.logger.error(f"æç¤ºè¯æ–‡ä»¶ç¼–ç é”™è¯¯: {prompt_file_path}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result
        except Exception as e:
            generator_instance.logger.error(f"è¯»å–æç¤ºè¯æ–‡ä»¶å¤±è´¥: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        # å®‰å…¨ç”Ÿæˆæç¤ºè¯
        try:
            summary_str: str = json.dumps(ai_result, ensure_ascii=False, indent=2)
            max_text_len: int = 800000  # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé˜²æ­¢APIè°ƒç”¨è¶…é™

            # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
            truncated_pdf_text = pdf_text[:max_text_len] if len(pdf_text) > max_text_len else pdf_text

            final_prompt = prompt_template.replace('{{PAPER_FULL_TEXT}}', truncated_pdf_text)
            final_prompt = final_prompt.replace('{{GENERATED_SUMMARY}}', summary_str)
        except Exception as e:
            generator_instance.logger.error(f"ç”ŸæˆéªŒè¯æç¤ºè¯å¤±è´¥: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        system_prompt = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯äº‹å®æ ¸æŸ¥å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹æ¯”è®ºæ–‡åŸæ–‡å’ŒAIç”Ÿæˆçš„æ‘˜è¦ï¼Œæ‰¾å‡ºå¹¶ä¿®æ­£æ‘˜è¦ä¸­çš„ä»»ä½•ä¸å‡†ç¡®ä¹‹å¤„ã€‚"

        # è°ƒç”¨éªŒè¯API
        try:
            # ä»é…ç½®ä¸­è¯»å–APIå‚æ•°
            validator_max_tokens: int = int((generator_instance.config.get('API_Parameters') or {}).get('validator_max_tokens', 4096))  # type: ignore
            validator_temperature: float = float((generator_instance.config.get('API_Parameters') or {}).get('validator_temperature', 0.3))  # type: ignore

            validation_report = _call_ai_api(
                final_prompt,
                validator_api_config,  # type: ignore
                system_prompt,
                max_tokens=validator_max_tokens,
                temperature=validator_temperature,
                response_format="json",
                logger=generator_instance.logger  # type: ignore
            )  # type: ignore
        except Exception as e:
            generator_instance.logger.error(f"è°ƒç”¨éªŒè¯APIå¤±è´¥: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        # å¤„ç†éªŒè¯ç»“æœ
        if not validation_report:
            generator_instance.logger.error("éªŒè¯è¿‡ç¨‹è¿”å›ç©ºæŠ¥å‘Šï¼Œå°†ä½¿ç”¨æœªç»æ ¸å®çš„æ‘˜è¦ã€‚")
            return ai_result

        if not validation_report:
            generator_instance.logger.error("éªŒè¯æŠ¥å‘Šæ ¼å¼æ— æ•ˆï¼Œå°†ä½¿ç”¨æœªç»æ ¸å®çš„æ‘˜è¦ã€‚")
            return ai_result

        # æ£€æŸ¥ä¸€è‡´æ€§å¹¶åº”ç”¨ä¿®æ­£
        is_consistent: bool = validation_report.get("is_consistent", True)
        if not is_consistent:
            feedback: str = validation_report.get('feedback', 'æ— åé¦ˆä¿¡æ¯')
            generator_instance.logger.warn(f"éªŒè¯å‘ç°ä¸ä¸€è‡´: {feedback}")

            corrections: List[Dict[str, Any]] = validation_report.get("corrections", [])
            if not corrections:
                generator_instance.logger.info("æŠ¥å‘Šå­˜åœ¨ä¸ä¸€è‡´ï¼Œä½†æœªæä¾›å…·ä½“ä¿®æ­£é¡¹ã€‚")
                return ai_result

            # ğŸ†• æ™ºèƒ½åº”ç”¨ä¿®æ­£ï¼šå¼•å…¥"æ™ºèƒ½è¿½åŠ "ç­–ç•¥
            applied_corrections: int = 0
            for i, correction in enumerate(corrections, 1):
                try:
                    if not correction:
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}æ ¼å¼æ— æ•ˆï¼Œè·³è¿‡")
                        continue

                    field_to_correct = correction.get("field")
                    corrected_value = correction.get("corrected_value")

                    if not field_to_correct or not isinstance(field_to_correct, str):
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ç¼ºå°‘å­—æ®µåæˆ–å­—æ®µåæ— æ•ˆï¼Œè·³è¿‡")
                        continue

                    if corrected_value is None:
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ç¼ºå°‘ä¿®æ­£å€¼ï¼Œè·³è¿‡")
                        continue
                    
                    # æ£€æŸ¥ä¿®æ­£å€¼çš„æœ‰æ•ˆæ€§
                    if isinstance(corrected_value, str) and corrected_value.strip() == '':
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ä¿®æ­£å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè·³è¿‡")
                        continue
                    
                    if isinstance(corrected_value, str) and len(corrected_value.strip()) < 3:
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ä¿®æ­£å€¼è¿‡çŸ­({len(corrected_value.strip())}å­—ç¬¦): '{corrected_value}'ï¼Œè·³è¿‡")
                        continue

                    # å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®
                    keys: List[str] = field_to_correct.split('.')
                    temp_dict: Dict[str, Any] = ai_result

                    # å®‰å…¨å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®
                    for key in keys[:-1]:
                        if key not in temp_dict:
                            temp_dict[key] = {}
                        elif not isinstance(temp_dict[key], dict):
                            generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}çš„ç›®æ ‡è·¯å¾„ '{field_to_correct}' åŒ…å«éå­—å…¸ç±»å‹ï¼Œè·³è¿‡")
                            break
                        temp_dict = temp_dict[key]
                    else:
                        field_name = keys[-1]
                        original_value = temp_dict.get(field_name, '')
                        
                        # è®°å½•ä¿®æ­£å‰çŠ¶æ€
                        generator_instance.logger.info(f"ğŸ” ä¿®æ­£å‰: {field_to_correct} = '{str(original_value)[:100]}...' (é•¿åº¦: {len(str(original_value))})")
                        generator_instance.logger.info(f"ğŸ” ä¿®æ­£å€¼: '{str(corrected_value)[:100]}...' (é•¿åº¦: {len(str(corrected_value))})")
                        
                        # ğŸ¯ æ™ºèƒ½åˆ†æ”¯å¤„ç†ç­–ç•¥
                        is_original_empty = (not original_value or 
                                           original_value in ['æœªæä¾›ç›¸å…³ä¿¡æ¯', 'æœªæåŠ', '', 'N/A', '...'])
                        is_corrected_valid = (corrected_value and 
                                             corrected_value not in ['æœªæä¾›ç›¸å…³ä¿¡æ¯', 'æœªæåŠ', '', 'N/A'])
                        
                        if isinstance(original_value, str) and isinstance(corrected_value, str):
                            original_len = len(original_value)
                            corrected_len = len(corrected_value)
                            
                            # æƒ…å†µAï¼šå®Œå…¨æ›¿æ¢ - ä¿®æ­£å€¼é•¿åº¦æ˜¾è‘—å¤§äºåŸå€¼ï¼ˆ>80%ï¼‰ï¼Œæˆ–è€…åŸå€¼ä¸ºç©º/å ä½ç¬¦
                            # æé«˜é˜ˆå€¼ä»0.6åˆ°0.8ï¼Œé¿å…è¿‡çŸ­ä¿®æ­£å¯¼è‡´ä¿¡æ¯ä¸¢å¤±
                            if is_original_empty or corrected_len > original_len * 0.8:
                                temp_dict[field_name] = corrected_value
                                generator_instance.logger.info(f"âœ… å­—æ®µ '{field_to_correct}' æ‰§è¡Œå®Œå…¨æ›¿æ¢ (ä¿®æ­£é•¿åº¦: {corrected_len}, åŸé•¿åº¦: {original_len})")
                                
                            # æƒ…å†µBï¼šç²¾å‡†æ›¿æ¢ - ä¿®æ­£å€¼è¾ƒçŸ­ï¼Œç›´æ¥æ›¿æ¢ï¼ˆä¸å†è¿½åŠ éªŒè¯å…ƒæ•°æ®ï¼‰
                            else:
                                # ç›´æ¥ä½¿ç”¨ä¿®æ­£å€¼æ›¿æ¢åŸå€¼ï¼Œé¿å…éªŒè¯å…ƒæ•°æ®æ±¡æŸ“æ‘˜è¦
                                temp_dict[field_name] = corrected_value
                                # è®°å½•ä¿®æ­£ä¾æ®ä¾›è°ƒè¯•å‚è€ƒï¼ˆä¸å­˜å‚¨åˆ°æ‘˜è¦ä¸­ï¼‰
                                justification = ""
                                for correction in corrections:
                                    if correction.get("field") == field_to_correct:
                                        justification = correction.get("justification", "")
                                        break
                                if justification:
                                    generator_instance.logger.debug(f"ğŸ”§ ä¿®æ­£ä¾æ®: {justification}")
                                generator_instance.logger.info(f"âœ… å­—æ®µ '{field_to_correct}' æ‰§è¡Œç²¾å‡†æ›¿æ¢ (ä¿®æ­£: {corrected_len}å­—ç¬¦æ›¿æ¢åŸå€¼: {original_len}å­—ç¬¦)")
                                
                        elif is_corrected_valid:
                            # éå­—ç¬¦ä¸²ç±»å‹ä¿®æ­£ï¼Œç›´æ¥æ›¿æ¢
                            temp_dict[field_name] = corrected_value
                            generator_instance.logger.info(f"âœ… å­—æ®µ '{field_to_correct}' å·²æ›¿æ¢ä¿®æ­£ä¿¡æ¯ (éå­—ç¬¦ä¸²ç±»å‹)")
                        else:
                            # ä¿®æ­£å€¼æ— æ•ˆï¼Œä¿æŒåŸå€¼
                            generator_instance.logger.warning(f"âš ï¸  å­—æ®µ '{field_to_correct}' ä¿æŒåŸå€¼ (ä¿®æ­£å€¼æ— æ•ˆ)")
                        
                        # è®°å½•ä¿®æ­£åçŠ¶æ€
                        final_value = temp_dict.get(field_name, '')
                        generator_instance.logger.info(f"ğŸ” ä¿®æ­£å: {field_to_correct} = '{str(final_value)[:100]}...' (é•¿åº¦: {len(str(final_value))})")
                        
                        applied_corrections += 1

                except Exception as e:
                    generator_instance.logger.error(f"åº”ç”¨ä¿®æ­£é¡¹{i}æ—¶å‡ºé”™: {e}")
                    continue

            generator_instance.logger.info(f"å…±åº”ç”¨äº† {applied_corrections}/{len(corrections)} ä¸ªä¿®æ­£é¡¹")

        else:
            generator_instance.logger.success("éªŒè¯é€šè¿‡ï¼Œåˆ†æå†…å®¹ä¸åŸæ–‡ä¸€è‡´ã€‚")

    except (configparser.NoSectionError, configparser.NoOptionError) as e:
        generator_instance.logger.error(f"é…ç½®æ–‡ä»¶é”™è¯¯: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚è¯·æ£€æŸ¥config.iniã€‚")
    except Exception as e:
        generator_instance.logger.error(f"éªŒè¯æ¨¡å—å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
        generator_instance.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    # ä¿å­˜éªŒè¯ç»“æœåˆ°ç¼“å­˜
    if use_cache and content_hash and cache_file_path and ai_result:
        try:
            with open(cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(ai_result, f, ensure_ascii=False, indent=2)
            generator_instance.logger.debug(f"éªŒè¯ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜: {cache_file_path}")
        except Exception as e:
            generator_instance.logger.warning(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    return ai_result

def _validate_claims_for_single_paper(source_summary: dict, sentences: List[str], api_config: dict, config: dict = None) -> Optional[dict]:  # type: ignore
    """ä¸ºå•ç¯‡è®ºæ–‡çš„æ‰€æœ‰å¼•ç”¨å¥å­è°ƒç”¨ä¸€æ¬¡AIè¿›è¡Œæ‰¹é‡éªŒè¯"""
    try:
        # è¯»å–APIå‚æ•°é…ç½®
        try:
            if config:
                max_tokens: int = int(config.get('API_Parameters', {}).get('claims_max_tokens', 8192))  # type: ignore
                temperature: float = float(config.get('API_Parameters', {}).get('claims_temperature', 0.3))  # type: ignore
            else:
                max_tokens = 8192
                temperature = 0.3
        except (ValueError, TypeError) as _:  # type: ignore
            max_tokens = 8192
            temperature = 0.3

        with open('prompts/prompt_validate_claims_batch.txt', 'r', encoding='utf-8') as f:
            prompt_template: str = f.read()

        summary_str: str = json.dumps(source_summary, ensure_ascii=False, indent=2)
        sentences_str: str = json.dumps(sentences, ensure_ascii=False, indent=2)

        final_prompt = prompt_template.replace('{{SOURCE_SUMMARY}}', summary_str)
        final_prompt = final_prompt.replace('{{SENTENCES_TO_VALIDATE}}', sentences_str)

        system_prompt = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç¼–è¾‘ï¼Œè´Ÿè´£æ‰¹é‡æ ¸æŸ¥æ–‡ç¨¿ä¸­å¼•ç”¨çš„å‡†ç¡®æ€§ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ä¸€ä¸ªå¥å­åˆ—è¡¨ä¸­çš„æ¯å¥è¯æ˜¯å¦éƒ½å¾—åˆ°äº†å…¶å¼•ç”¨çš„æ–‡çŒ®æ‘˜è¦çš„æ”¯æŒã€‚"

        return _call_ai_api(final_prompt, api_config, system_prompt, max_tokens=max_tokens, temperature=temperature, response_format="json")  # type: ignore

    except Exception as _:  # type: ignore
        # ä½¿ç”¨generator_instanceçš„loggerï¼Œå¦‚æœå¯ç”¨
        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰generator_instanceçš„å¼•ç”¨ï¼Œæ‰€ä»¥æš‚æ—¶ä¸è®°å½•æ—¥å¿—
        return None

def run_review_validation(generator_instance: Any) -> bool:  # type: ignore
    """
    [ç¬¬äºŒé˜¶æ®µéªŒè¯] å¯¹ç”Ÿæˆçš„æ–‡çŒ®ç»¼è¿°Wordæ–‡æ¡£è¿›è¡Œé«˜æ•ˆã€æ‰¹é‡çš„éªŒè¯ã€‚
    """
    generator_instance.logger.info("=" * 60 + "\næ–‡çŒ®ç»¼è¿°éªŒè¯é˜¶æ®µ (é«˜æ•ˆç‰ˆ)\n" + "=" * 60)  # type: ignore
    try:
        if not generator_instance.config.getboolean('Performance', 'enable_stage2_validation', fallback=False):  # type: ignore
            generator_instance.logger.warn("ç¬¬äºŒé˜¶æ®µéªŒè¯æœªåœ¨é…ç½®ä¸­å¯ç”¨ã€‚è·³è¿‡æ­¤æ­¥éª¤ã€‚")  # type: ignore
            return True

        if not DOCX_AVAILABLE:
            generator_instance.logger.error("python-docxæ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œç¬¬äºŒé˜¶æ®µéªŒè¯ã€‚è¯·è¿è¡Œ: pip install python-docx")  # type: ignore
            return False

        word_file: str = os.path.join(generator_instance.output_dir, f'{generator_instance.project_name}_literature_review.docx')  # type: ignore
        if not os.path.exists(word_file):
            generator_instance.logger.error(f"æ‰¾ä¸åˆ°æ–‡çŒ®ç»¼è¿°æ–‡ä»¶: {word_file}ã€‚è¯·å…ˆç”Ÿæˆç»¼è¿°ã€‚")  # type: ignore
            return False
            
        validator_api_config: Dict[str, str] = {
            'api_key': (generator_instance.config.get('Validator_API') or {}).get('api_key', ''),  # type: ignore
            'model': (generator_instance.config.get('Validator_API') or {}).get('model', ''),  # type: ignore
            'api_base': (generator_instance.config.get('Validator_API') or {}).get('api_base', 'https://api.openai.com/v1')  # type: ignore
        }
        api_config_valid: bool = bool(validator_api_config['api_key'] and validator_api_config['model'])  # type: ignore

        doc = Document(word_file)  # type: ignore
        
        # --- 1. å»ºç«‹æ–‡çŒ®åº“ç´¢å¼•å’Œå¼•ç”¨ç´¢å¼• ---
        generator_instance.logger.info("æ­¥éª¤1/3: æ­£åœ¨ç´¢å¼•æ–‡çŒ®åº“å’Œç»¼è¿°ä¸­çš„æ‰€æœ‰å¼•ç”¨...")
        valid_citation_map: Dict[str, Dict[str, Any]] = {} # {'(Author, YYYY)': summary}
        citation_to_key: Dict[str, str] = {}    # {'(Author et al., YYYY)': '(Author, YYYY)'}
        for i, summary in enumerate(generator_instance.summaries):  # type: ignore
            info: Dict[str, Any] = summary.get('paper_info', {})
            authors: List[str] = info.get('authors', [])
            year: str = str(info.get('year', 'N/A'))
            if authors and year != 'N/A':
                # åˆ›å»ºæ ‡å‡†å¼•ç”¨æ ¼å¼ (Author, YYYY)
                if len(authors) == 1:
                    standard_citation: str = f"({authors[0]}, {year})"
                elif len(authors) <= 3:
                    standard_citation: str = f"({', '.join(authors[:-1])} & {authors[-1]}, {year})"
                else:
                    standard_citation: str = f"({authors[0]} et al., {year})"
                
                valid_citation_map[standard_citation] = summary
                
                # åˆ›å»ºå¤šç§å¼•ç”¨æ ¼å¼çš„æ˜ å°„ï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ ¼å¼å˜ä½“
                # é¦–å…ˆå®šä¹‰æ ‡å‡†åŒ–å‡½æ•°ï¼ˆå±€éƒ¨ä½¿ç”¨ï¼‰
                def normalize_citation_for_mapping(citation: str) -> str:
                    """æ ‡å‡†åŒ–å¼•ç”¨å­—ç¬¦ä¸²ç”¨äºæ˜ å°„é”®"""
                    # ç§»é™¤å¤šä½™ç©ºæ ¼ï¼Œå°†å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
                    citation = re.sub(r'\s+', ' ', citation).strip()
                    # ç»Ÿä¸€æ ‡ç‚¹ï¼šä¸­æ–‡æ ‡ç‚¹æ›¿æ¢ä¸ºè‹±æ–‡æ ‡ç‚¹
                    citation = citation.replace('ï¼›', ';').replace('ï¼Œ', ',').replace('ã€', ',')
                    # ç§»é™¤å¸¸è§çš„ä¸­æ–‡å‰ç¼€ï¼ˆå¦‚"æ”¯æŒæ–‡çŒ®:"ã€"å‚è§:"ã€"æ¥æº:"ç­‰ï¼‰
                    # å¤„ç†æ‹¬å·å†…çš„å‰ç¼€ï¼Œä¾‹å¦‚"(æ”¯æŒæ–‡çŒ®: ä½œè€…, å¹´ä»½)" -> "(ä½œè€…, å¹´ä»½)"
                    citation = re.sub(r'\(æ”¯æŒæ–‡çŒ®[:ï¼š]\s*', '(', citation)
                    citation = re.sub(r'\(å‚è§[:ï¼š]\s*', '(', citation)
                    citation = re.sub(r'\(æ¥æº[:ï¼š]\s*', '(', citation)
                    citation = re.sub(r'\(å¼•ç”¨è‡ª[:ï¼š]\s*', '(', citation)
                    # ç»Ÿä¸€â€œå’Œâ€ä¸â€œ&â€
                    citation = citation.replace(' å’Œ ', ' & ').replace('å’Œ', ' & ')
                    # ç»Ÿä¸€â€œç­‰â€ä¸â€œet al.â€ - ä½¿ç”¨ä¸normalize_citationä¸€è‡´çš„é€»è¾‘
                    citation = re.sub(r'ç­‰\s*,', ' et al.,', citation)
                    citation = re.sub(r'ç­‰\s*;', ' et al.;', citation)
                    citation = re.sub(r'ç­‰\s*\)', ' et al.)', citation)
                    citation = re.sub(r'\sç­‰\s*,', ' et al.,', citation)
                    # ç¡®ä¿å¹´ä»½å‰æœ‰ç©ºæ ¼
                    citation = re.sub(r',(\d{4})', r', \1', citation)
                    # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„åŒé€—å·
                    citation = re.sub(r',\s*,', ', ', citation)
                    citation = re.sub(r'et al\.\s*,', 'et al.,', citation)
                    return citation
                
                # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å¼•ç”¨æ ¼å¼å˜ä½“
                citation_variants = []
                
                if len(authors) == 1:
                    # å•ä½œè€…å˜ä½“ï¼ˆåŒ…æ‹¬AIå¯èƒ½é”™è¯¯ç”Ÿæˆçš„'ç­‰'æ ¼å¼ï¼‰
                    base_formats = [
                        f"({authors[0]}, {year})",
                        f"({authors[0]} ç­‰, {year})",
                        f"({authors[0]}ç­‰, {year})",
                        f"({authors[0]} et al., {year})",
                        f"({authors[0]}, {year})",  # åŸå§‹æ ¼å¼
                        f"({authors[0]}, {year})",  # æ— ç©ºæ ¼å˜ä½“
                        f"({authors[0]}, {year})",  # å…¨è§’é€—å·å˜ä½“ï¼ˆæ ‡å‡†åŒ–åä¼šå¤„ç†ï¼‰
                    ]
                    citation_variants.extend(base_formats)
                    
                elif len(authors) == 2:
                    # åŒä½œè€…å˜ä½“
                    base_formats = [
                        f"({authors[0]} & {authors[1]}, {year})",
                        f"({authors[0]}, {authors[1]}, {year})",
                        f"({authors[0]} å’Œ {authors[1]}, {year})",
                        f"({authors[0]}ã€{authors[1]}, {year})",
                        f"({authors[0]}å’Œ{authors[1]}, {year})",
                        f"({authors[0]} & {authors[1]}, {year})",
                        f"({authors[0]} et al., {year})"
                    ]
                    citation_variants.extend(base_formats)
                    
                elif len(authors) == 3:
                    # ä¸‰ä½œè€…å˜ä½“
                    base_formats = [
                        f"({authors[0]}, {authors[1]} & {authors[2]}, {year})",
                        f"({authors[0]}, {authors[1]}, {authors[2]}, {year})",
                        f"({authors[0]}ã€{authors[1]}å’Œ{authors[2]}, {year})",
                        f"({authors[0]}ã€{authors[1]}å’Œ{authors[2]}, {year})",
                        f"({authors[0]}, {authors[1]}, {authors[2]}, {year})",
                        f"({authors[0]} et al., {year})"
                    ]
                    citation_variants.extend(base_formats)
                    
                else:
                    # å››ä½åŠä»¥ä¸Šä½œè€…å˜ä½“
                    base_formats = [
                        f"({authors[0]} et al., {year})",
                        f"({authors[0]} ç­‰, {year})",
                        f"({authors[0]}ç­‰, {year})"
                    ]
                    citation_variants.extend(base_formats)
                
                # ä¸ºæ‰€æœ‰å˜ä½“åˆ›å»ºæ˜ å°„
                for variant in citation_variants:
                    # åŸå§‹æ ¼å¼æ˜ å°„
                    citation_to_key[variant] = standard_citation
                    # æ ‡å‡†åŒ–æ ¼å¼æ˜ å°„ï¼ˆå¤„ç†ç©ºæ ¼å’Œæ ‡ç‚¹å·®å¼‚ï¼‰
                    normalized_variant = normalize_citation_for_mapping(variant)
                    if normalized_variant != variant:
                        citation_to_key[normalized_variant] = standard_citation
                
                # é¢å¤–å¤„ç†ï¼šä½œè€…åä¹‹é—´å¯èƒ½æœ‰ç©ºæ ¼å˜ä½“
                if len(authors) >= 2:
                    # ä¸ºåŒä½œè€…æ·»åŠ æ— ç©ºæ ¼å˜ä½“
                    no_space_variant = f"({authors[0]}&{authors[1]}, {year})"
                    citation_to_key[no_space_variant] = standard_citation

        # ä»Wordæ–‡æ¡£ä¸­æå–æ‰€æœ‰å¼•ç”¨
        full_text: str = "\n".join([p.text for p in doc.paragraphs])
        sentences: List[str] = re.split(r'(?<=[.ã€‚?ï¼Ÿ!ï¼])\s+', full_text)

        # è¾…åŠ©å‡½æ•°ï¼šæ ‡å‡†åŒ–å¼•ç”¨å­—ç¬¦ä¸²
        def normalize_citation(citation: str) -> str:
            """æ ‡å‡†åŒ–å¼•ç”¨å­—ç¬¦ä¸²ï¼Œç»Ÿä¸€æ ‡ç‚¹å’Œç©ºæ ¼"""
            if not citation:
                return citation
            # ç§»é™¤å¤šä½™ç©ºæ ¼ï¼Œå°†å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
            citation = re.sub(r'\s+', ' ', citation).strip()
            # ç»Ÿä¸€æ ‡ç‚¹ï¼šä¸­æ–‡æ ‡ç‚¹æ›¿æ¢ä¸ºè‹±æ–‡æ ‡ç‚¹
            citation = citation.replace('ï¼›', ';').replace('ï¼Œ', ',').replace('ã€', ',')
            # ç§»é™¤å¸¸è§çš„ä¸­æ–‡å‰ç¼€ï¼ˆå¦‚"æ”¯æŒæ–‡çŒ®:"ã€"å‚è§:"ã€"æ¥æº:"ç­‰ï¼‰
            # å¤„ç†æ‹¬å·å†…çš„å‰ç¼€ï¼Œä¾‹å¦‚"(æ”¯æŒæ–‡çŒ®: ä½œè€…, å¹´ä»½)" -> "(ä½œè€…, å¹´ä»½)"
            citation = re.sub(r'\(æ”¯æŒæ–‡çŒ®[:ï¼š]\s*', '(', citation)
            citation = re.sub(r'\(å‚è§[:ï¼š]\s*', '(', citation)
            citation = re.sub(r'\(æ¥æº[:ï¼š]\s*', '(', citation)
            citation = re.sub(r'\(å¼•ç”¨è‡ª[:ï¼š]\s*', '(', citation)
            # ç»Ÿä¸€â€œå’Œâ€ä¸â€œ&â€
            citation = citation.replace(' å’Œ ', ' & ').replace('å’Œ', ' & ')
            # ç»Ÿä¸€â€œç­‰â€ä¸â€œet al.â€ - æ›´ç²¾ç»†çš„å¤„ç†
            # å¤„ç†â€œç­‰â€åé¢è·Ÿé€—å·çš„æƒ…å†µï¼Œå¦‚â€œ(å¼ æ˜ç­‰, 2021)â€ -> â€œ(å¼ æ˜ et al., 2021)â€
            citation = re.sub(r'ç­‰\s*,', ' et al.,', citation)
            # å¤„ç†â€œç­‰â€åé¢è·Ÿåˆ†å·çš„æƒ…å†µï¼ˆå¤šä¸ªå¼•ç”¨åˆ†éš”ï¼‰
            citation = re.sub(r'ç­‰\s*;', ' et al.;', citation)
            # å¤„ç†â€œç­‰â€åé¢è·Ÿå³æ‹¬å·çš„æƒ…å†µï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‡ºç°ï¼Œä½†å®¹é”™å¤„ç†ï¼‰
            citation = re.sub(r'ç­‰\s*\)', ' et al.)', citation)
            # å¤„ç†â€œç­‰â€å‰é¢æœ‰ç©ºæ ¼çš„æƒ…å†µï¼Œå¦‚â€œ(å¼ æ˜ ç­‰, 2021)â€
            citation = re.sub(r'\sç­‰\s*,', ' et al.,', citation)
            # ç¡®ä¿å¹´ä»½å‰æœ‰ç©ºæ ¼
            citation = re.sub(r',(\d{4})', r', \1', citation)
            # ç§»é™¤ä½œè€…åä¹‹é—´çš„å¤šä½™ç©ºæ ¼ï¼ˆä»…ä¿ç•™ä¸€ä¸ªç©ºæ ¼ï¼‰
            citation = re.sub(r'\(\s*', '(', citation)
            citation = re.sub(r'\s*\)', ')', citation)
            citation = re.sub(r'\s*,\s*', ', ', citation)
            citation = re.sub(r'\s*&\s*', ' & ', citation)
            # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„åŒé€—å·ï¼ˆå¦‚"et al.,,"ï¼‰
            citation = re.sub(r',\s*,', ', ', citation)
            citation = re.sub(r'et al\.\s*,', 'et al.,', citation)
            return citation
        
        # è¾…åŠ©å‡½æ•°ï¼šä»å¥å­ä¸­æå–æ‰€æœ‰å¼•ç”¨ï¼ˆæ­£ç¡®å¤„ç†å¤šä¸ªå¼•ç”¨ï¼‰
        def extract_citations_from_sentence(sentence: str) -> List[str]:
            """ä»å¥å­ä¸­æå–æ‰€æœ‰å¼•ç”¨ï¼Œæ­£ç¡®å¤„ç†å¤šä¸ªå¼•ç”¨å’Œä¸­æ–‡æ ‡ç‚¹"""
            citations = []
            
            # é¦–å…ˆï¼ŒåŒ¹é…æ‰€æœ‰å¯èƒ½åŒ…å«å¤šä¸ªå¼•ç”¨çš„æ¨¡å¼
            # æ¨¡å¼ï¼šä»¥æ‹¬å·å¼€å¤´ï¼ŒåŒ…å«é€—å·å’Œå¹´ä»½ï¼Œå¯èƒ½ç”±åˆ†å·åˆ†éš”å¤šä¸ªå¼•ç”¨
            # ä¾‹å¦‚ï¼š(ä½œè€…1, å¹´ä»½; ä½œè€…2, å¹´ä»½) æˆ– (ä½œè€…1, å¹´ä»½) ç­‰
            multi_citation_pattern = r'\([^)]+,\s*\d{4}(?:[;ï¼›]\s*[^)]+,\s*\d{4})*\)'
            multi_matches = re.findall(multi_citation_pattern, sentence)
            
            for match in multi_matches:
                # ç§»é™¤å¤–å±‚æ‹¬å·
                inner = match[1:-1].strip()
                if not inner:
                    continue
                    
                # æŒ‰ä¸­æ–‡æˆ–è‹±æ–‡åˆ†å·åˆ†å‰²
                parts = re.split(r'[ï¼›;]\s*', inner)
                for part in parts:
                    if not part.strip():
                        continue
                        
                    # ç¡®ä¿éƒ¨åˆ†æœ‰æ‹¬å·
                    part_stripped = part.strip()
                    if not part_stripped.startswith('('):
                        part_stripped = '(' + part_stripped
                    if not part_stripped.endswith(')'):
                        part_stripped = part_stripped + ')'
                    
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¼•ç”¨æ ¼å¼
                    if re.match(r'^\([^)]+,\s*\d{4}\)$', part_stripped):
                        citations.append(part_stripped)
            
            # å¦‚æœæœªæ‰¾åˆ°å¤šä¸ªå¼•ç”¨æ¨¡å¼ï¼Œå°è¯•ç›´æ¥åŒ¹é…å•ä¸ªå¼•ç”¨
            if not citations:
                single_matches = re.findall(r'\([^)]+,\s*\d{4}\)', sentence)
                citations.extend(single_matches)
            
            # å»é‡å¹¶è¿”å›
            return list(dict.fromkeys(citations))  # ä¿æŒé¡ºåºçš„å»é‡

        all_found_citations: set[str] = set()
        citation_locations: Dict[str, List[str]] = {}  # {'(Author, YYYY)': [sentence1, sentence2, ...]}

        for sentence in sentences:
            citations_in_sentence: List[str] = extract_citations_from_sentence(sentence)
            for citation in citations_in_sentence:
                # æ ‡å‡†åŒ–å¼•ç”¨
                normalized_citation = normalize_citation(citation)
                
                # å°è¯•æŸ¥æ‰¾æ˜ å°„ï¼šå…ˆå°è¯•åŸå§‹å¼•ç”¨ï¼Œå†å°è¯•æ ‡å‡†åŒ–åçš„å¼•ç”¨
                mapped_key: str = citation_to_key.get(citation, citation)
                if mapped_key == citation:  # åŸå§‹å¼•ç”¨æœªæ‰¾åˆ°æ˜ å°„
                    mapped_key = citation_to_key.get(normalized_citation, citation)
                
                all_found_citations.add(citation)
                if mapped_key not in citation_locations:
                    citation_locations[mapped_key] = []
                citation_locations[mapped_key].append(sentence.strip())

        # --- 2. å¹»è§‰å¼•ç”¨æ£€æŸ¥ ---
        phantom_citations: List[str] = sorted(list(all_found_citations - set(citation_to_key.keys()) - set(valid_citation_map.keys())))
        report_lines: List[str] = ["llm_reviewer_generatoræ–‡çŒ®ç»¼è¿°éªŒè¯æŠ¥å‘Š", f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n", "="*30]
        if phantom_citations:
            generator_instance.logger.error(f"å‘ç° {len(phantom_citations)} å¤„å¯èƒ½çš„å¹»è§‰å¼•ç”¨ï¼")
            report_lines.append("ã€å¹»è§‰å¼•ç”¨æ£€æŸ¥ - å¤±è´¥ã€‘\nä»¥ä¸‹å¼•ç”¨æœªåœ¨æ‚¨çš„æ–‡çŒ®åº“ä¸­æ‰¾åˆ°ï¼š\n" + "\n".join(phantom_citations))
        else:
            generator_instance.logger.success("å¼•ç”¨æ¥æºæ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°å¹»è§‰å¼•ç”¨ã€‚")
            report_lines.append("ã€å¹»è§‰å¼•ç”¨æ£€æŸ¥ - é€šè¿‡ã€‘\næ‰€æœ‰å¼•ç”¨å‡æ¥è‡ªæä¾›çš„æ–‡çŒ®åº“ã€‚")

        # --- 3. æ‰¹é‡è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ ---
        generator_instance.logger.info("æ­¥éª¤2/3: æ­£åœ¨æ‰¹é‡è¿›è¡Œè§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥...")
        mismatch_reports: List[Dict[str, str]] = []
        if not api_config_valid:
            generator_instance.logger.error("Validator_APIçš„api_keyæˆ–modelæœªåœ¨é…ç½®ä¸­æ‰¾åˆ°ã€‚è·³è¿‡è§‚ç‚¹åŒ¹é…æ£€æŸ¥ã€‚")
        else:
            papers_to_validate: Dict[str, List[str]] = {key: sentences for key, sentences in citation_locations.items() if sentences and key in valid_citation_map}
            
            iterator = papers_to_validate.items()
            if TQDM_AVAILABLE:
                iterator = tqdm(iterator, desc="[éªŒè¯] é€ç¯‡æ–‡çŒ®æ‰¹é‡æ ¸å¯¹")

            for paper_key, sentences_for_validation in iterator:
                source_summary: Dict[str, Any] = valid_citation_map[paper_key]
                title: str = source_summary.get('paper_info', {}).get('title', 'N/A')
                if TQDM_AVAILABLE:
                    iterator.set_postfix_str(f"æ ¸å¯¹: {title[:30]}...")  # type: ignore
                else:
                    generator_instance.logger.info(f"æ­£åœ¨æ ¸å¯¹: {title[:30]}...")
                
                # å»é‡å¥å­åˆ—è¡¨ï¼Œå‡å°‘ä¸å¿…è¦çš„APIè°ƒç”¨
                unique_sentences: List[str] = sorted(list(set(sentences_for_validation)))

                validation_result: Optional[Dict[str, Any]] = _validate_claims_for_single_paper(source_summary, unique_sentences, validator_api_config, generator_instance.config)  # type: ignore
                
                if validation_result:
                    for claim in validation_result.get('claims', []):
                        sentence: str = claim.get('sentence', '')
                        status: str = claim.get('status', 'UNKNOWN')
                        reason: str = claim.get('reason', '')
                        suggestion: str = claim.get('suggestion', '')
                        
                        if status in ['UNSUPPORTED', 'PARTIAL_SUPPORT']:
                            mismatch_reports.append({
                                'citation': paper_key,
                                'title': title,
                                'sentence': sentence,
                                'status': status,
                                'reason': reason,
                                'suggestion': suggestion
                            })

        # --- 4. ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š ---
        generator_instance.logger.info("æ­¥éª¤3/3: æ­£åœ¨ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        if mismatch_reports:
            generator_instance.logger.error(f"å‘ç° {len(mismatch_reports)} å¤„è§‚ç‚¹-å¼•ç”¨ä¸åŒ¹é…ï¼")
            report_lines.append("\nã€è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ - å¤±è´¥ã€‘\nä»¥ä¸‹è®ºç‚¹å¯èƒ½æœªå¾—åˆ°æ–‡çŒ®å……åˆ†æ”¯æŒï¼š\n")
            
            for i, report in enumerate(mismatch_reports, 1):
                report_lines.append(f"\n{i}. å¼•ç”¨: {report['citation']}")
                report_lines.append(f"   è®ºæ–‡: {report['title']}")
                report_lines.append(f"   çŠ¶æ€: {report['status']}")
                report_lines.append(f"   åŸå¥: {report['sentence']}")
                report_lines.append(f"   ç†ç”±: {report['reason']}")
                if report['suggestion']:
                    report_lines.append(f"   å»ºè®®: {report['suggestion']}")
        else:
            if api_config_valid:
                generator_instance.logger.success("è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥é€šè¿‡ï¼Œæ‰€æœ‰è®ºç‚¹å‡å¾—åˆ°æ–‡çŒ®æ”¯æŒã€‚")
                report_lines.append("\nã€è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ - é€šè¿‡ã€‘\næ‰€æœ‰è®ºç‚¹å‡å¾—åˆ°æ–‡çŒ®æ”¯æŒã€‚")
            else:
                report_lines.append("\nã€è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ - è·³è¿‡ã€‘\nç”±äºAPIé…ç½®é—®é¢˜ï¼Œè·³è¿‡æ­¤é¡¹æ£€æŸ¥ã€‚")

        # ä¿å­˜æŠ¥å‘Š
        report_file: str = os.path.join(generator_instance.output_dir, f'{generator_instance.project_name}_validation_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        generator_instance.logger.info(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return True

    except (configparser.NoSectionError, configparser.NoOptionError):
        generator_instance.logger.error("æ— æ³•æ‰¾åˆ°[Validator_API]æˆ–[Performance]ä¸­çš„éªŒè¯é…ç½®ï¼Œè·³è¿‡éªŒè¯ã€‚")
        return False
    except Exception as e:
        generator_instance.logger.error(f"éªŒè¯ç»¼è¿°æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
        traceback.print_exc()
        return False