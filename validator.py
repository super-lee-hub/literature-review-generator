#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éªŒè¯ä¸ä¿®æ­£æ¨¡å—
è´Ÿè´£å¯¹AIç”Ÿæˆçš„å†…å®¹è¿›è¡Œäº¤å‰éªŒè¯ï¼Œç¡®ä¿å‡†ç¡®æ€§å’Œå¯ä¿¡åº¦ã€‚
"""
import os
import json
import re
import traceback
from typing import Optional, Dict, Any, List
from datetime import datetime
import configparser

# å¯¼å…¥ç±»å‹å®šä¹‰
from models import APIConfig  # type: ignore

# ä¼˜é›…åœ°å¤„ç†å¯é€‰ä¾èµ–ï¼Œç¡®ä¿æ¨¡å—çš„ç‹¬ç«‹å¥å£®æ€§
try:
    from docx import Document  # type: ignore
    DOCX_AVAILABLE = True  # type: ignore
except ImportError:
    DOCX_AVAILABLE = False  # type: ignore
    Document = None  # type: ignore

try:
    from tqdm import tqdm  # type: ignore
    TQDM_AVAILABLE = True  # type: ignore
except ImportError:
    TQDM_AVAILABLE = False  # type: ignore
    from typing import Any, Optional, Iterator
    class tqdm:
        def __init__(self, iterable: Optional[Any] = None, **kwargs: Any):
            self.iterable: Any = iterable if iterable else []  # type: ignore
        def __iter__(self) -> Iterator[Any]:
            return iter(self.iterable)
        def set_postfix_str(self, s: str) -> None:
            pass

# å¯¼å…¥ä¸»ç¨‹åºä¸­çš„AIæ¥å£è°ƒç”¨å‡½æ•°
from ai_interface import _call_ai_api  # type: ignore

def validate_paper_analysis(generator_instance: Any, pdf_text: str, ai_result: Dict[str, Any],
                           use_cache: bool = True) -> Dict[str, Any]:
    """
    [ç¬¬ä¸€é˜¶æ®µéªŒè¯] å¯¹å•ç¯‡è®ºæ–‡çš„AIåˆ†æç»“æœè¿›è¡Œäº¤å‰éªŒè¯å’Œä¿®æ­£ã€‚
    å¢å¼ºå¼‚å¸¸å¤„ç†å’Œè¾“å…¥éªŒè¯ï¼Œæ”¯æŒéªŒè¯ç»“æœç¼“å­˜

    Args:
        generator_instance: æ–‡çŒ®ç»¼è¿°ç”Ÿæˆå™¨å®ä¾‹
        pdf_text: PDFå…¨æ–‡å†…å®¹
        ai_result: AIåˆ†æç»“æœ
        use_cache: æ˜¯å¦ä½¿ç”¨éªŒè¯ç»“æœç¼“å­˜ï¼ˆæé«˜æ€§èƒ½ï¼‰

    Returns:
        ä¿®æ­£åçš„AIåˆ†æç»“æœ
    """
    # è¾“å…¥éªŒè¯
    if not pdf_text:
        generator_instance.logger.warning("PDFæ–‡æœ¬ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡éªŒè¯")
        return ai_result

    if not ai_result:
        generator_instance.logger.warning("AIåˆ†æç»“æœä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡éªŒè¯")
        return ai_result

    # ç”Ÿæˆå†…å®¹å“ˆå¸Œç”¨äºç¼“å­˜
    content_hash: Optional[str] = None
    cache_file_path: Optional[str] = None
    if use_cache:
        import hashlib
        paper_info: Any = ai_result.get('paper_info') or {}  # type: ignore
        content_str = pdf_text[:1000] + str(paper_info.get('title', '')) + str(paper_info.get('authors', []))  # type: ignore
        content_hash = hashlib.md5(content_str.encode('utf-8')).hexdigest()

        # æ„å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_dir = os.path.join(generator_instance.output_dir, 'cache')  # type: ignore
        try:
            os.makedirs(cache_dir, exist_ok=True)
            cache_file_path = os.path.join(cache_dir, f'{content_hash}.json')
        except Exception as _:  # type: ignore
            generator_instance.logger.warning(f"åˆ›å»ºç¼“å­˜ç›®å½•å¤±è´¥: {_}ï¼Œå°†è·³è¿‡ç¼“å­˜")  # type: ignore
            cache_file_path = None

    # æ£€æŸ¥ç¼“å­˜
    if use_cache and content_hash and cache_file_path and os.path.exists(cache_file_path):
        try:
            with open(cache_file_path, 'r', encoding='utf-8') as f:
                cached_result = json.load(f)
            generator_instance.logger.info("ä»ç¼“å­˜ä¸­åŠ è½½éªŒè¯ç»“æœ")
            return cached_result
        except Exception as e:
            generator_instance.logger.warning(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°éªŒè¯")

    generator_instance.logger.info("å¯åŠ¨ç¬¬ä¸€é˜¶æ®µäº¤å‰éªŒè¯...")

    # é¢„æ£€æŸ¥ï¼šå¦‚æœæ‘˜è¦åŒ…å«å ä½ç¬¦'...'ï¼Œè·³è¿‡éªŒè¯ï¼ˆå› ä¸ºéªŒè¯AIä¼šé”™è¯¯åœ°å¡«å……å®ƒï¼‰
    try:
        common_core = ai_result.get('common_core', {})
        placeholder_fields: List[str] = []
        
        # æ£€æŸ¥æ‰€æœ‰å­—æ®µæ˜¯å¦åŒ…å«'...'
        for field, value in common_core.items():
            if isinstance(value, str) and '...' in value:
                placeholder_fields.append(field)
            elif isinstance(value, list):
                for i, item in enumerate(value):  # type: ignore
                    if isinstance(item, str) and '...' in item:
                        placeholder_fields.append(f"{field}[{i}]")
        
        if placeholder_fields:
            generator_instance.logger.warning(f"å‘ç°å ä½ç¬¦'...'åœ¨å­—æ®µ: {', '.join(placeholder_fields)}ï¼Œè·³è¿‡éªŒè¯ä»¥é¿å…é”™è¯¯å¡«å……")
            generator_instance.logger.info("å†…å®¹è´¨é‡æ£€æŸ¥é€šè¿‡ï¼ˆè·³è¿‡éªŒè¯ï¼‰")
            return ai_result
    except Exception as e:
        generator_instance.logger.warning(f"é¢„æ£€æŸ¥å ä½ç¬¦æ—¶å‡ºé”™: {e}ï¼Œç»§ç»­æ­£å¸¸éªŒè¯æµç¨‹")

    try:
        # å®‰å…¨è·å–é…ç½®
        validator_config: Dict[str, str] = generator_instance.config.get('Validator_API', {})
        if not validator_config:
            generator_instance.logger.error("æœªæ‰¾åˆ°[Validator_API]é…ç½®æ®µï¼Œè·³è¿‡éªŒè¯ã€‚")  # type: ignore
            return ai_result

        validator_api_config: Dict[str, str] = {  # type: ignore
            'api_key': validator_config.get('api_key', ''),  # type: ignore
            'model': validator_config.get('model', ''),  # type: ignore
            'api_base': validator_config.get('api_base', 'https://api.openai.com/v1')  # type: ignore
        }  # type: ignore

        # éªŒè¯é…ç½®å®Œæ•´æ€§
        if not validator_api_config['api_key'] or not validator_api_config['api_key'].strip():
            generator_instance.logger.error("Validator_APIçš„api_keyæœªé…ç½®æˆ–ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        if not validator_api_config['model'] or not validator_api_config['model'].strip():
            generator_instance.logger.error("Validator_APIçš„modelæœªé…ç½®æˆ–ä¸ºç©ºï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        # ä½¿ç”¨ä¸¥æ ¼éªŒè¯æç¤ºè¯ï¼Œåªæ£€æŸ¥å®¢è§‚äº‹å®é”™è¯¯
        prompt_file_path: str = 'prompts/prompt_validate_analysis_strict.txt'
        try:
            with open(prompt_file_path, 'r', encoding='utf-8') as f:
                prompt_template = f.read()
        except FileNotFoundError:
            generator_instance.logger.error(f"æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_file_path}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result
        except UnicodeDecodeError:
            generator_instance.logger.error(f"æç¤ºè¯æ–‡ä»¶ç¼–ç é”™è¯¯: {prompt_file_path}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result
        except Exception as e:
            generator_instance.logger.error(f"è¯»å–æç¤ºè¯æ–‡ä»¶å¤±è´¥: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        # å®‰å…¨ç”Ÿæˆæç¤ºè¯
        try:
            summary_str: str = json.dumps(ai_result, ensure_ascii=False, indent=2)
            max_text_len: int = 800000  # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé˜²æ­¢APIè°ƒç”¨è¶…é™

            # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
            truncated_pdf_text = pdf_text[:max_text_len] if len(pdf_text) > max_text_len else pdf_text

            final_prompt = prompt_template.replace('{{PAPER_FULL_TEXT}}', truncated_pdf_text)
            final_prompt = final_prompt.replace('{{GENERATED_SUMMARY}}', summary_str)
        except Exception as e:
            generator_instance.logger.error(f"ç”ŸæˆéªŒè¯æç¤ºè¯å¤±è´¥: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        system_prompt = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯äº‹å®æ ¸æŸ¥å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹æ¯”è®ºæ–‡åŸæ–‡å’ŒAIç”Ÿæˆçš„æ‘˜è¦ï¼Œæ‰¾å‡ºå¹¶ä¿®æ­£æ‘˜è¦ä¸­çš„ä»»ä½•ä¸å‡†ç¡®ä¹‹å¤„ã€‚"

        # è°ƒç”¨éªŒè¯API
        try:
            # ä»é…ç½®ä¸­è¯»å–APIå‚æ•°
            validator_max_tokens: int = int((generator_instance.config.get('API_Parameters') or {}).get('validator_max_tokens', 4096))  # type: ignore
            validator_temperature: float = float((generator_instance.config.get('API_Parameters') or {}).get('validator_temperature', 0.3))  # type: ignore

            validation_report = _call_ai_api(
                final_prompt,
                validator_api_config,  # type: ignore
                system_prompt,
                max_tokens=validator_max_tokens,
                temperature=validator_temperature,
                response_format="json",
                logger=generator_instance.logger  # type: ignore
            )  # type: ignore
        except Exception as e:
            generator_instance.logger.error(f"è°ƒç”¨éªŒè¯APIå¤±è´¥: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚")
            return ai_result

        # å¤„ç†éªŒè¯ç»“æœ
        if not validation_report:
            generator_instance.logger.error("éªŒè¯è¿‡ç¨‹è¿”å›ç©ºæŠ¥å‘Šï¼Œå°†ä½¿ç”¨æœªç»æ ¸å®çš„æ‘˜è¦ã€‚")
            return ai_result

        if not validation_report:
            generator_instance.logger.error("éªŒè¯æŠ¥å‘Šæ ¼å¼æ— æ•ˆï¼Œå°†ä½¿ç”¨æœªç»æ ¸å®çš„æ‘˜è¦ã€‚")
            return ai_result

        # æ£€æŸ¥ä¸€è‡´æ€§å¹¶åº”ç”¨ä¿®æ­£
        is_consistent: bool = validation_report.get("is_consistent", True)
        if not is_consistent:
            feedback: str = validation_report.get('feedback', 'æ— åé¦ˆä¿¡æ¯')
            generator_instance.logger.warn(f"éªŒè¯å‘ç°ä¸ä¸€è‡´: {feedback}")

            corrections: List[Dict[str, Any]] = validation_report.get("corrections", [])
            if not corrections:
                generator_instance.logger.info("æŠ¥å‘Šå­˜åœ¨ä¸ä¸€è‡´ï¼Œä½†æœªæä¾›å…·ä½“ä¿®æ­£é¡¹ã€‚")
                return ai_result

            # ğŸ†• æ™ºèƒ½åº”ç”¨ä¿®æ­£ï¼šå¼•å…¥"æ™ºèƒ½è¿½åŠ "ç­–ç•¥
            applied_corrections: int = 0
            for i, correction in enumerate(corrections, 1):
                try:
                    if not correction:
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}æ ¼å¼æ— æ•ˆï¼Œè·³è¿‡")
                        continue

                    field_to_correct = correction.get("field")
                    corrected_value = correction.get("corrected_value")

                    if not field_to_correct or not isinstance(field_to_correct, str):
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ç¼ºå°‘å­—æ®µåæˆ–å­—æ®µåæ— æ•ˆï¼Œè·³è¿‡")
                        continue

                    if corrected_value is None:
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ç¼ºå°‘ä¿®æ­£å€¼ï¼Œè·³è¿‡")
                        continue
                    
                    # æ£€æŸ¥ä¿®æ­£å€¼çš„æœ‰æ•ˆæ€§
                    if isinstance(corrected_value, str) and corrected_value.strip() == '':
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ä¿®æ­£å€¼ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè·³è¿‡")
                        continue
                    
                    if isinstance(corrected_value, str) and len(corrected_value.strip()) < 3:
                        generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}ä¿®æ­£å€¼è¿‡çŸ­({len(corrected_value.strip())}å­—ç¬¦): '{corrected_value}'ï¼Œè·³è¿‡")
                        continue

                    # å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®
                    keys: List[str] = field_to_correct.split('.')
                    temp_dict: Dict[str, Any] = ai_result

                    # å®‰å…¨å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®
                    for key in keys[:-1]:
                        if key not in temp_dict:
                            temp_dict[key] = {}
                        elif not isinstance(temp_dict[key], dict):
                            generator_instance.logger.warning(f"ä¿®æ­£é¡¹{i}çš„ç›®æ ‡è·¯å¾„ '{field_to_correct}' åŒ…å«éå­—å…¸ç±»å‹ï¼Œè·³è¿‡")
                            break
                        temp_dict = temp_dict[key]
                    else:
                        field_name = keys[-1]
                        original_value = temp_dict.get(field_name, '')
                        
                        # è®°å½•ä¿®æ­£å‰çŠ¶æ€
                        generator_instance.logger.info(f"ğŸ” ä¿®æ­£å‰: {field_to_correct} = '{str(original_value)[:100]}...' (é•¿åº¦: {len(str(original_value))})")
                        generator_instance.logger.info(f"ğŸ” ä¿®æ­£å€¼: '{str(corrected_value)[:100]}...' (é•¿åº¦: {len(str(corrected_value))})")
                        
                        # ğŸ¯ æ™ºèƒ½åˆ†æ”¯å¤„ç†ç­–ç•¥
                        is_original_empty = (not original_value or 
                                           original_value in ['æœªæä¾›ç›¸å…³ä¿¡æ¯', 'æœªæåŠ', '', 'N/A', '...'])
                        is_corrected_valid = (corrected_value and 
                                             corrected_value not in ['æœªæä¾›ç›¸å…³ä¿¡æ¯', 'æœªæåŠ', '', 'N/A'])
                        
                        if isinstance(original_value, str) and isinstance(corrected_value, str):
                            original_len = len(original_value)
                            corrected_len = len(corrected_value)
                            
                            # æƒ…å†µAï¼šå®Œå…¨æ›¿æ¢ - ä¿®æ­£å€¼é•¿åº¦ä¸åŸå€¼ç›¸å½“ï¼Œæˆ–è€…åŸå€¼ä¸ºç©º/å ä½ç¬¦
                            if is_original_empty or corrected_len > original_len * 0.6:
                                temp_dict[field_name] = corrected_value
                                generator_instance.logger.info(f"âœ… å­—æ®µ '{field_to_correct}' æ‰§è¡Œå®Œå…¨æ›¿æ¢ (ä¿®æ­£é•¿åº¦: {corrected_len}, åŸé•¿åº¦: {original_len})")
                                
                            # æƒ…å†µBï¼šæ™ºèƒ½è¿½åŠ  - ä¿®æ­£å€¼è¾ƒçŸ­ï¼Œè®¤ä¸ºæ˜¯å±€éƒ¨ä¿®æ­£æˆ–è¡¥å……
                            else:
                                # è·å–ä¿®æ­£ä¾æ®
                                justification = ""
                                for correction in corrections:
                                    if correction.get("field") == field_to_correct:
                                        justification = correction.get("justification", "")
                                        break
                                
                                # è¿½åŠ ä¿®æ­£æ ¼å¼ï¼šåŸå€¼æ–‡æœ¬... [æ•°æ®æ ¸æŸ¥]: å»ºè®®ä¿®æ­£ä¸º"ä¿®æ­£å€¼æ–‡æœ¬"ï¼Œä¾æ®ï¼š"ä¿®æ­£ä¾æ®"
                                if justification:
                                    combined_value = f"{original_value}... [æ•°æ®æ ¸æŸ¥]: å»ºè®®ä¿®æ­£ä¸º\"{corrected_value}\"ï¼Œä¾æ®ï¼š\"{justification}\""
                                else:
                                    # å¦‚æœæ²¡æœ‰justificationï¼Œä½¿ç”¨ç®€å•çš„è¿½åŠ æ ¼å¼
                                    combined_value = f"{original_value} (ä¿®æ­£/è¡¥å……: {corrected_value})"
                                
                                temp_dict[field_name] = combined_value
                                generator_instance.logger.info(f"âœ… å­—æ®µ '{field_to_correct}' æ‰§è¡Œæ™ºèƒ½è¿½åŠ  (ä¿®æ­£: {corrected_len}å­—ç¬¦è¿½åŠ åˆ°åŸå€¼: {original_len}å­—ç¬¦ï¼ŒåŒ…å«è¯æ®é“¾: {bool(justification)})")
                                
                        elif is_corrected_valid:
                            # éå­—ç¬¦ä¸²ç±»å‹ä¿®æ­£ï¼Œç›´æ¥æ›¿æ¢
                            temp_dict[field_name] = corrected_value
                            generator_instance.logger.info(f"âœ… å­—æ®µ '{field_to_correct}' å·²æ›¿æ¢ä¿®æ­£ä¿¡æ¯ (éå­—ç¬¦ä¸²ç±»å‹)")
                        else:
                            # ä¿®æ­£å€¼æ— æ•ˆï¼Œä¿æŒåŸå€¼
                            generator_instance.logger.warning(f"âš ï¸  å­—æ®µ '{field_to_correct}' ä¿æŒåŸå€¼ (ä¿®æ­£å€¼æ— æ•ˆ)")
                        
                        # è®°å½•ä¿®æ­£åçŠ¶æ€
                        final_value = temp_dict.get(field_name, '')
                        generator_instance.logger.info(f"ğŸ” ä¿®æ­£å: {field_to_correct} = '{str(final_value)[:100]}...' (é•¿åº¦: {len(str(final_value))})")
                        
                        applied_corrections += 1

                except Exception as e:
                    generator_instance.logger.error(f"åº”ç”¨ä¿®æ­£é¡¹{i}æ—¶å‡ºé”™: {e}")
                    continue

            generator_instance.logger.info(f"å…±åº”ç”¨äº† {applied_corrections}/{len(corrections)} ä¸ªä¿®æ­£é¡¹")

        else:
            generator_instance.logger.success("éªŒè¯é€šè¿‡ï¼Œåˆ†æå†…å®¹ä¸åŸæ–‡ä¸€è‡´ã€‚")

    except (configparser.NoSectionError, configparser.NoOptionError) as e:
        generator_instance.logger.error(f"é…ç½®æ–‡ä»¶é”™è¯¯: {e}ï¼Œè·³è¿‡éªŒè¯ã€‚è¯·æ£€æŸ¥config.iniã€‚")
    except Exception as e:
        generator_instance.logger.error(f"éªŒè¯æ¨¡å—å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
        generator_instance.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")

    # ä¿å­˜éªŒè¯ç»“æœåˆ°ç¼“å­˜
    if use_cache and content_hash and cache_file_path and ai_result:
        try:
            with open(cache_file_path, 'w', encoding='utf-8') as f:
                json.dump(ai_result, f, ensure_ascii=False, indent=2)
            generator_instance.logger.debug(f"éªŒè¯ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜: {cache_file_path}")
        except Exception as e:
            generator_instance.logger.warning(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")

    return ai_result

def _validate_claims_for_single_paper(source_summary: dict, sentences: List[str], api_config: dict, config: dict = None) -> Optional[dict]:  # type: ignore
    """ä¸ºå•ç¯‡è®ºæ–‡çš„æ‰€æœ‰å¼•ç”¨å¥å­è°ƒç”¨ä¸€æ¬¡AIè¿›è¡Œæ‰¹é‡éªŒè¯"""
    try:
        # è¯»å–APIå‚æ•°é…ç½®
        try:
            if config:
                max_tokens: int = int(config.get('API_Parameters', {}).get('claims_max_tokens', 8192))  # type: ignore
                temperature: float = float(config.get('API_Parameters', {}).get('claims_temperature', 0.3))  # type: ignore
            else:
                max_tokens = 8192
                temperature = 0.3
        except (ValueError, TypeError) as _:  # type: ignore
            max_tokens = 8192
            temperature = 0.3

        with open('prompts/prompt_validate_claims_batch.txt', 'r', encoding='utf-8') as f:
            prompt_template: str = f.read()

        summary_str: str = json.dumps(source_summary, ensure_ascii=False, indent=2)
        sentences_str: str = json.dumps(sentences, ensure_ascii=False, indent=2)

        final_prompt = prompt_template.replace('{{SOURCE_SUMMARY}}', summary_str)
        final_prompt = final_prompt.replace('{{SENTENCES_TO_VALIDATE}}', sentences_str)

        system_prompt = "ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„å­¦æœ¯ç¼–è¾‘ï¼Œè´Ÿè´£æ‰¹é‡æ ¸æŸ¥æ–‡ç¨¿ä¸­å¼•ç”¨çš„å‡†ç¡®æ€§ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ä¸€ä¸ªå¥å­åˆ—è¡¨ä¸­çš„æ¯å¥è¯æ˜¯å¦éƒ½å¾—åˆ°äº†å…¶å¼•ç”¨çš„æ–‡çŒ®æ‘˜è¦çš„æ”¯æŒã€‚"

        return _call_ai_api(final_prompt, api_config, system_prompt, max_tokens=max_tokens, temperature=temperature, response_format="json")  # type: ignore

    except Exception as _:  # type: ignore
        # ä½¿ç”¨generator_instanceçš„loggerï¼Œå¦‚æœå¯ç”¨
        # æ³¨æ„ï¼šè¿™é‡Œæ²¡æœ‰generator_instanceçš„å¼•ç”¨ï¼Œæ‰€ä»¥æš‚æ—¶ä¸è®°å½•æ—¥å¿—
        return None

def run_review_validation(generator_instance: Any) -> bool:  # type: ignore
    """
    [ç¬¬äºŒé˜¶æ®µéªŒè¯] å¯¹ç”Ÿæˆçš„æ–‡çŒ®ç»¼è¿°Wordæ–‡æ¡£è¿›è¡Œé«˜æ•ˆã€æ‰¹é‡çš„éªŒè¯ã€‚
    """
    generator_instance.logger.info("=" * 60 + "\næ–‡çŒ®ç»¼è¿°éªŒè¯é˜¶æ®µ (é«˜æ•ˆç‰ˆ)\n" + "=" * 60)  # type: ignore
    try:
        if not generator_instance.config.getboolean('Performance', 'enable_stage2_validation', fallback=False):  # type: ignore
            generator_instance.logger.warn("ç¬¬äºŒé˜¶æ®µéªŒè¯æœªåœ¨é…ç½®ä¸­å¯ç”¨ã€‚è·³è¿‡æ­¤æ­¥éª¤ã€‚")  # type: ignore
            return True

        if not DOCX_AVAILABLE:
            generator_instance.logger.error("python-docxæ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œç¬¬äºŒé˜¶æ®µéªŒè¯ã€‚è¯·è¿è¡Œ: pip install python-docx")  # type: ignore
            return False

        word_file: str = os.path.join(generator_instance.output_dir, f'{generator_instance.project_name}_literature_review.docx')  # type: ignore
        if not os.path.exists(word_file):
            generator_instance.logger.error(f"æ‰¾ä¸åˆ°æ–‡çŒ®ç»¼è¿°æ–‡ä»¶: {word_file}ã€‚è¯·å…ˆç”Ÿæˆç»¼è¿°ã€‚")  # type: ignore
            return False
            
        validator_api_config: Dict[str, str] = {
            'api_key': (generator_instance.config.get('Validator_API') or {}).get('api_key', ''),  # type: ignore
            'model': (generator_instance.config.get('Validator_API') or {}).get('model', ''),  # type: ignore
            'api_base': (generator_instance.config.get('Validator_API') or {}).get('api_base', 'https://api.openai.com/v1')  # type: ignore
        }
        api_config_valid: bool = bool(validator_api_config['api_key'] and validator_api_config['model'])  # type: ignore

        doc = Document(word_file)  # type: ignore
        
        # --- 1. å»ºç«‹æ–‡çŒ®åº“ç´¢å¼•å’Œå¼•ç”¨ç´¢å¼• ---
        generator_instance.logger.info("æ­¥éª¤1/3: æ­£åœ¨ç´¢å¼•æ–‡çŒ®åº“å’Œç»¼è¿°ä¸­çš„æ‰€æœ‰å¼•ç”¨...")
        valid_citation_map: Dict[str, Dict[str, Any]] = {} # {'(Author, YYYY)': summary}
        citation_to_key: Dict[str, str] = {}    # {'(Author et al., YYYY)': '(Author, YYYY)'}
        for i, summary in enumerate(generator_instance.summaries):  # type: ignore
            info: Dict[str, Any] = summary.get('paper_info', {})
            authors: List[str] = info.get('authors', [])
            year: str = str(info.get('year', 'N/A'))
            if authors and year != 'N/A':
                # åˆ›å»ºæ ‡å‡†å¼•ç”¨æ ¼å¼ (Author, YYYY)
                if len(authors) == 1:
                    standard_citation: str = f"({authors[0]}, {year})"
                elif len(authors) <= 3:
                    standard_citation: str = f"({', '.join(authors[:-1])} & {authors[-1]}, {year})"
                else:
                    standard_citation: str = f"({authors[0]} et al., {year})"
                
                valid_citation_map[standard_citation] = summary
                
                # åˆ›å»ºå¤šç§å¼•ç”¨æ ¼å¼çš„æ˜ å°„ï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ ¼å¼å˜ä½“
                if len(authors) == 1:
                    # å•ä½œè€…ï¼šåªæœ‰ä¸€ç§æ ¼å¼
                    citation_to_key[f"({authors[0]}, {year})"] = standard_citation
                    
                elif len(authors) == 2:
                    # åŒä½œè€…ï¼šå¤šç§æ ¼å¼å˜ä½“
                    standard_citation = f"({authors[0]} & {authors[1]}, {year})"
                    valid_citation_map[standard_citation] = summary
                    
                    # è‹±æ–‡æ ¼å¼å˜ä½“
                    citation_to_key[f"({authors[0]}, {authors[1]}, {year})"] = standard_citation
                    
                    # ä¸­æ–‡æ ¼å¼å˜ä½“
                    citation_to_key[f"({authors[0]} å’Œ {authors[1]}, {year})"] = standard_citation
                    citation_to_key[f"({authors[0]}ã€{authors[1]}, {year})"] = standard_citation
                    
                    # æ ‡å‡†æ ¼å¼æœ¬èº«
                    citation_to_key[f"({authors[0]} & {authors[1]}, {year})"] = standard_citation
                    
                    # et al. æ ¼å¼æ˜ å°„
                    et_al_citation: str = f"({authors[0]} et al., {year})"
                    citation_to_key[et_al_citation] = standard_citation
                    
                elif len(authors) == 3:
                    # ä¸‰ä½œè€…ï¼šå¤šç§æ ¼å¼å˜ä½“
                    standard_citation = f"({authors[0]}, {authors[1]} & {authors[2]}, {year})"
                    valid_citation_map[standard_citation] = summary
                    
                    # è‹±æ–‡æ ¼å¼å˜ä½“
                    citation_to_key[f"({authors[0]}, {authors[1]}, {authors[2]}, {year})"] = standard_citation
                    
                    # ä¸­æ–‡æ ¼å¼å˜ä½“
                    citation_to_key[f"({authors[0]}ã€{authors[1]}å’Œ{authors[2]}, {year})"] = standard_citation
                    
                    # æ ‡å‡†æ ¼å¼æœ¬èº«
                    citation_to_key[f"({authors[0]}, {authors[1]} & {authors[2]}, {year})"] = standard_citation
                    
                    # et al. æ ¼å¼æ˜ å°„
                    et_al_citation = f"({authors[0]} et al., {year})"
                    citation_to_key[et_al_citation] = standard_citation
                    
                else:
                    # å››ä½åŠä»¥ä¸Šä½œè€…
                    standard_citation = f"({authors[0]} et al., {year})"
                    valid_citation_map[standard_citation] = summary
                    citation_to_key[standard_citation] = standard_citation

        # ä»Wordæ–‡æ¡£ä¸­æå–æ‰€æœ‰å¼•ç”¨
        full_text: str = "\n".join([p.text for p in doc.paragraphs])
        sentences: List[str] = re.split(r'(?<=[.ã€‚?ï¼Ÿ!ï¼])\s+', full_text)

        all_found_citations: set[str] = set()
        citation_locations: Dict[str, List[str]] = {}  # {'(Author, YYYY)': [sentence1, sentence2, ...]}

        for sentence in sentences:
            citations_in_sentence: List[str] = re.findall(r'\([^)]+,\s*\d{4}\)', sentence)
            for citation in citations_in_sentence:
                all_found_citations.add(citation)
                mapped_key: str = citation_to_key.get(citation, citation)
                if mapped_key not in citation_locations:
                    citation_locations[mapped_key] = []
                citation_locations[mapped_key].append(sentence.strip())

        # --- 2. å¹»è§‰å¼•ç”¨æ£€æŸ¥ ---
        phantom_citations: List[str] = sorted(list(all_found_citations - set(citation_to_key.keys()) - set(valid_citation_map.keys())))
        report_lines: List[str] = ["llm_reviewer_generatoræ–‡çŒ®ç»¼è¿°éªŒè¯æŠ¥å‘Š", f"ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\n", "="*30]
        if phantom_citations:
            generator_instance.logger.error(f"å‘ç° {len(phantom_citations)} å¤„å¯èƒ½çš„å¹»è§‰å¼•ç”¨ï¼")
            report_lines.append("ã€å¹»è§‰å¼•ç”¨æ£€æŸ¥ - å¤±è´¥ã€‘\nä»¥ä¸‹å¼•ç”¨æœªåœ¨æ‚¨çš„æ–‡çŒ®åº“ä¸­æ‰¾åˆ°ï¼š\n" + "\n".join(phantom_citations))
        else:
            generator_instance.logger.success("å¼•ç”¨æ¥æºæ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°å¹»è§‰å¼•ç”¨ã€‚")
            report_lines.append("ã€å¹»è§‰å¼•ç”¨æ£€æŸ¥ - é€šè¿‡ã€‘\næ‰€æœ‰å¼•ç”¨å‡æ¥è‡ªæä¾›çš„æ–‡çŒ®åº“ã€‚")

        # --- 3. æ‰¹é‡è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ ---
        generator_instance.logger.info("æ­¥éª¤2/3: æ­£åœ¨æ‰¹é‡è¿›è¡Œè§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥...")
        mismatch_reports: List[Dict[str, str]] = []
        if not api_config_valid:
            generator_instance.logger.error("Validator_APIçš„api_keyæˆ–modelæœªåœ¨é…ç½®ä¸­æ‰¾åˆ°ã€‚è·³è¿‡è§‚ç‚¹åŒ¹é…æ£€æŸ¥ã€‚")
        else:
            papers_to_validate: Dict[str, List[str]] = {key: sentences for key, sentences in citation_locations.items() if sentences and key in valid_citation_map}
            
            iterator = papers_to_validate.items()
            if TQDM_AVAILABLE:
                iterator = tqdm(iterator, desc="[éªŒè¯] é€ç¯‡æ–‡çŒ®æ‰¹é‡æ ¸å¯¹")

            for paper_key, sentences_for_validation in iterator:
                source_summary: Dict[str, Any] = valid_citation_map[paper_key]
                title: str = source_summary.get('paper_info', {}).get('title', 'N/A')
                if TQDM_AVAILABLE:
                    iterator.set_postfix_str(f"æ ¸å¯¹: {title[:30]}...")  # type: ignore
                else:
                    generator_instance.logger.info(f"æ­£åœ¨æ ¸å¯¹: {title[:30]}...")
                
                # å»é‡å¥å­åˆ—è¡¨ï¼Œå‡å°‘ä¸å¿…è¦çš„APIè°ƒç”¨
                unique_sentences: List[str] = sorted(list(set(sentences_for_validation)))

                validation_result: Optional[Dict[str, Any]] = _validate_claims_for_single_paper(source_summary, unique_sentences, validator_api_config, generator_instance.config)  # type: ignore
                
                if validation_result:
                    for claim in validation_result.get('claims', []):
                        sentence: str = claim.get('sentence', '')
                        status: str = claim.get('status', 'UNKNOWN')
                        reason: str = claim.get('reason', '')
                        suggestion: str = claim.get('suggestion', '')
                        
                        if status in ['UNSUPPORTED', 'PARTIAL_SUPPORT']:
                            mismatch_reports.append({
                                'citation': paper_key,
                                'title': title,
                                'sentence': sentence,
                                'status': status,
                                'reason': reason,
                                'suggestion': suggestion
                            })

        # --- 4. ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š ---
        generator_instance.logger.info("æ­¥éª¤3/3: æ­£åœ¨ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        if mismatch_reports:
            generator_instance.logger.error(f"å‘ç° {len(mismatch_reports)} å¤„è§‚ç‚¹-å¼•ç”¨ä¸åŒ¹é…ï¼")
            report_lines.append("\nã€è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ - å¤±è´¥ã€‘\nä»¥ä¸‹è®ºç‚¹å¯èƒ½æœªå¾—åˆ°æ–‡çŒ®å……åˆ†æ”¯æŒï¼š\n")
            
            for i, report in enumerate(mismatch_reports, 1):
                report_lines.append(f"\n{i}. å¼•ç”¨: {report['citation']}")
                report_lines.append(f"   è®ºæ–‡: {report['title']}")
                report_lines.append(f"   çŠ¶æ€: {report['status']}")
                report_lines.append(f"   åŸå¥: {report['sentence']}")
                report_lines.append(f"   ç†ç”±: {report['reason']}")
                if report['suggestion']:
                    report_lines.append(f"   å»ºè®®: {report['suggestion']}")
        else:
            if api_config_valid:
                generator_instance.logger.success("è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥é€šè¿‡ï¼Œæ‰€æœ‰è®ºç‚¹å‡å¾—åˆ°æ–‡çŒ®æ”¯æŒã€‚")
                report_lines.append("\nã€è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ - é€šè¿‡ã€‘\næ‰€æœ‰è®ºç‚¹å‡å¾—åˆ°æ–‡çŒ®æ”¯æŒã€‚")
            else:
                report_lines.append("\nã€è§‚ç‚¹-å¼•ç”¨åŒ¹é…æ£€æŸ¥ - è·³è¿‡ã€‘\nç”±äºAPIé…ç½®é—®é¢˜ï¼Œè·³è¿‡æ­¤é¡¹æ£€æŸ¥ã€‚")

        # ä¿å­˜æŠ¥å‘Š
        report_file: str = os.path.join(generator_instance.output_dir, f'{generator_instance.project_name}_validation_report.txt')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        generator_instance.logger.info(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        return True

    except (configparser.NoSectionError, configparser.NoOptionError):
        generator_instance.logger.error("æ— æ³•æ‰¾åˆ°[Validator_API]æˆ–[Performance]ä¸­çš„éªŒè¯é…ç½®ï¼Œè·³è¿‡éªŒè¯ã€‚")
        return False
    except Exception as e:
        generator_instance.logger.error(f"éªŒè¯ç»¼è¿°æ—¶å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
        traceback.print_exc()
        return False